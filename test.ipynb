{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from datasets import load_dataset, concatenate_datasets,  load_from_disk\n",
    "import peft\n",
    "\n",
    "# from safetensors.torch import load_model, save_model\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "from typing import Optional\n",
    "from jinja2 import Template\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = \"360M\"\n",
    "MODEL_PATH = f\"HuggingFaceTB/SmolLM2-{SIZE}-Instruct\"\n",
    "LORA_PATH = None\n",
    "# dataset = load_from_disk(\"/Users/ohi/Documents/GitHub/PersonalAssistant/dataset\")\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 0.72 GB of space (with buffer)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    # \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # attn_implementation='sdpa',\n",
    "    # attn_implementation='eager', # 'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    tie_word_embeddings=True,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Gradient checkpointing - Could take more memory in MPS\n",
    "# model.gradient_checkpointing_enable(dict(use_reentrant=False))\n",
    "model.gradient_checkpointing_disable()\n",
    "# model.resize_token_embeddings(49162)\n",
    "print(f\"Model took {model.get_memory_footprint()/1e9:.2f} GB of space (with buffer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft.PeftModel.from_pretrained(\n",
    "   model,\n",
    "   \"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-360M-sft-r64-old/checkpoint-8700/smolthink\",\n",
    "   is_trainable=False, # 👈 here,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): ModulesToSaveWrapper(\n",
      "          (original_module): Embedding(49152, 960, padding_idx=2)\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Embedding(49152, 960, padding_idx=2)\n",
      "          )\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=320, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=320, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=960, out_features=49152, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=960, out_features=49152, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# 49152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens -> base_model.model.model.embed_tokens.original_module.weight : False\n",
      "embed_tokens -> base_model.model.model.embed_tokens.modules_to_save.smolthink.weight : True\n",
      "lm_head -> base_model.model.lm_head.modules_to_save.smolthink.weight : True\n",
      "Total LoRA params: 34.73 million (7.61 %) = 34.73 million\n",
      "Total LoRA layers: 448\n",
      "Approx size: 69.47 mb\n"
     ]
    }
   ],
   "source": [
    "# if lora_r:\n",
    "lora_r = 64\n",
    "# SAVE_PATH += f'r{lora_r}'\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=lora_r,                   # 64\n",
    "    lora_alpha=2*lora_r,        # alpha = 4 * r\n",
    "    lora_dropout=0.05,\n",
    "    target_modules='all-linear',\n",
    "    modules_to_save = [\n",
    "        \"embed_tokens\", \n",
    "        \"lm_head\"\n",
    "    ],\n",
    "    use_rslora=True,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "model = peft.get_peft_model(model, peft_config, adapter_name=\"smolthink\", autocast_adapter_dtype=False)\n",
    "\n",
    "# Sanity check\n",
    "non_lora_param = 0\n",
    "lora_param = 0\n",
    "lora_layers = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        # param.requires_grad = True\n",
    "        assert param.requires_grad == True, f\"{name} is not trainable\"\n",
    "        lora_param += param.numel()\n",
    "        lora_layers += 1\n",
    "    else:\n",
    "        # if not param.requires_grad:\n",
    "        #     print(f\"{name} is trainable\")\n",
    "        non_lora_param += param.numel()\n",
    "\n",
    "    if 'lm_head' in name:\n",
    "        print(\"lm_head ->\", name, \":\", param.requires_grad)\n",
    "    if 'embed_tokens' in name:\n",
    "        print(\"embed_tokens ->\", name, \":\", param.requires_grad)\n",
    "\n",
    "\n",
    "def into_million(val):\n",
    "    return f\"{val / 1000 / 1000 :.2f} million\"\n",
    "\n",
    "# print(\"LoRA adapter added.\")\n",
    "print(f\"Total LoRA params: {into_million(lora_param)} ({(lora_param/non_lora_param)*100:.2f} %) = {into_million(lora_param)}\")\n",
    "print(f\"Total LoRA layers: {lora_layers}\")\n",
    "print(f\"Approx size: {lora_param * 2e-6:.2f} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModulesToSaveWrapper(\n",
       "  (original_module): Embedding(49152, 960, padding_idx=2)\n",
       "  (modules_to_save): ModuleDict(\n",
       "    (smolthink): Embedding(49152, 960, padding_idx=2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is same weight of embed_tokens and lm_head? True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is same weight of embed_tokens and lm_head?\", torch.equal(model.base_model.model.model.embed_tokens.modules_to_save[\"smolthink\"].weight, model.base_model.model.lm_head.modules_to_save[\"smolthink\"].weight))\n",
    "print(model.base_model.model.model.embed_tokens.original_module.weight.data.data_ptr() == model.base_model.model.lm_head.original_module.weight.data.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.decoder.embed_tokens.modules_to_save[\"default\"].weight = model.base_model.model.lm_head.modules_to_save[\"default\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 0.72 GB of space (with buffer)\n"
     ]
    }
   ],
   "source": [
    "model = model.merge_and_unload(safe_merge=True).eval().to(torch.bfloat16)\n",
    "print(f\"Model took {model.get_memory_footprint()/1e9:.2f} GB of space (with buffer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409.00704\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_tokens -> base_model.model.model.embed_tokens.original_module.weight : False\n",
    "embed_tokens -> base_model.model.model.embed_tokens.modules_to_save.smolthink.weight : True\n",
    "lm_head -> base_model.model.lm_head.modules_to_save.smolthink.weight : True\n",
    "Total LoRA params: 34.73 million (7.61 %) = 34.73 million\n",
    "Total LoRA layers: 448\n",
    "Approx size: 69.47 mb\n",
    "Model took 0.82 GB of space (with buffer)\n",
    "409.00704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "69468160\n"
     ]
    }
   ],
   "source": [
    "total_param = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print('lora' in name.lower())\n",
    "    total_param += param.numel()\n",
    "\n",
    "print(lora_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer length: 49152\n",
      "[44, 17400, 46]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"\"\"{%- if tools %}\n",
    "    {{- '<|im_start|>system\\\\n' }}\n",
    "        {%- if messages[0]['role'] == 'system' %}\n",
    "            {- messages[0]['content'] }}\n",
    "        {%- else %}\n",
    "            {{- 'You are a helpful AI assistant named SmolThink.' }}\n",
    "        {%- endif %}\n",
    "    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> tags:\\\\n<tools>\\\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \\\"\\\\n\\\" }}\n",
    "            {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \\\"\\\\n</tools>\\\\n\\\\nYou first think/plan inside <think></think> tags.\\\\nThen for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\\\\n\\\" }}\n",
    "{%- else %}\n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\n",
    "    {%- else %}\n",
    "        {{- '<|im_start|>system\\\\nYou are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside \\\\'think\\\\' tag and provide final answer to user query inside \\\\'answer\\\\' tag.\\\\nRespond in the following format:\\\\n<think>\\\\nLet\\\\'s think step by step...\\\\n</think>\\\\n<answer>\\\\nThe final answer is...\\\\n</answer><|im_end|>\\\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- for message in messages %}\n",
    "    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\n",
    "    {%- elif message.role == \\\"assistant\\\" %}\n",
    "        {{- '<|im_start|>' + message.role }}\n",
    "        {%- if message.content %}\n",
    "            {{- '\\\\n' + message.content }}\n",
    "        {%- endif %}\n",
    "        {%- for tool_call in message.tool_calls %}\n",
    "            {%- if tool_call.function is defined %}\n",
    "                {%- set tool_call = tool_call.function %}\n",
    "            {%- endif %}\n",
    "            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\n",
    "            {{- tool_call.name }}\n",
    "            {{- '\\\", \\\"arguments\\\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- '}\\\\n</tool_call>' }}\n",
    "        {%- endfor %}\n",
    "        {{- '<|im_end|>\\\\n' }}\n",
    "    {%- elif message.role == \\\"tool\\\" %}\n",
    "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_response>\\\\n' }}\n",
    "        {{- message.content }}\n",
    "        {{- '\\\\n</tool_response>' }}\n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_end|>\\\\n' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\\\n' }}\n",
    "{%- endif %}\"\"\"\n",
    "\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"pad_token\": \"<|im_end|>\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"think_start\": \"<think>\",\n",
    "    \"think_end\": \"</think>\",\n",
    "    \"answer_start\": \"<answer>\",\n",
    "    \"answer_end\": \"</answer>\",\n",
    "    \"tool_def_start\": \"<tool>\",\n",
    "    \"tool_def_end\": \"</tool>\",\n",
    "    \"tool_call_start\": \"<tool_call>\",\n",
    "    \"tool_call_end\": \"</tool_call>\",\n",
    "    \"tool_res_start\": \"<tool_response>\",\n",
    "    \"tool_res_end\": \"</tool_response>\",\n",
    "}\n",
    "\n",
    "class SpecialTokens(str, Enum):\n",
    "    think_start = \"<think>\",\n",
    "    think_end = \"</think>\",\n",
    "    answer_start = \"<answer>\",\n",
    "    answer_end = \"</answer>\",\n",
    "    tool_def_start = \"<tool>\",\n",
    "    tool_def_end = \"</tool>\",\n",
    "    tool_call_start = \"<tool_call>\",\n",
    "    tool_call_end = \"</tool_call>\",\n",
    "    tool_res_start = \"<tool_response>\",\n",
    "    tool_res_end = \"</tool_response>\",\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    # additional_special_tokens=SpecialTokens.list()\n",
    ")\n",
    "tokenizer.chat_template = chat_template\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "print(\"Tokenizer length:\", len(tokenizer))\n",
    "tokenizer.save_pretrained(\"SmolThink-360M-Tokenizer\")\n",
    "\n",
    "\n",
    "# tokenizer.add_special_tokens(\n",
    "#     # special_tokens_dict=special_tokens_dict, \n",
    "#     {\"additional_special_tokens\":[\"<think>\"]},\n",
    "#     replace_additional_special_tokens=True)\n",
    "# print(\"Tokenizer length:\", len(tokenizer))\n",
    "\n",
    "# print(\"New token map\")\n",
    "# for v in SpecialTokens.list():\n",
    "#     print(v, '->', tokenizer.encode(v))\n",
    "# print(\"---\")\n",
    "\n",
    "# print(tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"I am fine\"}\n",
    "# ], tokenize=False))\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_payment_status\",\n",
    "            \"description\": \"Get payment status of a transaction\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"transaction_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The transaction id.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"transaction_id\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_payment_date\",\n",
    "            \"description\": \"Get payment date of a transaction\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"transaction_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The transaction id.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"transaction_id\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "# print(\"\\n-----\\n\")\n",
    "# print(tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"<tool_call>[retrieve_payment_date(12)]</tool_call>\"},\n",
    "#     {\"role\": \"tool\", \"content\": \"12/12/12\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"12/12/12\"}\n",
    "# ], tools=tools, tokenize=False))\n",
    "\n",
    "print(tokenizer.encode(\"<think>\"))\n",
    "print(tokenizer.encode(\"<|im_start|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hi, can you tell me the current stock price of Apple?'}, {'role': 'assistant', 'content': \"<think>\\nAlright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think>\\n<tool_call>\\n[{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}]\\n</tool_call>\"}, {'role': 'tool', 'content': \"{'stock_price': '$150.75'}\"}, {'role': 'assistant', 'content': 'The current stock price of Apple is $150.75.'}, {'role': 'user', 'content': \"That's great. Can you also tell me the stock price for Microsoft?\"}, {'role': 'assistant', 'content': \"<think>\\nAlright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think>\\n<tool_call>\\n[{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}]\\n</tool_call>\"}, {'role': 'tool', 'content': \"{'stock_price': '$210.22'}\"}, {'role': 'assistant', 'content': 'The current stock price of Microsoft is $210.22.'}, {'role': 'user', 'content': 'Thank you for the information.'}, {'role': 'assistant', 'content': \"You're welcome! If you have any other questions, feel free to ask.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m fc_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJofthomas/hermes-function-calling-thinking-V1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# fc_dataset = fc_dataset.select(range(100))\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m fc_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfc_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhermes_fc_thinking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m fc_dataset \u001b[38;5;241m=\u001b[39m fc_dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction calling dataset length (after filter):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(fc_dataset))\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3495\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3494\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3392\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36mhermes_fc_thinking\u001b[0;34m(raw_data)\u001b[0m\n\u001b[1;32m     49\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(tool_def)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(seq)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ret \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(seq, tools\u001b[38;5;241m=\u001b[39mtool_def, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#+ \"<tool_call>\\n\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m: ret}\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_tag(input_str, tag):\n",
    "    tool_def = re.findall(f\"<{tag}>(.*?)</{tag}>\", input_str, re.DOTALL)\n",
    "    tool_def = map(str.strip, tool_def)\n",
    "    tool_def = filter(lambda x: len(x) > 0, tool_def)\n",
    "    return list(tool_def)\n",
    "\n",
    "def hermes_fc_thinking(raw_data):\n",
    "    data = deepcopy(raw_data['conversations'])\n",
    "    seq = []\n",
    "    tool_def = None\n",
    "    tool_names = None\n",
    "    for d in data:\n",
    "        if d['role'] == 'system':\n",
    "            tool_def = extract_tag(d['content'], 'tools')\n",
    "            if len(tool_def) != 0:\n",
    "                try:\n",
    "                    tool_def = ast.literal_eval(tool_def[0])\n",
    "                    tool_names = [tool['function']['name'] for tool in tool_def]\n",
    "                    continue\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "            else:\n",
    "                return {\"conversations\": \"\"}\n",
    "\n",
    "        seq.append({})\n",
    "        seq[-1]['role'] = {\"human\": \"user\", \"model\": \"assistant\", \"system\": \"system\", \"tool\": \"tool\"}[d['role']]\n",
    "        seq[-1]['content'] = d['content']\n",
    "        if seq[-1]['role'] == 'assistant':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<think>', '<think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('</think>', '</think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<tool_call>\\n', '<tool_call>\\n[')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('\\n</tool_call>', ']\\n</tool_call>')\n",
    "            tool_calls = re.findall(r\"<tool_call>(.*?)</tool_call>\", seq[-1]['content'], re.DOTALL)\n",
    "            if tool_calls:\n",
    "                # print(tool_calls, tool_def)\n",
    "                try:\n",
    "                    tool_calls = json.loads(tool_calls[0].strip().replace(\"'\", '\"'))\n",
    "                    for tool_call in tool_calls:\n",
    "                        if tool_call['name'] not in tool_names:\n",
    "                            raise NotImplementedError\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "        if seq[-1]['role'] == 'tool':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"<tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"</tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].strip()\n",
    "        # seq[-1]['content'] = d['value']\n",
    "    \n",
    "    random.shuffle(tool_def)\n",
    "    print(seq)\n",
    "\n",
    "    raise ValueError\n",
    "    ret = tokenizer.apply_chat_template(seq, tools=tool_def, tokenize=False, add_generation_prompt=False) #+ \"<tool_call>\\n\"\n",
    "    return {\"conversations\": ret}\n",
    "\n",
    "fc_dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")['train']\n",
    "# fc_dataset = fc_dataset.select(range(100))\n",
    "fc_dataset = fc_dataset.map(hermes_fc_thinking)\n",
    "fc_dataset = fc_dataset.filter(lambda x: len(x['conversations']) > 0)\n",
    "print(\"Function calling dataset length (after filter):\", len(fc_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(49162, 960, padding_idx=2)\n"
     ]
    }
   ],
   "source": [
    "print(model.cpu().resize_token_embeddings(len(tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49162, 960, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49162, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer\",\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    # additional_special_tokens=SpecialTokens.list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49152]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.encode(\"<think>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/tokenizer_config.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/special_tokens_map.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/vocab.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/merges.txt',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/added_tokens.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|> <|im_end|>\n",
      "[2]\n",
      "<issue_comment>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(tokenizer.encode(\"<|im_end|>\"))\n",
    "print(tokenizer.decode([9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_filter(data, limit):\n",
    "    # if data['thought_len'] + data['answer_len'] > 896:\n",
    "        # return False\n",
    "    return 0 < data['thought_len'] <= limit and 0 < data['answer_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1-distill dataset length (after filter): 29290\n"
     ]
    }
   ],
   "source": [
    "def r1distillsft_conv(data):\n",
    "    thought_len, answer_len = 0, 0\n",
    "    for idx, conv in enumerate(data['reannotated_messages']):\n",
    "        # print(conv)\n",
    "        role = conv['role']\n",
    "        if role == 'assistant':\n",
    "            reply = data['reannotated_messages'][idx]['content']\n",
    "            # print(reply)\n",
    "            thought = re.findall(r\"<think>(.*?)</think>\", reply, re.DOTALL)\n",
    "            thought = ''.join(thought).strip()\n",
    "            thought_len += len(thought.split()) #len(tokenizer.encode(thought))\n",
    "\n",
    "            end_tag = \"</think>\"\n",
    "            if end_tag in reply:\n",
    "                answer = reply[reply.find(end_tag)+len(end_tag):]\n",
    "                answer = answer.strip()\n",
    "            else:\n",
    "                answer = ''\n",
    "            if thought.lower() == answer.lower():\n",
    "                answer = ''\n",
    "            # print(\"Think:\", thought)\n",
    "            # print(\"Answer:\", answer)\n",
    "            # print(\"----\")\n",
    "            answer_len += len(answer.split()) #len(tokenizer.encode(answer))\n",
    "            data['reannotated_messages'][idx]['content'] = f\"<think>\\n{thought}\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n",
    "\n",
    "    if 'system' in data:\n",
    "        del data['system']\n",
    "    data['thought_len'] = thought_len\n",
    "    data['answer_len'] = answer_len\n",
    "    return data\n",
    "\n",
    "r1_dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", \"v1\")['train']\n",
    "r1_dataset.shuffle(123)\n",
    "r1_dataset = r1_dataset.select(range(90_000))\n",
    "r1_dataset = r1_dataset.map(r1distillsft_conv)\n",
    "r1_dataset = r1_dataset.filter(lambda x: length_filter(x, 256))\n",
    "delete_keys = list(r1_dataset.column_names)\n",
    "r1_dataset = r1_dataset.map(lambda x: {\"conversations\": tokenizer.apply_chat_template(x['reannotated_messages'], tools=None, tokenize=False)})\n",
    "r1_dataset = r1_dataset.remove_columns(delete_keys)\n",
    "print(\"R1-distill dataset length (after filter):\", len(r1_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing dataset length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 666.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DatasetGen_v1(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = None\n",
    "        self.cache_idx = -1\n",
    "        self.cache_len = 0\n",
    "        self.indices = []\n",
    "        self._get_len()\n",
    "\n",
    "    def _get_len(self):\n",
    "        print(\"Computing dataset length\")\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            self.gen(idx)\n",
    "            for i in range(self.cache_len):\n",
    "                self.indices.append((idx, i))\n",
    "        print(\"Total length of data:\", len(self.indices))\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def gen(self, idx):\n",
    "        self.cache = self.dataset[idx]['conversations'].rstrip()\n",
    "        self.cache = self.tokenizer(\n",
    "            self.cache,\n",
    "            max_length=CONTEXT_LEN,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=True, # Return the overflowing tokens\n",
    "            stride=CONTEXT_LEN // 8,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        self.cache_idx = idx\n",
    "        self.cache_len = len(self.cache['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p, q = self.indices[idx]\n",
    "        if self.cache_idx != p:\n",
    "            self.gen(p)\n",
    "        \n",
    "        input_ids = self.cache['input_ids'][q]\n",
    "        attention_mask = self.cache['attention_mask'][q]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "CONTEXT_LEN = 832 # 1024\n",
    "train_ds = DatasetGen_v1(\n",
    "    dataset=r1_dataset.select(range(10)), \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
      "Respond in the following format:\n",
      "<think>\n",
      "Let's think step by step...\n",
      "</think>\n",
      "<answer>\n",
      "The final answer is...\n",
      "</answer><|im_end|>\n",
      "<|im_start|>user\n",
      "There were 27 boys and 35 girls on the playground at recess. There were _____ children on the playground at recess.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "First, I need to determine the total number of children on the playground by adding the number of boys and girls.\n",
      "\n",
      "There are 27 boys and 35 girls.\n",
      "\n",
      "Adding these together: 27 boys + 35 girls = 62 children.\n",
      "\n",
      "Therefore, the total number of children on the playground is 62.\n",
      "</think>\n",
      "<answer>\n",
      "To find the total number of children on the playground, we simply add the number of boys and girls together.\n",
      "\n",
      "\\[\n",
      "\\text{Total children} = \\text{Number of boys} + \\text{Number of girls}\n",
      "\\]\n",
      "\n",
      "Plugging in the given values:\n",
      "\n",
      "\\[\n",
      "\\text{Total children} = 27 \\text{ boys} + 35 \\text{ girls} = 62 \\text{ children}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{62}\n",
      "\\]\n",
      "</answer><|im_end|>"
     ]
    }
   ],
   "source": [
    "for (t, m) in zip(train_ds[0]['input_ids'], train_ds[0]['attention_mask']):\n",
    "    if m == 1:\n",
    "        print(tokenizer.decode(t), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770 17520\n"
     ]
    }
   ],
   "source": [
    "long, short = 0, 0\n",
    "for idx, d in enumerate(r1_dataset):\n",
    "    \n",
    "    # print(d['conversations'])\n",
    "#     s = d['conversations'].replace('''<|im_start|>system\n",
    "# You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
    "# Respond in the following format:\n",
    "# <think>\n",
    "# Let's think step by step...\n",
    "# </think>\n",
    "# <answer>\n",
    "# The final answer is...\n",
    "# </answer><|im_end|>''', '')\n",
    "#     think_cnt = s.count(\"<think>\")\n",
    "#     ans_cnt = s.count(\"<answer>\")\n",
    "\n",
    "#     # print(s)\n",
    "#     if think_cnt > 1 or ans_cnt > 1:\n",
    "#         print(\"Check idx:\", idx)\n",
    "#         break\n",
    "    # break\n",
    "\n",
    "    en = tokenizer.encode(d['conversations'])\n",
    "    if len(en) > 832:\n",
    "        # print(idx, len(en))\n",
    "        long += 1\n",
    "    else:\n",
    "        short += 1\n",
    "\n",
    "print(long, short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4973 24317\n"
     ]
    }
   ],
   "source": [
    "long, short = 0, 0\n",
    "for idx, d in enumerate(r1_dataset):\n",
    "    \n",
    "    # print(d['conversations'])\n",
    "#     s = d['conversations'].replace('''<|im_start|>system\n",
    "# You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
    "# Respond in the following format:\n",
    "# <think>\n",
    "# Let's think step by step...\n",
    "# </think>\n",
    "# <answer>\n",
    "# The final answer is...\n",
    "# </answer><|im_end|>''', '')\n",
    "#     think_cnt = s.count(\"<think>\")\n",
    "#     ans_cnt = s.count(\"<answer>\")\n",
    "\n",
    "#     # print(s)\n",
    "#     if think_cnt > 1 or ans_cnt > 1:\n",
    "#         print(\"Check idx:\", idx)\n",
    "#         break\n",
    "    # break\n",
    "\n",
    "    en = tokenizer.encode(d['conversations'])\n",
    "    if len(en) > 1024:\n",
    "        # print(idx, len(en))\n",
    "        long += 1\n",
    "    else:\n",
    "        short += 1\n",
    "\n",
    "print(long, short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
      "Respond in the following format:\n",
      "<think>\n",
      "Let's think step by step...\n",
      "</think>\n",
      "<answer>\n",
      "The final answer is...\n",
      "</answer><|im_end|>\n",
      "<|im_start|>user\n",
      "Michael and Thomas are selling their lego collections. They agree to split any money they earn. They sell them based on how many circles are on top. Each circle costs 1 cent. They sold a certain number of single pieces, 45 double pieces, 50 triple pieces and 165 quadruple pieces. They earned $5 each. How many single pieces did they sell?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "First, I need to determine the total amount of money Michael and Thomas earned together. Since each earned $5 and they split the money equally, the total earnings amount to $10.\n",
      "\n",
      "Next, I'll calculate the earnings from the double, triple, and quadruple pieces separately.\n",
      "\n",
      "For the double pieces, there are 45 pieces with 2 circles each. Each circle costs 1 cent, so the earnings from double pieces are 45 * 2 * $0.01 = $0.90.\n",
      "\n",
      "For the triple pieces, there are 50 pieces with 3 circles each. The earnings from triple pieces are 50 * 3 * $0.01 = $1.50.\n",
      "\n",
      "For the quadruple pieces, there are 165 pieces with 4 circles each. The earnings from quadruple pieces are 165 * 4 * $0.01 = $6.60.\n",
      "\n",
      "Adding up the earnings from double, triple, and quadruple pieces: $0.90 + $1.50 + $6.60 = $9.00.\n",
      "\n",
      "Subtracting this from the total earnings gives the amount earned from single pieces: $10 - $9.00 = $1.00.\n",
      "\n",
      "Since each single piece earns 1 cent, the number of single pieces sold is $1.00 / $0.01 = 100.\n",
      "\n",
      "Therefore, Michael and Thomas sold 100 single pieces.\n",
      "</think>\n",
      "<answer>\n",
      "To determine how many single pieces Michael and Thomas sold, let's break down the problem step by step.\n",
      "\n",
      "### **Given:**\n",
      "- **Single pieces:** \\( x \\) pieces (unknown)\n",
      "- **Double pieces:** 45 pieces (each with 2 circles)\n",
      "- **Triple pieces:** 50 pieces (each with 3 circles)\n",
      "- **Quadruple pieces:** 165 pieces (each with 4 circles)\n",
      "- **Earnings per person:** \\$5\n",
      "- **Total earnings:** \\$10 (since they split the money equally)\n",
      "\n",
      "### **Step 1: Calculate Earnings from Each Type of Piece**\n",
      "\n",
      "1. **Double Pieces:**\n",
      "   \\[\n",
      "   45 \\text{ pieces} \\times 2 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$0.90\n",
      "   \\]\n",
      "\n",
      "2. **Triple Pieces:**\n",
      "   \\[\n",
      "   50 \\text{ pieces} \\times 3 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$1.50\n",
      "   \\]\n",
      "\n",
      "3. **Quadruple Pieces:**\n",
      "   \\[\n",
      "   165 \\text{ pieces} \\times 4 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$6.60\n",
      "   \\]\n",
      "\n",
      "### **Step 2: Calculate Total Earnings from Double, Triple, and Quadruple Pieces**\n",
      "\n",
      "\\[\n",
      "\\$0.90 + \\$1.50 + \\$6.60 = \\$9.00\n",
      "\\]\n",
      "\n",
      "### **Step 3: Determine Earnings from Single Pieces**\n",
      "\n",
      "Total earnings: \\$10  \n",
      "Earnings from other pieces: \\$9.00  \n",
      "Earnings from single pieces:  \n",
      "\\[\n",
      "\\$10 - \\$9.00 = \\$1.00\n",
      "\\]\n",
      "\n",
      "### **Step 4: Calculate the Number of Single Pieces Sold**\n",
      "\n",
      "Each single piece earns \\$0.01.  \n",
      "Number of single pieces:  \n",
      "\\[\n",
      "\\$1.00 \\div \\$0.01 = 100 \\text{ pieces}\n",
      "\\]\n",
      "\n",
      "### **Final Answer**\n",
      "\n",
      "\\[\n",
      "\\boxed{100}\n",
      "\\]\n",
      "</answer><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling dataset length (after filter): 3497\n"
     ]
    }
   ],
   "source": [
    "def extract_tag(input_str, tag):\n",
    "    tool_def = re.findall(f\"<{tag}>(.*?)</{tag}>\", input_str, re.DOTALL)\n",
    "    tool_def = map(str.strip, tool_def)\n",
    "    tool_def = filter(lambda x: len(x) > 0, tool_def)\n",
    "    return list(tool_def)\n",
    "\n",
    "def hermes_fc_thinking(raw_data):\n",
    "    data = deepcopy(raw_data['conversations'])\n",
    "    seq = []\n",
    "    tool_def = None\n",
    "    tool_names = None\n",
    "    for d in data:\n",
    "        if d['role'] == 'system':\n",
    "            tool_def = extract_tag(d['content'], 'tools')\n",
    "            if len(tool_def) != 0:\n",
    "                try:\n",
    "                    tool_def = ast.literal_eval(tool_def[0])\n",
    "                    tool_names = [tool['function']['name'] for tool in tool_def]\n",
    "                    continue\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "            else:\n",
    "                return {\"conversations\": \"\"}\n",
    "\n",
    "        seq.append({})\n",
    "        seq[-1]['role'] = {\"human\": \"user\", \"model\": \"assistant\", \"system\": \"system\", \"tool\": \"tool\"}[d['role']]\n",
    "        seq[-1]['content'] = d['content']\n",
    "        if seq[-1]['role'] == 'assistant':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<think>', '<think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('</think>', '</think>\\n')\n",
    "            # seq[-1]['content'] = seq[-1]['content'].replace('<tool_call>\\n', '<tool_call>\\n[')\n",
    "            # seq[-1]['content'] = seq[-1]['content'].replace('\\n</tool_call>', ']\\n</tool_call>')\n",
    "            tool_calls = re.findall(r\"<tool_call>(.*?)</tool_call>\", seq[-1]['content'], re.DOTALL)\n",
    "            seq[-1]['tool-call'] = []\n",
    "            if tool_calls:\n",
    "                # print(tool_calls, tool_def)\n",
    "                for tool_call in tool_calls:\n",
    "                    try:\n",
    "                        tool_call = json.loads(tool_call.strip().replace(\"'\", '\"'))\n",
    "                        if tool_call['name'] not in tool_names:\n",
    "                            raise NotImplementedError\n",
    "                        seq[-1]['tool_call'] = tool_call\n",
    "                    except Exception as E:\n",
    "                        return {\"conversations\": \"\"}\n",
    "        if seq[-1]['role'] == 'tool':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"<tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"</tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].strip()\n",
    "        # seq[-1]['content'] = d['value']\n",
    "    \n",
    "    random.shuffle(tool_def)\n",
    "    ret = tokenizer.apply_chat_template(seq, tools=tool_def, tokenize=False, add_generation_prompt=False) #+ \"<tool_call>\\n\"\n",
    "    return {\"conversations\": ret}\n",
    "\n",
    "fc_dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")['train']\n",
    "# fc_dataset = fc_dataset.select(range(100))\n",
    "fc_dataset = fc_dataset.map(hermes_fc_thinking)\n",
    "fc_dataset = fc_dataset.filter(lambda x: len(x['conversations']) > 0)\n",
    "print(\"Function calling dataset length (after filter):\", len(fc_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink.\n",
      "\n",
      "# Tools\n",
      "\n",
      "You may call one or more functions to assist with the user query.\n",
      "\n",
      "You are provided with function signatures within <tools></tools> tags:\n",
      "<tools>\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"get_stock_price\", \"description\": \"Get the current stock price of a company\", \"parameters\": {\"type\": \"object\", \"properties\": {\"company\": {\"type\": \"string\", \"description\": \"The name of the company\"}}, \"required\": [\"company\"]}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"get_movie_details\", \"description\": \"Get details about a movie\", \"parameters\": {\"type\": \"object\", \"properties\": {\"title\": {\"type\": \"string\", \"description\": \"The title of the movie\"}}, \"required\": [\"title\"]}}}\n",
      "</tools>\n",
      "\n",
      "You first think/plan inside <think></think> tags.\n",
      "Then for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hi, can you tell me the current stock price of Apple?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\n",
      "</think>\n",
      "<tool_call>\n",
      "{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "{'stock_price': '$150.75'}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "The current stock price of Apple is $150.75.<|im_end|>\n",
      "<|im_start|>user\n",
      "That's great. Can you also tell me the stock price for Microsoft?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\n",
      "\n",
      "Now, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \n",
      "\n",
      "I shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\n",
      "</think>\n",
      "<tool_call>\n",
      "{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "{'stock_price': '$210.22'}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "The current stock price of Microsoft is $210.22.<|im_end|>\n",
      "<|im_start|>user\n",
      "Thank you for the information.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "You're welcome! If you have any other questions, feel free to ask.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fc_dataset[0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/Users/ohi/Documents/GitHub/PersonalAssistant/datasets/merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'thought_len', 'answer_len', 'conversations'],\n",
       "    num_rows: 34125\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    conv = d['conversations']\n",
    "    if conv.count(\"</answer>\") > 2:\n",
    "        print(conv)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since XeTute/Open-Coding-Thoughts couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/ohi/.cache/huggingface/datasets/XeTute___open-coding-thoughts/default/0.0.0/b63d89b184b9048a18e2cd42be298db6e44ab255 (last modified on Sun Mar 16 12:25:21 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1025/1025 [00:00<00:00, 15778.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    Before organizing my code, I should consider the following to avoid mistakes:\\n    *   Make sure to create a clear directory structure to keep related files together.\\n    *   Use a consistent naming convention for your files and variables to avoid confusion.\\n    *   Be mindful of the file path and import statements to avoid errors.\\n    ', \"\\n    Before defining variables, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for variables to avoid confusion.\\n    *   Avoid using magic numbers or hardcoded values, as they can be difficult to maintain.\\n    *   Consider using a preprocessor-specific syntax for variables, such as Sass's `$` symbol or Less's `@` symbol.\\n    \", \"\\n    Before creating mixins and functions, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for mixins and functions to avoid confusion.\\n    *   Avoid over-engineering and keep the code simple and easy to understand.\\n    *   Consider using a preprocessor-specific syntax for mixins and functions, such as Sass's `@mixin` directive or Less's `.extend` method.\\n    \", \"\\n    Before using inheritance and nesting, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for classes and IDs to avoid confusion.\\n    *   Avoid over-nesting and keep the code simple and easy to understand.\\n    *   Consider using a preprocessor-specific syntax for inheritance and nesting, such as Sass's `@extend` directive or Less's `.extend` method.\\n    \", \"\\n    Before using a preprocessor-specific syntax, I should consider the following to avoid mistakes:\\n    *   Make sure to read and understand the documentation for the preprocessor you're using.\\n    *   Avoid using outdated or deprecated features, as they may not be supported in future versions.\\n    *   Consider using a code editor or IDE that provides support for the preprocessor-specific syntax, such as Sass's `scss` file type or Less's `less` file type.\\n    \"]\n",
      "[\"\\n\\n*   When dealing with large-scale projects, it's easy to get overwhelmed by the sheer amount of CSS code.\\n*   Modularizing your CSS code will help you break it down into smaller, manageable chunks.\\n*   This will make it easier to maintain and update individual components without affecting the entire project.\\n*   To modularize your CSS, consider creating separate files for different components or sections of your project.\\n\\n\", \"\\n\\n*   When working on a large team, it's essential to establish a consistent naming convention for your CSS classes.\\n*   This will help prevent conflicts and make it easier for others to understand your code.\\n*   Consider using a naming convention like BEM (Block, Element, Modifier) or OOCSS (Object-Oriented CSS).\\n\\n\", '\\n\\n*   Using broad, generic CSS selectors can lead to unintended styling of other elements on your page.\\n*   Instead, use specific selectors that target the exact elements you want to style.\\n*   Consider using the descendant combinator (` `) to target specific elements within a parent element.\\n\\n', '\\n\\n*   Inline styles can make it difficult to maintain and update your CSS code.\\n*   Instead, define your styles in an external CSS file.\\n*   Use a tool like a CSS preprocessor to help manage your styles.\\n\\n', '\\n\\n*   CSS preprocessors like Sass and Less can help you write more efficient and maintainable CSS code.\\n*   Tools like Gulp and Webpack can automate the concatenation and minification of your CSS files.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n\\n', '\\n\\n*   Testing and debugging your CSS code is crucial to ensuring it works as intended.\\n*   Use tools like the Chrome DevTools or Firefox Developer Edition to test and debug your CSS code.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n\\n', '\\n\\n*   Reviewing and refactoring your CSS code is essential to ensuring it remains maintainable and efficient.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n*   Use a tool like a code analyzer to help identify potential issues with your CSS code.\\n\\n']\n",
      "[\"\\n\\nFirst, I need to design the HTML structure for the table. This will include a header row, a dropdown menu for selecting the number, and the table itself.\\n\\nNext, I need to think about how to link the dropdown menu to the table. I will use JavaScript to update the table when a number is selected from the dropdown menu.\\n\\nI will use CSS to style the table, making it look visually appealing.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start designing the HTML code.\\n\\n\", \"\\n\\nNext, I need to write the CSS code to style the table. This will include setting the table's width and border, and styling the table's rows and columns.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start writing the CSS code.\\n\\n\", \"\\n\\nNext, I need to write the JavaScript code to update the table when a number is selected. This will include getting the selected number from the dropdown menu, creating the table rows and columns, and updating the table's content.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start writing the JavaScript code.\\n\\n\"]\n",
      "[\"\\n\\nTo create a responsive navigation menu, I'll consider the following requirements:\\n\\n*   A navigation menu that adapts to different screen sizes and devices (responsive design)\\n*   A clean and simple layout\\n*   Easy navigation for users with assistive technologies (e.g., screen readers)\\n\\nI'll start by designing a basic HTML structure for the navigation menu and then apply CSS styles to make it responsive.\\n\\n\", \"\\n\\nTo make this navigation menu responsive, I'll use CSS media queries to apply different styles based on screen size. I'll also use a CSS framework (in this case, Bootstrap) to simplify the process.\\n\\nOne potential mistake I might make is not considering screen reader users, who may have difficulty navigating menus with complex layouts. To fix this, I'll use ARIA attributes to provide a clear and consistent navigation experience.\\n\\nAnother potential mistake is not testing the responsiveness of the navigation menu on different devices and browsers. To fix this, I'll use tools like BrowserStack or CodePen to test the menu on various devices and browsers.\\n\\n\", \"\\n\\nTo further improve this design, I could use a CSS framework like Bootstrap to simplify the process of creating a responsive navigation menu. I could also use JavaScript to add interactivity to the menu, such as hover effects or animations.\\n\\nOne potential mistake I might make is not considering the accessibility of the navigation menu. To fix this, I'll use ARIA attributes to provide a clear and consistent navigation experience.\\n\\nAnother potential mistake is not testing the responsiveness of the navigation menu on different devices and browsers. To fix this, I'll use tools like BrowserStack or CodePen to test the menu on various devices and browsers.\\n\\n\"]\n",
      "['\\nFirst, I need to create a basic HTML file structure. I will include a `<script>` tag to link the JavaScript file.\\nNext, I will write the JavaScript code to display a greeting message when the page loads.\\nI need to ensure that the JavaScript code is correct and will not throw any errors.\\nI will also consider the security implications of directly executing JavaScript code from an HTML file. \\n', '\\nI will create a new file called greeting.js and write the JavaScript code to display the greeting message.\\nI will use the `window.onload` event to ensure that the code is executed when the page has finished loading.\\nI will also consider the best practices for coding, such as using a more secure way to execute the JavaScript code.\\n']\n",
      "[\"\\nI need to create a CSS rule that targets an HTML element with the class 'submit-button'. The rule should specify the styles for this element. However, I need to consider that CSS rules are case-sensitive and class selectors are usually defined inside a CSS selector, so I'll need to use a CSS selector that targets the class.\\n\", \"\\nTo ensure I don't make any mistakes, I've double-checked the CSS syntax. The above code defines a CSS rule that targets an HTML element with the class 'submit-button'. The styles defined are for a basic button with a green background color and white text. The padding and border radius are set to 10px and 5px respectively. The cursor is set to a pointing hand to indicate that the button is clickable. The font size is set to 16px. However, I also need to consider that this is a basic example and might need to be adjusted based on the specific requirements of the project.\\n\"]\n",
      "[\"\\n\\nBefore writing the script, I need to consider the following:\\n\\n1. What is the structure of the CSV file? Are there any header rows or specific column names?\\n2. What data manipulation is required? Is it as simple as filtering, sorting, or grouping data or do we need to perform more complex operations like data cleansing or data transformation?\\n3. Are there any specific libraries or dependencies required to read and manipulate the CSV file?\\n4. How should I handle errors or exceptions that may occur during the data manipulation process?\\n\\nAssuming the CSV file is well-structured and the data manipulation is relatively simple, I'll provide a basic script to get us started.\\n\\n\", '\\n\\nTo improve this script, I could consider the following:\\n\\n1. Add error handling mechanisms to handle exceptions that may occur during the data manipulation process.\\n2. Validate the structure of the CSV file to ensure it matches the expected format.\\n3. Provide more complex data manipulation operations, such as data cleansing or transformation.\\n4. Consider using more advanced libraries or tools to handle large-scale data manipulation tasks.\\n\\n']\n",
      "['\\n\\n*   The DOCTYPE declaration must be included at the beginning of the file.\\n*   The HTML tag is the root element of the HTML document.\\n*   The head section is used for the metadata of the document, such as the title, styles, and scripts.\\n*   The body section is where the content of the HTML document is placed.\\n*   In the head section, a link tag is used to link the external CSS file.\\n*   A script tag is used to link the external JavaScript file.\\n\\n', '\\n\\n*   The CSS style block is linked in the head section of the HTML file.\\n*   The CSS style block must have the same name as the linked file, in this case, style.css.\\n*   The body tag in CSS is used to select the HTML element with the tag name body.\\n*   The background-color property is used to set the background color of the element.\\n\\n', '\\n\\n*   The JavaScript function is linked in the body section of the HTML file.\\n*   The JavaScript function must be placed inside a script tag.\\n*   The console.log() function is used to output messages to the console.\\n*   The message to be logged must be placed inside the console.log() function.\\n\\n']\n",
      "[\"\\n\\nFirst, I need to determine the best way to simulate a coin toss. Since C++ does not have a built-in random number generator, I can use the `rand()` function to generate a random number. However, `rand()` is not recommended for generating random numbers for security or statistical purposes, so I will use the `random` library which is more secure.\\n\\nNext, I need to decide how to use the generated random number to determine the outcome of the coin toss. I can use a simple if statement to check if the number is even or odd, and return 'heads' or 'tails' accordingly.\\n\\nHowever, I also need to consider the potential mistakes I could make. One mistake could be to forget to include the necessary headers for the `random` library. Another mistake could be to not check if the random number generator is actually generating a random number.\\n\\n\", '\\n\\nA better way to simulate a coin toss would be to use the XOR shift algorithm to generate a random number. This algorithm is more complex than a simple if statement, but it is also more secure and more unpredictable.\\n\\nTo use the XOR shift algorithm, I need to generate a random number using the `random` library, and then use that number to calculate a new number using the XOR shift formula. I can then use that new number to determine the outcome of the coin toss.\\n\\nHowever, I also need to consider the potential mistakes I could make. One mistake could be to forget to include the necessary headers for the `random` library. Another mistake could be to not check if the random number generator is actually generating a random number.\\n\\n']\n",
      "[\"\\n\\nTo solve this problem, we need to:\\n\\n1. Import a library that will allow us to generate HTML. We'll use `string` and `template` for this, although it's worth noting that using Jinja2 or a similar templating engine would be a more conventional approach.\\n\\n2. Define a function that will take in the title and generate the HTML.\\n\\n3. Ensure that we're handling any potential errors that might occur during this process.\\n\\n\", \"\\n\\nIn this script, we use the `string` module to check if the title is a string and to remove any non-ASCII characters from it. We then define a function `generate_html` that takes in the title and returns the generated HTML.\\n\\nThe function first checks if the title is a string. If it's not, it raises a `TypeError`. It then removes any non-ASCII characters from the title to avoid encoding issues. Finally, it generates the HTML using an f-string and returns it.\\n\\nIn the example usage, we define a title and pass it to the `generate_html` function. The generated HTML is then printed out.\\n\\nMistakes I could make:\\n\\n* Not checking if the title is a string, which could lead to a `TypeError` or other errors.\\n* Not removing non-ASCII characters from the title, which could lead to encoding issues.\\n* Not handling potential errors that might occur during the generation of the HTML.\\n\\nTo fix these mistakes, I would:\\n\\n* Add a type check for the title to ensure it's a string.\\n* Remove non-ASCII characters from the title to avoid encoding issues.\\n* Handle potential errors that might occur during the generation of the HTML, such as raising a `TypeError` or other exceptions.\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1025/1025 [00:00<00:00, 13955.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenThought dataset length (after filter): 1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def openthought_code(data):\n",
    "    thought_len, answer_len = 0, 0\n",
    "\n",
    "    reply = data['output']\n",
    "    thought = re.findall(r\"<thoughts>(.*?)</thoughts>\", reply, re.DOTALL)\n",
    "    if len(thought) > 1:\n",
    "        print(thought)\n",
    "    thought = ''.join(thought).strip()\n",
    "    thought_len += len(thought.split())\n",
    "    \n",
    "    end_tag = \"</thoughts>\"\n",
    "    answer = reply[reply.find(end_tag)+len(end_tag):]\n",
    "    answer = answer.strip()\n",
    "    answer_len += len(answer.split()) #len(tokenizer.encode(answer))\n",
    "\n",
    "    if end_tag not in reply:\n",
    "        answer_len = 0\n",
    "\n",
    "    final_answer = f\"<think>\\n{thought}\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n",
    "    final_answer = final_answer.replace(\"<thoughts>\", \"\").replace(\"</thoughts>\", \"\")\n",
    "    \n",
    "    output_data = {\n",
    "        'thought_len': thought_len,\n",
    "        'answer_len': answer_len,\n",
    "        'conversations': [\n",
    "            {\"role\": \"user\", 'content': data['input']},\n",
    "            {\"role\": \"assistant\", 'content': final_answer}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return output_data\n",
    "\n",
    "openthought_dataset = load_dataset(\"XeTute/Open-Coding-Thoughts\")['train']\n",
    "print(\"Dataset length:\", len(openthought_dataset))\n",
    "openthought_dataset = openthought_dataset.map(openthought_code)\n",
    "openthought_dataset = openthought_dataset.map(lambda x: {\"conversations\": tokenizer.apply_chat_template(x['conversations'], tools=None, tokenize=False)})\n",
    "print(\"OpenThought dataset length (after filter):\", len(openthought_dataset))\n",
    "# print(openthought_dataset[0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 8133/8133 [00:08<00:00, 984.77 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"open-r1/codeforces-cots\", \"solutions_py_decontaminated\")['train']\n",
    "\n",
    "def take(d):\n",
    "    return len(d['generation'].split()) <= 2000\n",
    "\n",
    "ds = ds.filter(take)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to solve this problem where I have to convert a given integer n into its binary representation without leading zeros. Let's think about how to approach this.\n",
      "\n",
      "The problem says that n can be up to 1e6. Wait, 1e6 isn't that big. But even so, I need an efficient way to convert it to binary. But what's the standard way to do this in Python?\n",
      "\n",
      "Hmm, Python has built-in functions for this. Like bin(n), which returns a string starting with '0b', so I can just slice that off. For example, bin(5) gives '0b101', so removing the first two characters gives '101'. That's exactly what the examples show. So for the input 5, output is 101. So using bin() seems straightforward.\n",
      "\n",
      "But wait, the problem says to output without leading zeros. But bin() automatically doesn't have leading zeros except for the case when n is 0, but since n is a positive integer (as per the problem statement), n can't be zero. So using bin(n)[2:] should give the correct result.\n",
      "\n",
      "So the code would be something like:\n",
      "\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "\n",
      "That's really simple. But is there any case where this might not work? Like when n is 1? Let's see. bin(1) is '0b1', so [2:] is '1', which is correct.\n",
      "\n",
      "What about edge cases like n=1e6? Let's compute 1e6 in binary. Well, bin(1000000) is '0b11110100001001000000', which when sliced gives the correct binary.\n",
      "\n",
      "This approach is O(1) time because converting an integer to binary in Python is done efficiently, even for large numbers. Since the maximum n is 1e6, which is manageable, this method should work within the time constraints. The built-in functions are optimized in C, so they're much faster than any manual implementation.\n",
      "\n",
      "Another approach would be to manually compute the binary digits by dividing n by 2 each time and collecting the remainders. But that's more code and possibly less efficient. For example:\n",
      "\n",
      "res = []\n",
      "while n > 0:\n",
      "    res.append(str(n % 2))\n",
      "    n = n // 2\n",
      "res.reverse()\n",
      "print(''.join(res))\n",
      "\n",
      "But this would also work and has O(log n) time complexity, which is acceptable. But why write all that code when Python's built-in does it more efficiently?\n",
      "\n",
      "So the optimal approach is to use bin() and slice off the first two characters. It's the fastest and most memory-efficient way. The code is straightforward and will handle all test cases within the constraints.\n",
      "\n",
      "Testing the examples:\n",
      "\n",
      "For input 5: bin(5)[2:] → '101', correct.\n",
      "\n",
      "For input 13: bin(13)[2:] → '1101', correct.\n",
      "\n",
      "So the code should work.\n",
      "</think>\n",
      "\n",
      "```python\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(ds[0]['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is 16*8?\n",
      "Assistant: 16 * 8 = 128\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\n",
      "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(processor.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<|im_start|>{% for message in messages %}\n",
    "  {{message['role'] | capitalize}}\n",
    "  {% if message['content'][0]['type'] == 'image' %}{{':'}}\n",
    "  {% else %}{{': '}}\n",
    "  {% endif %}\n",
    "  {% for line in message['content'] %}\n",
    "    {% if line['type'] == 'text' %}{{line['text']}}\n",
    "    {% elif line['type'] == 'image' %}{{ '<image>' }}\n",
    "    {% endif %}\n",
    "  {% endfor %}<end_of_utterance>\n",
    "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\"Muhammad Yunus[a] (born 28 June 1940) is a Bangladeshi economist, businessman, and politician who has been serving as Chief Adviser of the Interim Government of Bangladesh since 8 August 2024.[1] Yunus was awarded the Nobel Peace Prize in 2006 for founding the Grameen Bank and pioneering the concepts of microcredit and microfinance.[2] Yunus has received several other national and international honors, including the United States Presidential Medal of Freedom in 2009 and the Congressional Gold Medal in 2010.[3]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Search Results\n",
      "\n",
      "[List of presidents of Russia - Wikipedia](https://en.wikipedia.org/wiki/List_of_presidents_of_Russia)\n",
      "The office of the president of Russia is the highest authority in the Russian Federation.The holder is the federation's head of state and has formal presidency over the State Council as well as being the commander in chief of the Russian Armed Forces.The office was introduced in 1918 after the February Revolution with the current office emerging after a referendum of 1991. [1]\n",
      "\n",
      "[Vladimir Putin - Wikipedia](https://en.wikipedia.org/wiki/Vladimir_Putin)\n",
      "Putin, FIFA president Gianni Infantino and French president Emmanuel Macron at the 2018 FIFA World Cup Final in Russia as French forward Kylian Mbappé receives the best young player award In 2007, Putin led a successful effort on behalf of Sochi for the 2014 Winter Olympics and the 2014 Winter Paralympics , [ 469 ] the first Winter Olympic Games to ever be hosted by Russia.\n",
      "\n",
      "[President of Russia - Wikipedia](https://en.wikipedia.org/wiki/President_of_Russia)\n",
      "The president of Russia, officially the president of the Russian Federation (Russian: Президент Российской Федерации, romanized: Prezident Rossiyskoy Federatsii), is the executive head of state of Russia.The president is the chair of the Federal State Council and the supreme commander-in-chief of the Russian Armed Forces.It is the highest office in Russia.\n",
      "\n",
      "[Vladimir Putin | Biography, KGB, Political Career, & Facts | Britannica](https://www.britannica.com/biography/Vladimir-Putin)\n",
      "Vladimir Putin (born October 7, 1952, Leningrad, Russia, U.S.S.R. [now St. Petersburg, Russia]) is a Russian intelligence officer and politician who has served as president (1999-2008 and 2012- ) of Russia and as the country's prime minister (1999 and 2008-12). One of the 21st century's most influential leaders, Putin has shaped his country's political landscape for decades with a ...\n",
      "\n",
      "[Biographies of presidents of Russia - President of Russia](http://www.en.kremlin.ru/structure/president/presidents)\n",
      "Since May 8, 2008, Vladimir Putin is a Prime Minister of Russia. 2012 On March 4, 2012, he was elected President of Russia and inaugurated on May 7, 2012. 2018 On March 18, 2018, he was re-elected President of Russia. Assumed office on May 7, 2018. 2024 Won Russia's presidential election held on March 15-17, 2024.\n",
      "\n",
      "[Who has power in Russia president or prime minister?](https://www.ncesc.com/geographic-faq/who-has-power-in-russia-president-or-prime-minister/)\n",
      "The current president of Russia is Vladimir Putin. He has held positions as both president and prime minister since 1999. Is Putin a prime minister or a president? Vladimir Putin has served as both president and prime minister of Russia. Since 1999, he has continuously held these positions, serving as prime minister from 1999 to 2000 and from ...\n",
      "\n",
      "[Persons ∙ Directory ∙ President of Russia](http://en.kremlin.ru/catalog/persons/54/biography)\n",
      "On March 14, 2004, he was elected President of Russia for the second term. 2008 Since May 8, 2008, Vladimir Putin is a Prime Minister of Russia. 2012 On March 4, 2012, he was elected President of Russia and inaugurated on May 7, 2012. 2018 On March 18, 2018, he was re-elected President of Russia. Assumed office on May 7, 2018. 2024\n",
      "\n",
      "[President ∙ Structure ∙ President of Russia](http://en.kremlin.ru/structure/president)\n",
      "President of Russia The President of the Russian Federation is the head of state and guarantor of the Constitution and of human and civil rights and freedoms. The President is elected for a 6-year term by the Russian Federation's citizens on the basis of universal, equal and direct suffrage by secret ballot.\n",
      "\n",
      "[Vladimir Putin: Who is the Russian President, and what does he want? - BBC](https://www.bbc.co.uk/newsround/60521909)\n",
      "Vladimir Putin is the President of Russia, and has been the country's leader for more than 22 years. He has been in the news a lot recently because of his decision to invade the country of Ukraine.\n",
      "\n",
      "[Putin inaugurated as president for fifth term with Russia under ... - CNN](https://www.cnn.com/2024/05/07/europe/putin-inauguration-russia-president-fifth-term-intl/index.html)\n",
      "Vladimir Putin has formally begun his fifth term as Russia's president in a carefully choreographed inauguration ceremony, in a country he has shaped in his image after first taking office ...\n"
     ]
    }
   ],
   "source": [
    "from smolagents import DuckDuckGoSearchTool\n",
    "\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "print(search_tool(\"Who's the current president of Russia?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 251/251 [00:00<00:00, 3643.22 examples/s]\n",
      "Map: 100%|██████████| 251/251 [00:00<00:00, 4417.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General reason dataset length: 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process(data):\n",
    "    for idx, message in enumerate(data['messages']):\n",
    "        if message['role'] != 'assistant': continue\n",
    "        content = message['content']\n",
    "        tag = \"</think>\"\n",
    "        pos = content.find(tag)\n",
    "        answer = content[pos+len(tag):].strip()\n",
    "        data['messages'][idx]['content'] = content[:pos].strip() + f\"\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n",
    "    return data\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "codeforces_cot = load_dataset(\"open-r1/codeforces-cots\", \"solutions_py_decontaminated\")['train']#.select(range(500))\n",
    "codeforces_cot = codeforces_cot.filter(lambda x: len(str(x['messages'])) < 8000)\n",
    "delete_keys = list(codeforces_cot.column_names)\n",
    "codeforces_cot = codeforces_cot.map(process)\n",
    "codeforces_cot = codeforces_cot.map(lambda x: {\"conversations\": tokenizer.apply_chat_template(x['messages'], tools=None, tokenize=False)})\n",
    "codeforces_cot = codeforces_cot.remove_columns(delete_keys)\n",
    "print(\"Codeforces CoT dataset length:\", len(codeforces_cot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
      "Respond in the following format:\n",
      "<think>\n",
      "Let's think step by step...\n",
      "</think>\n",
      "<answer>\n",
      "The final answer is...\n",
      "</answer><|im_end|>\n",
      "<|im_start|>user\n",
      "You will be given a competitive programming problem.\n",
      "Analyze the maximum input constraints and identify the optimal algorithmic approach and data structures needed to process the largest possible test cases within the time and memory limits, then explain why your chosen implementation strategy is the most efficient solution. Please reason step by step about your solution approach, then provide a complete implementation in Python 3 that is thoroughly optimized for both speed and memory usage.\n",
      "\n",
      "Your solution must read input from standard input (input()), write output to standard output (print()).\n",
      "Do not include any debug prints or additional output.\n",
      "\n",
      "Put your final solution within a single code block:\n",
      "```python\n",
      "<your code here>\n",
      "```\n",
      "\n",
      "# Problem\n",
      "\n",
      "You are given a positive integer n. Output its binary notation.\n",
      "\n",
      "## Constraints\n",
      "Time limit per test: 2.0 seconds\n",
      "Memory limit per test: 256.0 megabytes\n",
      "\n",
      "## Input Format\n",
      "The only line of input data contains an integer n (1 ≤ n ≤ 106).\n",
      "\n",
      "## Output Format\n",
      "Output the binary notation of n (without any leading zeros).\n",
      "\n",
      "## Examples\n",
      "```input\n",
      "5\n",
      "```\n",
      "```output\n",
      "101\n",
      "```\n",
      "-----\n",
      "```input\n",
      "13\n",
      "```\n",
      "```output\n",
      "1101\n",
      "```\n",
      "\n",
      "## Note\n",
      "In the first example 5 = 1 * 22 + 0 * 21 + 1 * 20.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, I need to solve this problem where I have to convert a given integer n into its binary representation without leading zeros. Let's think about how to approach this.\n",
      "\n",
      "The problem says that n can be up to 1e6. Wait, 1e6 isn't that big. But even so, I need an efficient way to convert it to binary. But what's the standard way to do this in Python?\n",
      "\n",
      "Hmm, Python has built-in functions for this. Like bin(n), which returns a string starting with '0b', so I can just slice that off. For example, bin(5) gives '0b101', so removing the first two characters gives '101'. That's exactly what the examples show. So for the input 5, output is 101. So using bin() seems straightforward.\n",
      "\n",
      "But wait, the problem says to output without leading zeros. But bin() automatically doesn't have leading zeros except for the case when n is 0, but since n is a positive integer (as per the problem statement), n can't be zero. So using bin(n)[2:] should give the correct result.\n",
      "\n",
      "So the code would be something like:\n",
      "\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "\n",
      "That's really simple. But is there any case where this might not work? Like when n is 1? Let's see. bin(1) is '0b1', so [2:] is '1', which is correct.\n",
      "\n",
      "What about edge cases like n=1e6? Let's compute 1e6 in binary. Well, bin(1000000) is '0b11110100001001000000', which when sliced gives the correct binary.\n",
      "\n",
      "This approach is O(1) time because converting an integer to binary in Python is done efficiently, even for large numbers. Since the maximum n is 1e6, which is manageable, this method should work within the time constraints. The built-in functions are optimized in C, so they're much faster than any manual implementation.\n",
      "\n",
      "Another approach would be to manually compute the binary digits by dividing n by 2 each time and collecting the remainders. But that's more code and possibly less efficient. For example:\n",
      "\n",
      "res = []\n",
      "while n > 0:\n",
      "    res.append(str(n % 2))\n",
      "    n = n // 2\n",
      "res.reverse()\n",
      "print(''.join(res))\n",
      "\n",
      "But this would also work and has O(log n) time complexity, which is acceptable. But why write all that code when Python's built-in does it more efficiently?\n",
      "\n",
      "So the optimal approach is to use bin() and slice off the first two characters. It's the fastest and most memory-efficient way. The code is straightforward and will handle all test cases within the constraints.\n",
      "\n",
      "Testing the examples:\n",
      "\n",
      "For input 5: bin(5)[2:] → '101', correct.\n",
      "\n",
      "For input 13: bin(13)[2:] → '1101', correct.\n",
      "\n",
      "So the code should work.\n",
      "</think>\n",
      "<answer>\n",
      "```python\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "```\n",
      "</answer><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(codeforces_cot[0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: smolagents 1.12.0\n",
      "Uninstalling smolagents-1.12.0:\n",
      "  Would remove:\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/bin/smolagent\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/bin/webagent\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/smolagents-1.12.0.dist-info/*\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/smolagents/*\n",
      "Proceed (Y/n)? ^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 Best CI/CD Tools That You Should Know - GeeksforGeeks\n",
      "\n",
      "Skip to content\n",
      "CoursesDSA to DevelopmentMachine Learning & Data ScienceGenerative AI & ChatGPTBecome AWS CertifiedDSA CoursesData Structure & Algorithm(C++/JAVA)Data Structure & Algorithm(Python)Data Structure & Algorithm(JavaScript)Programming LanguagesCPPJavaPythonJavaScriptCAll CoursesTutorialsPythonPython TutorialPython ProgramsPython QuizPython ProjectsPython Interview QuestionsPython Data StructuresJavaJava TutorialJava CollectionsJava 8 TutorialJava ProgramsJava QuizJava ProjectsJava Interview QuestionsAdvanced JavaProgramming LanguagesJavaScriptC++R TutorialSQLPHPC#CScalaPerlGo LanguageKotlinInterview CornerSystem Design TutorialCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsComputer Science SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps and LinuxDevOps TutorialGITAWSDockerKubernetesMicrosoft Azure TutorialGoogle Cloud PlatformLinux TutorialSoftware TestingSoftware Testing TutorialSoftware Engineering TutorialTesting Interview QuestionsJiraDatabasesDBMS TutorialSQL TutorialPostgreSQL TutorialMongoDB TutorialSQL Interview QuestionsMySQL Interview QuestionsPL/SQL Interview QuestionsAndroidAndroid TutorialAndroid Studio TutorialKotlin For AndroidAndroid ProjectsAndroid Interview Questions6 Weeks of Android App DevelopmentExcelMS Excel TutorialIntroduction to MS ExcelData Analysis in ExcelBasic Excel Formulas & FunctionsData Analysis in Advanced ExcelWorkbooksStatistical FunctionsData Visualization in ExcelPivot Tables in ExcelExcel Spreadsheets in PythonBasic Excel ShortcutsMathematicsNumber SystemAlgebraLinear AlgebraTrigonometrySet TheoryStatisticsProbabilityGeometryMensurationLogarithmsCalculusDSADSA TutorialPracticePractice Coding ProblemsProblem of the DayGfG SDE SheetCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetTop Interview QuestionsPuzzlesAll PuzzlesTop 100 Puzzles Asked In InterviewsTop 20 Puzzles Commonly Asked During SDE InterviewsData SciencePython TutorialR TutorialMachine LearningData Science using PythonData Science using RData Science PackagesPandas TutorialNumPy TutorialData VisualizationPython Data Visualization TutorialData Visualization with RData AnalysisData Analysis with PythonData Analysis with RDeep LearningNLP TutorialWeb TechHTML TutorialCSS TutorialJavaScript TutorialPHP TutorialReactJS TutorialNodeJS TutorialBootstrap TutorialTypescriptWeb Development Using PythonDjango TutorialFlask TutorialPostmanGithubCheat SheetsHTML Cheat SheetCSS Cheat SheetJavaScript Cheat SheetReact Cheat SheetAngular Cheat SheetjQuery Cheat SheetBootstrap Cheat SheetLearn Complete Web Development\n",
      "DevOps LifecycleDevOps RoadmapDocker TutorialKubernetes TutorialsAmazon Web Services [AWS] TutorialAZURE TutorialsGCP TutorialsDocker Cheat sheetKubernetes cheat sheetAWS interview questionsDocker Interview QuestionsAnsible Interview QuestionsJenkins Interview Questions \n",
      "\n",
      "▲\n",
      "\n",
      "Open In App\n",
      "\n",
      "Next Article:\n",
      "Understanding Deployment Automation\n",
      "15 Best CI/CD Tools That You Should Know\n",
      "Last Updated : \n",
      "18 Sep, 2024\n",
      "\n",
      "Summarize\n",
      "\n",
      "Comments\n",
      "Improve\n",
      "Suggest changes\n",
      "\n",
      "Like Article\n",
      "Like\n",
      "\n",
      "Share\n",
      "Report\n",
      "Follow\n",
      "Continuous Integration and Continuous Delivery (CI/CD) is modern-day practices to continuously improve the software by adding new feature and for that we have to change the existing files and deploy them. Doing it manually every time may consume a lot of developer's time, so to make the process more simple and reliable developers use CI/CD tools that are much more efficient and make the whole process automated. As a software Engineer or beginner in development, you should know the best CI/CD tools.As you go deep into software development, you will ultimately need these tools to save time and release a more reliable and stable version of your software application with proper automated testing and deploying processes. But before knowing the tools let’s first under what exactly is CI and CD.What is CI/CD?CI and CD stand for continuous Integration and are continuous Deployment/Delivery which is used in the process of building efficient software. These are the approaches that are used to integrate the code changes into a shared repository (Continuous Integration) while automating the testing and deployment process to the staging and production environment (Continuous Delivery). These approaches are used to provide a more reliable and stable version of a software application by testing it before release to end-users with the help of CI/CD Tools. To learn more about CI/CD, refer to this article- Continuous Integration and Continuous Delivery15 Best CI/CD Tools For DevelopersContinuous Integration and Continuous Delivery are not only necessary to build a great application but also help the business in meeting its various requirements, from automating tasks to receiving quick feedback, so that developers can improve the application accordingly. They are both combined known as continuous software development. In this process of developing efficient applications, developers have created some CI/CS Tools that help the software engineers test the software application effectively. Here are some of the best CI/CD tools that you should know1. JenkinsJenkins is one of the most popular and effective CI/CD tools which is an open-source automation server. It provides several functionalities like automation testing, deploying code changes for smooth collaboration among the teams of product development, and faster identification of issues. It has a feature “Pipeline as Code” which allows developers to manage their deployment pipelines programmatically. It has a master-slave architecture which is very useful in their efficient resource usage. It also has a large community support to help new developers. To enhance the quality of software and increase the speed of delivery of applications, Jenkins has played a very important role.To learn more, refer to - JenkinsFeatures:Extensibility with PluginsPipeline as CodeDistributed Builds2. Travis CIFor the automation process of building, testing, and deploying software applications there are various CI/CD tools in the market but among those Travis CI is one of the most popular and useful tools that software developers prefer over other tools due to its crucial role in the software development lifecycle by checking the reliability and quality of code changes before integrating into the production codebase. Automatic building and testing of code during push commands and support for various languages and frameworks are some of the important factors that differentiate these CI/CD tools from the others. All of its feature ensures smooth development workflows and faster feedback, so Travis CI enables collaboration decrease integration issues, and help the developers build more high-quality software applications.To learn more, refer to - Travis CIFeatures:GitHub IntegrationBuild MatrixCaching Dependencies3. GitLabGitLab is a web-based platform that provides multiple tools and functionalities to developers to provide help in the process of the software development lifecycle. It is one of the most common CI/CD tools that is used by the developers which provide several useful functionalities like collaborative project management, continuous integration and continuous deployment pipelines, code review, and issue tracking with the help of version control through Git repositories. GitLab allows development teams to manage the collaborative project in real-time and track progress within an integrated environment. It is very flexible for various development workflows and is available in both private servers and as a cloud-hosted service.To learn more, refer to - GitLabFeaturesIntegrated DevOps PlatformContainer Registry and Kubernetes IntegrationBuilt-in Issue Tracking and Collaboration4. DockerDocker is a software platform that is used in the process of software development by providing special functionalities such as building, testing, and deploying software applications. Docker’s libraries, system tools, code, and runtime are stored in standard units called containers. The software that hosts the container is known as Docker Engine. It offers both free and premium services that completely depend on the use case. To use the resources efficiently, Docker uses OS-level virtualization. The application and its dependencies can be packaged into a portable format that can run on any system that supports Docker.To learn more, refer to - DockerFeaturesContainerization TechnologyImage-Based DeploymentIsolation and Resource Efficiency5. TeamCityOne of the widely used CI/CD server tools is TeamCity which is developed by JetBrains. Since it is a CI/CD tool then definitely is used for the automation of building, testing, and deploying software applications. And one of the most helpful parts of it is that TeamCity has a very user-friendly web interface, It can easily integrate with popular version control systems like Git and Subversion, also integrate with cloud platforms, and has the ability to trigger builds based on code commits. Projects of different sizes and complexities can easily use these tools due to their flexibility and extensibility.To learn more, refer to - TeamCityFeaturesCross-Platform CompatibilityBuild Chains and DependenciesAdvanced Build and Deployment Workflows6. SemaphoreSemaphore is one of the fastest-growing CI/CD tools for DevOps in the software industry that allows you to deploy various software updates every day on platforms like Linux, Android, and macOS with no barriers. Semaphore allow teams to deploy applications through customizable pipelines triggered by code changes. It also automates the process of build, testing, and deploying the software application process with a continuous feedback loop to improve the application continuously to make it a stable and reliable application. This CI/CD tool is an example of an amazing tool that allows an end-to-end development process.FeaturesYAML-Based ConfigurationParallel and Distributed TestingContinuous Delivery Pipelines7. Circle CIWhen it comes to using cloud-based CI/CD tools that automate and optimize the software development lifecycle, Circle CI is the very first tool that developers prioritize. CircleCI supports container-based builds for a consistent environment with parallel and distributed testing and an advanced caching mechanism to speed up build times. It also supports multiple programming languages and frameworks, allowing development teams to customize their workflows on the cloud platform itself. CircleCI helps in debugging early in the development process and ensures that the code quality is good to make reliable software products.FeaturesOrbs for Reusable ConfigurationWorkspaces for Data SharingCloud-Based and Self-Hosted Option8. BambooBamboo is a continuous integration and continuous delivery (CI/CD) tool which is developed by Atlassian. The purpose behind the Bamboo tools is to make the software development process very simple by automating the various stages such as building, testing, and deploying the code changes. Development teams can create pipelines that automatically trigger code commits when necessary and help in integration and collaboration. Bamboo helps product development teams with faster feedback loops and improves their code quality to make stable releases of software applications with the help of automated CI/CD pipelines.FeaturesIntegration with Atlassian EcosystemDocker and Container SupportJIRA Integration and Release Management9. BuddyBuddy is a CI/CD tool and is one of the modern and user-friendly platforms to automate software development workflows. To set up a complex workflow, Buddy provides us with a visual pipeline builder that makes it easy to visualize the workflow and it also has built-in Docker support for containerized builds and deployments. It is also integrated with various cloud providers. It is very suitable for every development team size due to its flexible and intuitive interface and extensive features.FeaturesPipeline as CodeDeployment AutomationServerless Actions10. Azure DevOps ServerAzure DevOps server is a CI/CD tool which is created by Microsoft that provides various functionalities such as version control, reporting, requirements management, project management, testing, and several other capabilities. It was formerly known as Team Foundation Server (TFS). It is a cloud-based service that is much more suitable for an organization that requires physical hosting due to security, compliance, or any other reasons. Since it is a product of Microsoft, it integrates well with other Microsoft products as well. It supports the entire software development lifecycle by providing tools for version control, agile planning, and CI/CD.To learn more, refer to - Azure DevOps serverFeaturesCustomizable Work Items and BoardsBuild and Release PipelinesReporting and Analytics11. BitriseBitrise is one of the top cloud-based CI/CD tool that is primarily used for the complete automation process of mobile-first software. Since it is a clud-based software tool, so there is no need to spend more on the physical infrastructure and hardware. Bitrise support all the major programming language used in mobile app development. Bitrise works very smoothly with the public, private, and ad-hoc  Git services. With it's parallel and distributed builds, developers can increase their speed of completing the process by splitting tasks across multiple agents or threads.FeaturesMobile-centric AutomationVisual Workflow EditorParallel and Distributed Builds12. CodeShipCodeShip is a CI/CD tool by CloudBees that helps the developers to automate the complete process of application development, from building to deployment. CodeShip has multiple helpful features like integration with version control systems like GitHub and Bitbucket, container support and notification alert. It has very user-friendly interface, and with its focus on simplicity, CodeShip aims to simplify the development process and increase the efficiency in software development lifecycle. It also has basic and pro paid versions, that you can use according to your specific requirements.FeaturesMulti-Environment DeploymentNotification and AlertsProject Lifecycle Management13. SpinnakerSpinnaker is a free and open-source, multi-cloud continuous delivery software platform. It was originally developed by the engineers of Netflix and further extended many features by Google. It is designed to work with other software that help in the process of software development such as Kubernetes. Since it is a tool that helps in the lifecycle of software development, it definitely provides functionalities such as automating system tests, managing rollouts, and triggering pipelines.FeaturesMulti-Cloud DeploymentBuilt-in Rollback and RollforwardAutomated Canary Analysis14. BuildbotBuildbot is an open-source CI/CD tool that focuses on the automation of complex testing and application deployment processes. It is based on Python. It started with a lightweight alternative to the Mozilla project’s Tinderbox, and now it is used by several others like Python, WebKit, and ReactOS. Buildbot follows a more decentralized and configurable architecture that allows users to define their build pipelines using scripts that are based on Python. Buildbots are generally suitable for those who need deep customizability and have specific requirements in their CI/CD workflows.FeaturesHighly CustomizableDistributed BuildsReal-time Monitoring and Logging15. WreckerWrecker is a CI/CD platform that is based on Docker that helps software developers build and deploy their software applications and microservices. It was acquired by Oracle. Wrecker is totally based on the concepts of Pipeline that automatically executes the code by taking it from the program. It Provides programmatic access to information about applications, builds, and deploys that speed up the development process. It triggers the pipelines in case any new code is committed and can be integrated with Git and other popular notification systems.FeaturesGit Integrations with various platformsDebug and test pipelines before deployingConclusionContinuous Integration and Continuous Deployment is an integral part of the software development process and helps the development teams in building reliable and continuously evolving software with continuous feedback loops. It is necessary to debug the code and receive the feedback after better testing and at the same time deploy the updates when ready, so to automate these processes, developers have built these CI/CD tools. Each tool has its unique features and functionalities which are capable of doing different actions according to the requirements of the team and project. Also, you can customize your specific requirements with different tools but for more premium features, you have to pay extra for it. FAQs on Best CI/CD Tools1. What are the top CI/CD tools?These are some of the top CI/CD tools that are currently trending among developers:1. JenKins2. Docker3. GitLab4. Microsoft Azure5. Wrecker2. Who uses CI/CD Tools?DevOps engineers use CI/CD tools such as Jenkins and GitLab to automate the process of Software Development.3. What is CI/CD, and why is it important in software development?CI/CD stands for Continuous Integration and Continuous Deployment (or Continuous Delivery). It automate and streamline the process of building, testing, and deploying code changes to production environments in software development4. How to choose the right CI/CD tool for my project?Choosing the right CI/CD tool for project is depend upon various factor such as project requirement, existing tool, and end goal of project etc.\n",
      "\n",
      "        Comment\n",
      "    More info\n",
      "Advertise with us\n",
      "Next Article\n",
      "\n",
      "Understanding Deployment Automation\n",
      "H\n",
      "harshintotfv \n",
      " Follow \n",
      "Improve\n",
      "\n",
      "Article Tags : \n",
      "\n",
      "GBlog\n",
      "\n",
      "DevOps\n",
      "\n",
      "Tools\n",
      " \n",
      "Similar Reads\n",
      "\n",
      "DevOps Tutorial\n",
      "                    \n",
      "DevOps is a collection of two words, \"Development\" and \"Operations,\" representing a cultural approach that emphasizes collaboration between development and operations teams to streamline the entire software delivery lifecycle. This DevOps tutorial is designed to help learn DevOps basics and advanced\n",
      "9 min read\n",
      "Introduction\n",
      "What is DevOps ?\n",
      "                    \n",
      "As technology keeps advancing, businesses need to deliver software faster and more reliably. DevOps helps achieve this by bringing together development and operations teams, ensuring they work as one instead of in separate silos. DevOps is more than just a set of toolsâ€”itâ€™s a way of working that pro\n",
      "12 min read\n",
      "\n",
      "DevOps Lifecycle\n",
      "                    \n",
      "DevOps is a practice that enables a single team to handle the whole application lifecycle, including development, testing, release, deployment, operation, display, and planning. It is a mix of the terms \"Dev\" (for development) and \"Ops\" (for operations). We can speed up the delivery of applications\n",
      "6 min read\n",
      "\n",
      "The Evolution of DevOps - 3 Major Trends for Future\n",
      "                    \n",
      "DevOps is a software engineering culture and practice that aims to unify software development and operations. It is an approach to software development that emphasizes collaboration, communication, and integration between software developers and IT operations. DevOps has come a long way since its in\n",
      "7 min read\n",
      "Version Control\n",
      "Version Control Systems\n",
      "                    \n",
      "Version Control Systems (VCS) are essential tools used in software development and collaborative projects to track and manage changes to code, documents, and other files. Whether you're working alone or as part of a team, version control helps ensure that your work is safe, organized, and easy to co\n",
      "7 min read\n",
      "\n",
      "Merge Strategies in Git\n",
      "                    \n",
      "Understanding merge strategies in Git can simplify your workflow, resolve conflicts efficiently, and ensure seamless collaboration among team members. In this article, we'll explore various merge strategies, their use cases, and best practices for effective branch management. What is a Merge in Git?\n",
      "4 min read\n",
      "\n",
      "Which Version Control System Should I Choose?\n",
      "                    \n",
      "While building a project, you need a system wherein you can track the modifications made. That's where Version Control System comes into the picture. It came into existence in 1972 at Bell Labs. The very first VCS made was SCCS (Source Code Control System) and was available only for UNIX. When any p\n",
      "5 min read\n",
      "Continuous Integration (CI) & Continuous Deployment (CD)\n",
      "What is CI/CD?\n",
      "                    \n",
      "CI/CD is the practice of automating the integration of code changes from multiple developers into a single codebase. It is a software development practice where the developers commit their work frequently to the central code repository (Github or Stash). Then there are automated tools that build the\n",
      "8 min read\n",
      "\n",
      "15 Best CI/CD Tools That You Should Know\n",
      "                    \n",
      "Continuous Integration and Continuous Delivery (CI/CD) is modern-day practices to continuously improve the software by adding new feature and for that we have to change the existing files and deploy them. Doing it manually every time may consume a lot of developer's time, so to make the process more\n",
      "11 min read\n",
      "\n",
      "Understanding Deployment Automation\n",
      "                    \n",
      "In this article we will discuss deployment automation, categories in Automated Deployment, how automation can be implemented in deployment, how it is assisting DevOps and finally the benefits and drawbacks of Deployment Automation. So, let's start exploring the topic in detail. Deployment Automation\n",
      "4 min read\n",
      "Containerization\n",
      "What is Docker?\n",
      "                    \n",
      "Docker is a set of Platforms as a service (PaaS) products that use Operating system-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries, and configuration files; they can communicate with each other\n",
      "15+ min read\n",
      "\n",
      "What is Dockerfile Syntax?\n",
      "                    \n",
      "Pre-requsites: Docker,Dockerfile A Dockerfile is a script that uses the Docker platform to generate containers automatically. It is essentially a text document that contains all the instructions that a user may use to create an image from the command line. The Docker platform is a Linux-based platfo\n",
      "5 min read\n",
      "\n",
      "Kubernetes - Introduction to Container Orchestration\n",
      "                    \n",
      "In this article, we will look into Container Orchestration in Kubernetes. But first, let's explore the trends that gave rise to containers, the need for container orchestration, and how that it has created the space for Kubernetes to rise to dominance and growth. The growth of technology into every\n",
      "4 min read\n",
      "Orchestration\n",
      "Kubernetes - Introduction to Container Orchestration\n",
      "                    \n",
      "In this article, we will look into Container Orchestration in Kubernetes. But first, let's explore the trends that gave rise to containers, the need for container orchestration, and how that it has created the space for Kubernetes to rise to dominance and growth. The growth of technology into every\n",
      "4 min read\n",
      "\n",
      "Fundamental Kubernetes Components and their role in Container Orchestration\n",
      "                    \n",
      "Kubernetes or K8s is an open-sourced container orchestration technology that is used for automating the manual processes of deploying, managing and scaling applications by the help of containers. Kubernetes was originally developed by engineers at Google and In 2015, it was donated to CNCF (Cloud Na\n",
      "12 min read\n",
      "\n",
      "How to Use AWS ECS to Deploy and Manage Containerized Applications?\n",
      "                    \n",
      "Containers can be deployed for applications on the AWS cloud platform. AWS has a special application for managing containerized applications. Elastic Container Service (ECS) serves this purpose. ECS is AWS's container orchestration tool which simplifies the management of containers. All the containe\n",
      "4 min read\n",
      "Infrastructure as Code (IaC)\n",
      "What is Infrastructure as Code (IaC)?\n",
      "                    \n",
      "Infrastructure as Code (IaC) is a method of managing and provisioning IT infrastructure using code rather than manual configuration. It allows teams to automate the setup and management of their infrastructure, making it more efficient and consistent. This is particularly useful in the DevOps enviro\n",
      "7 min read\n",
      "\n",
      "Introduction to Terraform\n",
      "                    \n",
      "Terraform is an open-source infrastructure as code (IaC) software tool which can be used to provision the infrastructure of a cloud platform. The HCL scripts which have been used to provision infrastructure can be human-readable configuration files that can be versioned, reused, and shared. You can\n",
      "14 min read\n",
      "\n",
      "What is AWS Cloudformation?\n",
      "                    \n",
      "Amazon Web Services(AWS) offers cloud formation as a service by which you can provision and manage complicated services offered by AWS by using the code. CloudFormation will help you to manage the infrastructure and the services in the form of a declarative way. Table of Content How Does AWS Cloudfo\n",
      "13 min read\n",
      "Monitoring and Logging\n",
      "Working with Prometheus and Grafana Using Helm\n",
      "                    \n",
      "Pre-requisite: HELM Package Manager Helm is a package manager for Kubernetes that allows you to install, upgrade, and manage applications on your Kubernetes cluster. With Helm, you can define, install, and upgrade your application using a single configuration file, called a Chart. Charts are easy to\n",
      "5 min read\n",
      "\n",
      "Working with Monitoring and Logging Services\n",
      "                    \n",
      "Pre-requisite: Google Cloud Platform Monitoring and Logging services are essential tools for any organization that wants to ensure the reliability, performance, and security of its systems. These services allow organizations to collect and analyze data about the health and behavior of their systems,\n",
      "5 min read\n",
      "\n",
      "Microsoft Teams vs Slack\n",
      "                    \n",
      "Both Microsoft Teams and Slack are the communication channels used by organizations to communicate with their employees. Microsoft Teams was developed in 2017 whereas Slack was created in 2013. Microsoft Teams is mainly used in large organizations and is integrated with Office 365 enhancing the feat\n",
      "4 min read\n",
      "Security in DevOps\n",
      "What is DevSecOps: Overview and Tools\n",
      "                    \n",
      "DevSecOps methodology is an extension of the DevOps model that helps development teams to integrate security objectives very early into the lifecycle of the software development process, giving developers the team confidence to carry out several security tasks independently to protect code from adva\n",
      "10 min read\n",
      "\n",
      "DevOps Best Practices for Kubernetes\n",
      "                    \n",
      "DevOps is the hot topic in the market these days. DevOps is a vague term used for wide number of operations, most agreeable defination of DevOps would be that DevOps is an intersection of development and operations. Certain practices need to be followed during the application release process in DevO\n",
      "11 min read\n",
      "\n",
      "Like\n",
      "\n",
      "Corporate & Communications Address:\n",
      "\n",
      "                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \n",
      "Registered Address:\n",
      "                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \n",
      "Advertise with us\n",
      "CompanyAbout UsLegalPrivacy PolicyIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramGeeksforGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialTutorials ArchiveDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapTop 100 DSA Interview ProblemsDSA Roadmap by Sandeep JainAll Cheat SheetsData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSBootstrapWeb DesignPython TutorialPython Programming ExamplesPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionDjangoComputer ScienceOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsSoftware DevelopmentSoftware TestingDevOpsGitLinuxAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsInteview PreparationCompetitive ProgrammingTop DS or Algo for CPCompany-Wise Recruitment ProcessCompany-Wise PreparationAptitude PreparationPuzzlesSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceWorld GKGeeksforGeeks VideosDSAPythonJavaC++Web DevelopmentData ScienceCS Subjects \n",
      "\n",
      "@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\n",
      "\n",
      "        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "        acknowledge that you have read and understood our\n",
      "        Cookie Policy &\n",
      "        Privacy Policy\n",
      "\n",
      "        Got It !\n",
      "    \n",
      "\n",
      "Improvement\n",
      "Suggest changes\n",
      "Suggest Changes\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "Create Improvement\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "Suggest Changes\n",
      "\n",
      "min 4 words, max Words Limit:1000\n",
      "\n",
      "Thank You!\n",
      "Your suggestions are valuable to us.\n",
      "\n",
      "What kind of Experience do you want to share?\n",
      "Interview Experiences\n",
      "Admission Experiences\n",
      "Career Journeys\n",
      "Work Experiences\n",
      "Campus Experiences\n",
      "Competitive Exam Experiences\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install BeautifulSoup and requests\n",
    "# Open your terminal or command prompt and run the following commands:\n",
    "# pip install beautifulsoup4\n",
    "# pip install requests\n",
    "\n",
    "# Step 2: Import BeautifulSoup and requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Step 3: Load the HTML content\n",
    "url = 'https://www.geeksforgeeks.org/best-ci-cd-tools/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Step 4: Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 5: Extract text from HTML\n",
    "# Example: Extract the text from a specific element\n",
    "# element = soup.find('div',) #class_='example')\n",
    "text_content = soup.get_text()\n",
    "\n",
    "text_content = '\\n'.join(filter(lambda x: len(x.strip()) > 0, text_content.split('\\n\\n')))\n",
    "\n",
    "# Step 6: Print the extracted text\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Best CI/CD Tools That You Should Know\n",
      "(CI/CD) is modern-day practices to continuously improve the software by adding new feature and for that we have to change the existing files and deploy them. Doing it manually every time may consume a lot of developer's time, so to make the process more simple and reliable developers use\n",
      "that are much more efficient and make the whole process automated. As a software Engineer or beginner in development,\n",
      "you should know the best CI/CD tools\n",
      "As you go deep into software development, you will ultimately need these tools to save time and release a more reliable and stable version of your software application with proper automated testing and deploying processes. But before knowing the tools let’s first under what exactly is CI and CD.\n",
      "CI and CD stand for continuous Integration and are continuous Deployment/Delivery which is\n",
      "used in the process of building efficient software\n",
      ". These are the approaches that are used to integrate the code changes into a shared repository (Continuous Integration) while automating the testing and deployment process to the staging and production environment (Continuous Delivery). These approaches are used to provide a more reliable and stable version of a software application by testing it before release to end-users with the help of CI/CD Tools.\n",
      "To learn more about CI/CD, refer to this article-\n",
      "15 Best CI/CD Tools For Developers\n",
      "Continuous Integration and Continuous Delivery are not only necessary to build a great application but also help the business in meeting its various requirements, from automating tasks to receiving quick feedback, so that developers can improve the application accordingly. They are both combined known as continuous software development. In this process of developing efficient applications, developers have created some CI/CS Tools that help the software engineers test the software application effectively. Here are some of the best CI/CD tools that you should know\n",
      "Jenkins is one of the most popular and effective CI/CD tools which is an\n",
      ". It provides several functionalities like automation testing, deploying code changes for smooth collaboration among the teams of product development, and faster identification of issues. It has a feature “\n",
      "” which allows developers to manage their deployment pipelines programmatically. It has a\n",
      "which is very useful in their efficient resource usage. It also has a large community support to help new developers. To enhance the quality of software and increase the speed of delivery of applications, Jenkins has played a very important role.\n",
      "For the automation process of building, testing, and deploying software applications there are various CI/CD tools in the market but among those Travis CI is one of the most popular and useful tools that software developers prefer over other tools due to its crucial role in the software development lifecycle by\n",
      "checking the reliability and quality of code\n",
      "changes before integrating into the production codebase. Automatic building and testing of code during push commands and support for various languages and frameworks are some of the important factors that differentiate these CI/CD tools from the others. All of its feature ensures smooth development workflows and faster feedback, so Travis CI enables collaboration decrease integration issues, and help the developers build more high-quality software applications.\n",
      "GitLab is a web-based platform that provides multiple tools and functionalities to developers to provide help in the process of the software development lifecycle. It is one of the most common CI/CD tools that is used by the developers which provide several useful functionalities like\n",
      "pipelines, code review, and issue tracking with the help of version control through Git repositories. GitLab allows development teams to manage the collaborative project in real-time and track progress within an integrated environment. It is very flexible for various development workflows and is available in both private servers and as a cloud-hosted service.\n",
      "Container Registry and Kubernetes Integration\n",
      "Built-in Issue Tracking and Collaboration\n",
      "Docker is a software platform that is used in the process of software development by providing special functionalities such as\n",
      ". Docker’s libraries, system tools, code, and runtime are stored in standard units called containers. The software that hosts the container is known as Docker Engine. It offers both free and premium services that completely depend on the use case. To use the resources efficiently, Docker uses OS-level virtualization. The application and its dependencies can be packaged into a portable format that can run on any system that supports Docker.\n",
      "Isolation and Resource Efficiency\n",
      "One of the widely used CI/CD server tools is TeamCity which is developed by JetBrains. Since it is a CI/CD tool then definitely is used for the automation of building, testing, and deploying software applications. And one of the most helpful parts of it is that TeamCity has a very\n",
      ", It can easily integrate with popular version control systems like Git and Subversion, also integrate with cloud platforms, and has the ability to trigger builds based on code commits. Projects of different sizes and complexities can easily use these tools due to their flexibility and extensibility.\n",
      "Advanced Build and Deployment Workflows\n",
      "Semaphore is one of the fastest-growing CI/CD tools for DevOps in the software industry that allows you to deploy various software updates every day on platforms like\n",
      ", Android, and macOS with no barriers. Semaphore allow teams to deploy applications through\n",
      "triggered by code changes. It also automates the process of build, testing, and deploying the software application process with a continuous feedback loop to improve the application continuously to make it a stable and reliable application. This CI/CD tool is an example of an amazing tool that allows an end-to-end development process.\n",
      "When it comes to using cloud-based CI/CD tools that automate and optimize the software development lifecycle, Circle CI is the very first tool that developers prioritize. CircleCI\n",
      "supports container-based builds for a consistent environment\n",
      "with parallel and distributed testing and an advanced caching mechanism to speed up build times. It also supports multiple programming languages and frameworks, allowing development teams to customize their workflows on the cloud platform itself. CircleCI helps in debugging early in the development process and ensures that the code quality is good to make reliable software products.\n",
      "Cloud-Based and Self-Hosted Option\n",
      "Bamboo is a continuous integration and continuous delivery (CI/CD) tool which is\n",
      ". The purpose behind the Bamboo tools is to make the software development process very simple by automating the various stages such as building, testing, and deploying the code changes. Development teams can create pipelines that automatically trigger code commits when necessary and help in integration and collaboration. Bamboo\n",
      "helps product development teams with faster feedback loops\n",
      "and improves their code quality to make stable releases of software applications with the help of automated CI/CD pipelines.\n",
      "Integration with Atlassian Ecosystem\n",
      "Integration and Release Management\n",
      "Buddy is a CI/CD tool and is one of the\n",
      "modern and user-friendly platforms\n",
      "automate software development workflows\n",
      ". To set up a complex workflow, Buddy provides us with a visual pipeline builder that makes it easy to visualize the workflow and it also has built-in Docker support for containerized builds and deployments. It is also integrated with various cloud providers. It is very suitable for every development team size due to its flexible and intuitive interface and extensive features.\n",
      "Azure DevOps server is a CI/CD tool which is created by\n",
      "that provides various functionalities such as version control, reporting, requirements management, project management, testing, and several other capabilities. It was formerly known as\n",
      "(TFS). It is a cloud-based service that is much more suitable for an organization that requires physical hosting due to security, compliance, or any other reasons. Since it is a product of Microsoft, it integrates well with other Microsoft products as well. It supports the entire software development lifecycle by providing tools for version control, agile planning, and CI/CD.\n",
      "Customizable Work Items and Boards\n",
      "Bitrise is one of the top cloud-based CI/CD tool that is primarily\n",
      "used for the complete automation process of mobile-first software\n",
      ". Since it is a clud-based software tool, so there is no need to spend more on the physical infrastructure and hardware. Bitrise support all the major programming language used in mobile app development. Bitrise works very smoothly with the public, private, and ad-hoc  Git services. With it's parallel and distributed builds, developers can increase their speed of completing the process by splitting tasks across multiple agents or threads.\n",
      "CodeShip is a CI/CD tool by CloudBees that helps the developers to\n",
      "automate the complete process of application development\n",
      ", from building to deployment. CodeShip has multiple helpful features like integration with version control systems like GitHub and Bitbucket, container support and notification alert. It has very user-friendly interface, and with its focus on simplicity, CodeShip aims to simplify the development process and increase the efficiency in software development lifecycle. It also has basic and pro paid versions, that you can use according to your specific requirements.\n",
      "Spinnaker is a free and open-source, multi-cloud continuous delivery software platform. It was originally\n",
      "developed by the engineers of Netflix\n",
      "and further extended many features by Google. It is designed to work with other software that help in the process of software development such as Kubernetes. Since it is a tool that\n",
      "helps in the lifecycle of software development\n",
      ", it definitely provides functionalities such as automating system tests, managing rollouts, and triggering pipelines.\n",
      "Built-in Rollback and Rollforward\n",
      "Buildbot is an open-source CI/CD tool that focuses on the\n",
      "automation of complex testing and application deployment processes\n",
      ". It started with a lightweight alternative to the Mozilla project’s Tinderbox, and now it is used by several others like Python, WebKit, and ReactOS. Buildbot follows a more decentralized and configurable architecture that allows users to define their build pipelines using scripts that are based on Python. Buildbots are generally suitable for those who need deep customizability and have specific requirements in their CI/CD workflows.\n",
      "Wrecker is a CI/CD platform that is\n",
      "that helps software developers build and deploy their software applications and microservices. It was\n",
      ". Wrecker is totally based on the concepts of Pipeline that automatically executes the code by taking it from the program. It Provides programmatic access to information about applications, builds, and deploys that speed up the development process. It\n",
      "triggers the pipelines in case any new code is committed\n",
      "and can be integrated with Git and other popular notification systems.\n",
      "Git Integrations with various platforms\n",
      "Debug and test pipelines before deploying\n",
      "Continuous Integration and Continuous Deployment is an integral part of the software development process and helps the development teams in building reliable and continuously evolving software with continuous feedback loops. It is necessary to debug the code and receive the feedback after better testing and at the same time deploy the updates when ready, so to automate these processes, developers have built these CI/CD tools. Each tool has its unique features and functionalities which are capable of doing different actions according to the requirements of the team and project. Also, you can customize your specific requirements with different tools but for more premium features, you have to pay extra for it.\n",
      "These are some of the top CI/CD tools that are currently trending among developers:\n",
      "DevOps engineers use CI/CD tools such as Jenkins and GitLab to automate the process of Software Development.\n",
      "3. What is CI/CD, and why is it important in software development?\n",
      "CI/CD stands for Continuous Integration and Continuous Deployment (or Continuous Delivery). It automate and streamline the process of building, testing, and deploying code changes to production environments in software development\n",
      "4. How to choose the right CI/CD tool for my project?\n",
      "Choosing the right CI/CD tool for project is depend upon various factor such as project requirement, existing tool, and end goal of project etc.\n",
      "Continuous Integration (CI) & Continuous Deployment (CD)\n",
      "We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "acknowledge that you have read and understood our\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "min 4 words, max Words Limit:1000\n",
      "Your suggestions are valuable to us.\n",
      "What kind of Experience do you want to share?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_main_text(url):\n",
    "    # Fetch the page content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove unwanted tags: scripts, styles, navs, headers, footers, etc.\n",
    "    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript', 'iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove ad/promo/banner-related classes or IDs\n",
    "    ad_classes = ['advertisement', 'ad', 'adsbygoogle', 'promo', 'banner', 'cookie-banner', 'subscribe']\n",
    "    for class_name in ad_classes:\n",
    "        for tag in soup.select(f'.{class_name}, #{class_name}'):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Remove all anchor tags including their text\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    # Extract visible text from the body or the entire soup if body missing\n",
    "    body = soup.body if soup.body else soup\n",
    "    text = body.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    # Clean up excessive empty lines and whitespace\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    lines = list(filter(lambda x: len(x) > 32, lines))\n",
    "    clean_text = '\\n'.join(lines)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.geeksforgeeks.org/best-ci-cd-tools/\"\n",
    "print(extract_main_text(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 15 Best CI/CD Tools That You Should Know\n",
      "\n",
      "Continuous IntegrationandContinuous Delivery(CI/CD) is modern-day practices to continuously improve the software by adding new feature and for that we have to change the existing files and deploy them. Doing it manually every time may consume a lot of developer's time, so to make the process more simple and reliable developers useCI/CD toolsthat are much more efficient and make the whole process automated. As a software Engineer or beginner in development,you should know the best CI/CD tools.\n",
      "\n",
      "As you go deep into software development, you will ultimately need these tools to save time and release a more reliable and stable version of your software application with proper automated testing and deploying processes. But before knowing the tools let’s first under what exactly is CI and CD.\n",
      "\n",
      "## What is CI/CD?\n",
      "\n",
      "CI and CD stand for continuous Integration and are continuous Deployment/Delivery which isused in the process of building efficient software. These are the approaches that are used to integrate the code changes into a shared repository (Continuous Integration) while automating the testing and deployment process to the staging and production environment (Continuous Delivery). These approaches are used to provide a more reliable and stable version of a software application by testing it before release to end-users with the help of CI/CD Tools.\n",
      "\n",
      "To learn more about CI/CD, refer to this article-\n",
      "\n",
      "## 15 Best CI/CD Tools For Developers\n",
      "\n",
      "Continuous Integration and Continuous Delivery are not only necessary to build a great application but also help the business in meeting its various requirements, from automating tasks to receiving quick feedback, so that developers can improve the application accordingly. They are both combined known as continuous software development. In this process of developing efficient applications, developers have created some CI/CS Tools that help the software engineers test the software application effectively. Here are some of the best CI/CD tools that you should know\n",
      "\n",
      "### 1. Jenkins\n",
      "\n",
      "Jenkins is one of the most popular and effective CI/CD tools which is anopen-source automation server. It provides several functionalities like automation testing, deploying code changes for smooth collaboration among the teams of product development, and faster identification of issues. It has a feature “Pipeline as Code” which allows developers to manage their deployment pipelines programmatically. It has awhich is very useful in their efficient resource usage. It also has a large community support to help new developers. To enhance the quality of software and increase the speed of delivery of applications, Jenkins has played a very important role.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features:\n",
      "\n",
      "- Extensibility with Plugins\n",
      "- Pipeline as Code\n",
      "- Distributed Builds\n",
      "### 2. Travis CI\n",
      "\n",
      "For the automation process of building, testing, and deploying software applications there are various CI/CD tools in the market but among those Travis CI is one of the most popular and useful tools that software developers prefer over other tools due to its crucial role in the software development lifecycle bychecking the reliability and quality of codechanges before integrating into the production codebase. Automatic building and testing of code during push commands and support for various languages and frameworks are some of the important factors that differentiate these CI/CD tools from the others. All of its feature ensures smooth development workflows and faster feedback, so Travis CI enables collaboration decrease integration issues, and help the developers build more high-quality software applications.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features:\n",
      "\n",
      "- GitHub Integration\n",
      "- Build Matrix\n",
      "- Caching Dependencies\n",
      "### 3. GitLab\n",
      "\n",
      "GitLab is a web-based platform that provides multiple tools and functionalities to developers to provide help in the process of the software development lifecycle. It is one of the most common CI/CD tools that is used by the developers which provide several useful functionalities likecollaborative project management,continuous integrationandcontinuous deploymentpipelines, code review, and issue tracking with the help of version control through Git repositories. GitLab allows development teams to manage the collaborative project in real-time and track progress within an integrated environment. It is very flexible for various development workflows and is available in both private servers and as a cloud-hosted service.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features\n",
      "\n",
      "- Integrated DevOps Platform\n",
      "- Container Registry and Kubernetes Integration\n",
      "- Built-in Issue Tracking and Collaboration\n",
      "### 4. Docker\n",
      "\n",
      "Docker is a software platform that is used in the process of software development by providing special functionalities such asbuilding,testing, anddeploying software applications. Docker’s libraries, system tools, code, and runtime are stored in standard units called containers. The software that hosts the container is known as Docker Engine. It offers both free and premium services that completely depend on the use case. To use the resources efficiently, Docker uses OS-level virtualization. The application and its dependencies can be packaged into a portable format that can run on any system that supports Docker.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features\n",
      "\n",
      "- Containerization Technology\n",
      "- Image-Based Deployment\n",
      "- Isolation and Resource Efficiency\n",
      "### 5. TeamCity\n",
      "\n",
      "One of the widely used CI/CD server tools is TeamCity which is developed by JetBrains. Since it is a CI/CD tool then definitely is used for the automation of building, testing, and deploying software applications. And one of the most helpful parts of it is that TeamCity has a veryuser-friendly web interface, It can easily integrate with popular version control systems like Git and Subversion, also integrate with cloud platforms, and has the ability to trigger builds based on code commits. Projects of different sizes and complexities can easily use these tools due to their flexibility and extensibility.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features\n",
      "\n",
      "- Cross-Platform Compatibility\n",
      "- Build Chains and Dependencies\n",
      "- Advanced Build and Deployment Workflows\n",
      "### 6. Semaphore\n",
      "\n",
      "Semaphore is one of the fastest-growing CI/CD tools for DevOps in the software industry that allows you to deploy various software updates every day on platforms like, Android, and macOS with no barriers. Semaphore allow teams to deploy applications throughcustomizable pipelinestriggered by code changes. It also automates the process of build, testing, and deploying the software application process with a continuous feedback loop to improve the application continuously to make it a stable and reliable application. This CI/CD tool is an example of an amazing tool that allows an end-to-end development process.\n",
      "\n",
      "Features\n",
      "\n",
      "- YAML-Based Configuration\n",
      "- Parallel and Distributed Testing\n",
      "- Continuous Delivery Pipelines\n",
      "### 7. Circle CI\n",
      "\n",
      "When it comes to using cloud-based CI/CD tools that automate and optimize the software development lifecycle, Circle CI is the very first tool that developers prioritize. CircleCIsupports container-based builds for a consistent environmentwith parallel and distributed testing and an advanced caching mechanism to speed up build times. It also supports multiple programming languages and frameworks, allowing development teams to customize their workflows on the cloud platform itself. CircleCI helps in debugging early in the development process and ensures that the code quality is good to make reliable software products.\n",
      "\n",
      "Features\n",
      "\n",
      "- Orbs for Reusable Configuration\n",
      "- Workspaces for Data Sharing\n",
      "- Cloud-Based and Self-Hosted Option\n",
      "### 8. Bamboo\n",
      "\n",
      "Bamboo is a continuous integration and continuous delivery (CI/CD) tool which isdeveloped by Atlassian. The purpose behind the Bamboo tools is to make the software development process very simple by automating the various stages such as building, testing, and deploying the code changes. Development teams can create pipelines that automatically trigger code commits when necessary and help in integration and collaboration. Bamboohelps product development teams with faster feedback loopsand improves their code quality to make stable releases of software applications with the help of automated CI/CD pipelines.\n",
      "\n",
      "Features\n",
      "\n",
      "- Integration with Atlassian Ecosystem\n",
      "- Docker and Container Support\n",
      "- Integration and Release Management\n",
      "### 9. Buddy\n",
      "\n",
      "Buddy is a CI/CD tool and is one of themodern and user-friendly platformstoautomate software development workflows. To set up a complex workflow, Buddy provides us with a visual pipeline builder that makes it easy to visualize the workflow and it also has built-in Docker support for containerized builds and deployments. It is also integrated with various cloud providers. It is very suitable for every development team size due to its flexible and intuitive interface and extensive features.\n",
      "\n",
      "Features\n",
      "\n",
      "- Pipeline as Code\n",
      "- Deployment Automation\n",
      "- Serverless Actions\n",
      "### 10. Azure DevOps Server\n",
      "\n",
      "Azure DevOps server is a CI/CD tool which is created byMicrosoftthat provides various functionalities such as version control, reporting, requirements management, project management, testing, and several other capabilities. It was formerly known asTeam Foundation Server(TFS). It is a cloud-based service that is much more suitable for an organization that requires physical hosting due to security, compliance, or any other reasons. Since it is a product of Microsoft, it integrates well with other Microsoft products as well. It supports the entire software development lifecycle by providing tools for version control, agile planning, and CI/CD.\n",
      "\n",
      "To learn more, refer to -\n",
      "\n",
      "Features\n",
      "\n",
      "- Customizable Work Items and Boards\n",
      "- Build and Release Pipelines\n",
      "- Reporting and Analytics\n",
      "### 11. Bitrise\n",
      "\n",
      "Bitrise is one of the top cloud-based CI/CD tool that is primarilyused for the complete automation process of mobile-first software. Since it is a clud-based software tool, so there is no need to spend more on the physical infrastructure and hardware. Bitrise support all the major programming language used in mobile app development. Bitrise works very smoothly with the public, private, and ad-hoc  Git services. With it's parallel and distributed builds, developers can increase their speed of completing the process by splitting tasks across multiple agents or threads.\n",
      "\n",
      "Features\n",
      "\n",
      "- Mobile-centric Automation\n",
      "- Visual Workflow Editor\n",
      "- Parallel and Distributed Builds\n",
      "### 12. CodeShip\n",
      "\n",
      "CodeShip is a CI/CD tool by CloudBees that helps the developers toautomate the complete process of application development, from building to deployment. CodeShip has multiple helpful features like integration with version control systems like GitHub and Bitbucket, container support and notification alert. It has very user-friendly interface, and with its focus on simplicity, CodeShip aims to simplify the development process and increase the efficiency in software development lifecycle. It also has basic and pro paid versions, that you can use according to your specific requirements.\n",
      "\n",
      "Features\n",
      "\n",
      "- Multi-Environment Deployment\n",
      "- Notification and Alerts\n",
      "- Project Lifecycle Management\n",
      "### 13. Spinnaker\n",
      "\n",
      "Spinnaker is a free and open-source, multi-cloud continuous delivery software platform. It was originallydeveloped by the engineers of Netflixand further extended many features by Google. It is designed to work with other software that help in the process of software development such as Kubernetes. Since it is a tool thathelps in the lifecycle of software development, it definitely provides functionalities such as automating system tests, managing rollouts, and triggering pipelines.\n",
      "\n",
      "Features\n",
      "\n",
      "- Multi-Cloud Deployment\n",
      "- Built-in Rollback and Rollforward\n",
      "- Automated Canary Analysis\n",
      "### 14. Buildbot\n",
      "\n",
      "Buildbot is an open-source CI/CD tool that focuses on theautomation of complex testing and application deployment processes. It is based on. It started with a lightweight alternative to the Mozilla project’s Tinderbox, and now it is used by several others like Python, WebKit, and ReactOS. Buildbot follows a more decentralized and configurable architecture that allows users to define their build pipelines using scripts that are based on Python. Buildbots are generally suitable for those who need deep customizability and have specific requirements in their CI/CD workflows.\n",
      "\n",
      "Features\n",
      "\n",
      "- Highly Customizable\n",
      "- Distributed Builds\n",
      "- Real-time Monitoring and Logging\n",
      "### 15. Wrecker\n",
      "\n",
      "Wrecker is a CI/CD platform that isbased on Dockerthat helps software developers build and deploy their software applications and microservices. It wasacquired by Oracle. Wrecker is totally based on the concepts of Pipeline that automatically executes the code by taking it from the program. It Provides programmatic access to information about applications, builds, and deploys that speed up the development process. Ittriggers the pipelines in case any new code is committedand can be integrated with Git and other popular notification systems.\n",
      "\n",
      "Features\n",
      "\n",
      "- Git Integrations with various platforms\n",
      "- Debug and test pipelines before deploying\n",
      "## Conclusion\n",
      "\n",
      "Continuous Integration and Continuous Deployment is an integral part of the software development process and helps the development teams in building reliable and continuously evolving software with continuous feedback loops. It is necessary to debug the code and receive the feedback after better testing and at the same time deploy the updates when ready, so to automate these processes, developers have built these CI/CD tools. Each tool has its unique features and functionalities which are capable of doing different actions according to the requirements of the team and project. Also, you can customize your specific requirements with different tools but for more premium features, you have to pay extra for it.\n",
      "\n",
      "## FAQs on Best CI/CD Tools\n",
      "\n",
      "### 1. What are the top CI/CD tools?\n",
      "\n",
      "These are some of the top CI/CD tools that are currently trending among developers:\n",
      "\n",
      "1. JenKins2. Docker3. GitLab4. Microsoft Azure5. Wrecker\n",
      "\n",
      "### 2. Who uses CI/CD Tools?\n",
      "\n",
      "DevOps engineers use CI/CD tools such as Jenkins and GitLab to automate the process of Software Development.\n",
      "\n",
      "### 3. What is CI/CD, and why is it important in software development?\n",
      "\n",
      "CI/CD stands for Continuous Integration and Continuous Deployment (or Continuous Delivery). It automate and streamline the process of building, testing, and deploying code changes to production environments in software development\n",
      "\n",
      "### 4. How to choose the right CI/CD tool for my project?\n",
      "\n",
      "Choosing the right CI/CD tool for project is depend upon various factor such as project requirement, existing tool, and end goal of project etc.\n",
      "\n",
      "### Similar Reads\n",
      "\n",
      "## Introduction\n",
      "\n",
      "## Version Control\n",
      "\n",
      "## Continuous Integration (CI) & Continuous Deployment (CD)\n",
      "\n",
      "## Containerization\n",
      "\n",
      "## Orchestration\n",
      "\n",
      "## Infrastructure as Code (IaC)\n",
      "\n",
      "## Monitoring and Logging\n",
      "\n",
      "## Security in DevOps\n",
      "\n",
      "## Thank You!\n",
      "\n",
      "## What kind of Experience do you want to share?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_markdown_text(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove scripts, styles, navs, headers, footers, and typical ad elements\n",
    "    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript', 'iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "    ad_classes = ['advertisement', 'ad', 'adsbygoogle', 'promo', 'banner', 'cookie-banner', 'subscribe']\n",
    "    for class_name in ad_classes:\n",
    "        for tag in soup.select(f'.{class_name}, #{class_name}'):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Remove all links and their content\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    # Markdown content building\n",
    "    markdown_lines = []\n",
    "\n",
    "    # Process headings and paragraphs\n",
    "    for element in soup.body.descendants if soup.body else soup.descendants:\n",
    "        if element.name:\n",
    "            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                level = int(element.name[1])\n",
    "                markdown_lines.append(f\"{'#' * level} {element.get_text(strip=True)}\\n\")\n",
    "            elif element.name in ['ul', 'ol']:\n",
    "                for li in element.find_all('li'):\n",
    "                    markdown_lines.append(f\"- {li.get_text(strip=True)}\")\n",
    "            elif element.name == 'p':\n",
    "                text = element.get_text(strip=True)\n",
    "                if text:\n",
    "                    markdown_lines.append(f\"{text}\\n\")\n",
    "\n",
    "    # Final cleanup: remove empty lines\n",
    "    markdown_lines = list(filter(lambda x: len(x) > 2, markdown_lines))\n",
    "    markdown_content = '\\n'.join([line for line in markdown_lines if line.strip()])\n",
    "    \n",
    "    return markdown_content\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.geeksforgeeks.org/best-ci-cd-tools/\"\n",
    "print(extract_markdown_text(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "  \"/Users/ohi/Documents/GitHub/PersonalAssistant/weights/SmolThink-360M-sft/checkpoint-45807\",\n",
    "#   revision=\"v2.0.1\" # tag name, or branch name, or commit hash\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/ohi/Documents/GitHub/PersonalAssistant/weights/SmolThink-360M-sft/checkpoint-45807\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e5c3bf-08a9130203aef20f66ccf6d7;92755505-03bd-4d73-a65e-0a459a938b89)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSmolThink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3085\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[1;32m   3084\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[0;32m-> 3085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:872\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    870\u001b[0m organization \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 872\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[1;32m    877\u001b[0m model_card \u001b[38;5;241m=\u001b[39m create_and_tag_model_card(\n\u001b[1;32m    878\u001b[0m     repo_id, tags, token\u001b[38;5;241m=\u001b[39mtoken, ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors\n\u001b[1;32m    879\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:679\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    676\u001b[0m             repo_id \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    677\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3511\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3508\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3511\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3514\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:481\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e5c3bf-08a9130203aef20f66ccf6d7;92755505-03bd-4d73-a65e-0a459a938b89)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"SmolThink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformers-cli <command> [<args>]\n",
      "Transformers CLI tool: error: argument {chat,convert,download,env,run,serve,add-new-model-like,add-fast-image-processor}: invalid choice: 'login' (choose from 'chat', 'convert', 'download', 'env', 'run', 'serve', 'add-new-model-like', 'add-fast-image-processor')\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hi, can you tell me the current stock price of Apple?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"<think>\\nAlright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think>\\n<tool_call>\\n[{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}]\\n</tool_call>\"}, \n",
    "    {\"role\": \"tool\", \"content\": \"{'stock_price': '$150.75'}\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"The current stock price of Apple is $150.75.\"}, \n",
    "    {'role': 'user', 'content': \"That's great. Can you also tell me the stock price for Microsoft?\"}, \n",
    "    {'role': 'assistant', 'content': \"<think>\\nAlright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think>\\n<tool_call>\\n[{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}]\\n</tool_call>\"}, {'role': 'tool', 'content': \"{'stock_price': '$210.22'}\"}, {'role': 'assistant', 'content': 'The current stock price of Microsoft is $210.22.'}, {'role': 'user', 'content': 'Thank you for the information.'}, {'role': 'assistant', 'content': \"You're welcome! If you have any other questions, feel free to ask.\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"You are a helpful assistant.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello, how are you?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing great. How can I help you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I'd like to show off how chat templating works!\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{{- if .Messages }}\n",
    "{{- if .Tools }}<|im_start|>system\n",
    "You are a helpful AI assistant named SmolThink.\n",
    "\n",
    "# Tools\n",
    " \n",
    "You may call one or more functions to assist with the user query.\n",
    "\n",
    "You are provided with function signatures within <tools></tools> tags:\n",
    "<tools>\n",
    "{{ .Tools }}\n",
    "</tools>\n",
    "\n",
    "You first think/plan inside <think></think> tags.\n",
    "Then for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\n",
    "{{ else if .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{- else if not .Tools }}\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside <think></think> tag and provide final answer to user query inside <answer></answer> tag.<|im_end|>\n",
    "{{ end }}\n",
    "{{- range $i, $_ := .Messages }}\n",
    "{{- $last := eq (len (slice $.Messages $i)) 1 -}}\n",
    "{{- if eq .Role \"user\" }}<|im_start|>user\n",
    "{{ .Content }}<|im_end|>\n",
    "{{ else if eq .Role \"assistant\" }}<|im_start|>assistant\n",
    "{{ if .Content }}{{ .Content }}\n",
    "{{- else if .ToolCalls }}<tool_call>[\n",
    "{{ range .ToolCalls }}{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ .Function.Arguments }}}{{ end }}\n",
    "]</tool_call>\n",
    "{{- end }}{{ if not $last }}<|im_end|>\n",
    "{{ end }}\n",
    "{{- else if eq .Role \"tool\" }}<|im_start|>user\n",
    "<tool_response>\n",
    "{{ .Content }}\n",
    "</tool_response><|im_end|>\n",
    "{{ end }}\n",
    "{{- if and (ne .Role \"assistant\") $last }}<|im_start|>assistant\n",
    "{{ end }}\n",
    "{{- end }} \n",
    "{{- else }}\n",
    "{{- if .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}{{ if .Prompt }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "{{ end }}<|im_start|>assistant\n",
    "{{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside <think></think> tag and provide final answer to user query inside <answer></answer> tag.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 12+9?.<|im_end|>\n",
      "\n",
      "<|endoftext|><|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside <think></think> tag and provide final answer to user query inside <answer></answer> tag.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 12+9?.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "</think>\n",
      "<answer>\n",
      "12 + 9 = 21\n",
      "</answer><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"quwsarohi/SmolThink\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"quwsarohi/SmolThink\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,\n",
    "    tie_word_embeddings=True,\n",
    ").to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 12+9?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, StoppingCriteria\n",
    "from datasets import load_dataset\n",
    "import peft\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Optional, List\n",
    "from jinja2 import Template\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def url_content(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove scripts, styles, navs, headers, footers, and typical ad elements\n",
    "    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript', 'iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "    ad_classes = ['advertisement', 'ad', 'adsbygoogle', 'promo', 'banner', 'cookie-banner', 'subscribe']\n",
    "    for class_name in ad_classes:\n",
    "        for tag in soup.select(f'.{class_name}, #{class_name}'):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Remove all links and their content\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    # Markdown content building\n",
    "    markdown_lines = []\n",
    "\n",
    "    # Process headings and paragraphs\n",
    "    for element in soup.body.descendants if soup.body else soup.descendants:\n",
    "        if element.name:\n",
    "            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                level = int(element.name[1])\n",
    "                markdown_lines.append(f\"{'#' * level} {element.get_text(strip=True)}\\n\")\n",
    "            elif element.name in ['ul', 'ol']:\n",
    "                for li in element.find_all('li'):\n",
    "                    markdown_lines.append(f\"- {li.get_text(strip=True)}\")\n",
    "            elif element.name == 'p':\n",
    "                text = element.get_text(strip=True)\n",
    "                if text:\n",
    "                    markdown_lines.append(f\"{text}\\n\")\n",
    "\n",
    "    # Final cleanup: remove empty lines\n",
    "    markdown_lines = list(filter(lambda x: len(x) > 2, markdown_lines))\n",
    "    markdown_content = '\\n'.join([line for line in markdown_lines if line.strip()])    \n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "def search_tool(search_str, full_content=True, max_results=1):\n",
    "    rets = None\n",
    "    with DDGS() as ddg:\n",
    "        rets = list(ddg.text(keywords=search_str, region=\"wt-wt\", max_results=max_results))\n",
    "\n",
    "    str_rets = ''\n",
    "    if full_content:\n",
    "        str_rets = \"Use following information to answer user_question:\\n\\n\"\n",
    "        for i, r in enumerate(rets):\n",
    "            r = url_content(rets[i]['href'])[:1024*3]\n",
    "            str_rets += f\"# Source {i+1}:\\n\" + \"-\"*10 + f\"\\n\\n{r}\\n\\n\\n\"\n",
    "    else:\n",
    "        str_rets = \"Use following information to answer user_question:\\n\\n\"\n",
    "        for i, r in enumerate(rets):\n",
    "            str_rets += f\"# Source {i+1}:\\n\" + \"-\"*10 + f\"\\n\\n{r}\\n\\n\\n\"\n",
    "        \n",
    "    return str_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use following information to answer user_question:\n",
      "\n",
      "# Source 1:\n",
      "----------\n",
      "\n",
      "# Math Calculator\n",
      "\n",
      "Step 1:\n",
      "\n",
      "Enter the expression you want to evaluate.\n",
      "\n",
      "The Math Calculator will evaluate your problem down to a final solution. You can also add, subtraction, multiply, and divide and complete any arithmetic you need.\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Click theblue arrowto submit and see your result!\n",
      "\n",
      "Please ensure that your password is at least 8 characters and contains each of the following:\n",
      "\n",
      "- a number\n",
      "- a letter\n",
      "- a special character: @$#!%*?&\n",
      "\n",
      "\n",
      "# Source 2:\n",
      "----------\n",
      "\n",
      "# Basic Operations Calculator\n",
      "\n",
      "## Number Line\n",
      "\n",
      "- Show More\n",
      "basic-operations-calculator\n",
      "\n",
      "en\n",
      "\n",
      "Please add a message.\n",
      "\n",
      "Message received. Thanks for the feedback.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(search_tool(\"perform math operation: 32 + 64 = ?\", max_results=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FirstURL': 'https://duckduckgo.com/c/Python_(programming_language)', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Python_(programming_language)\">Python (programming language) Category</a>', 'Text': 'Python (programming language) Category'}, {'FirstURL': 'https://duckduckgo.com/Python_syntax_and_semantics', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/Python_syntax_and_semantics\">Python syntax and semantics</a> - The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted. The Python language has many similarities to Perl, C, and Java. However, there are some definite differences between the languages.', 'Text': 'Python syntax and semantics - The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted. The Python language has many similarities to Perl, C, and Java. However, there are some definite differences between the languages.'}, {'FirstURL': 'https://duckduckgo.com/History_of_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/History_of_programming_languages\">History of programming languages</a> - The history of programming languages spans from documentation of early mechanical computers to modern tools for software development. Early programming languages were highly specialized, relying on mathematical notation and similarly obscure syntax.', 'Text': 'History of programming languages - The history of programming languages spans from documentation of early mechanical computers to modern tools for software development. Early programming languages were highly specialized, relying on mathematical notation and similarly obscure syntax.'}, {'FirstURL': 'https://duckduckgo.com/c/Pattern_matching_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Pattern_matching_programming_languages\">Pattern matching programming languages</a>', 'Text': 'Pattern matching programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Multi-paradigm_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Multi-paradigm_programming_languages\">Multi-paradigm programming languages</a>', 'Text': 'Multi-paradigm programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Class-based_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Class-based_programming_languages\">Class-based programming languages</a>', 'Text': 'Class-based programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Text-oriented_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Text-oriented_programming_languages\">Text-oriented programming languages</a>', 'Text': 'Text-oriented programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Concurrent_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Concurrent_programming_languages\">Concurrent programming languages</a>', 'Text': 'Concurrent programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Dynamically_typed_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Dynamically_typed_programming_languages\">Dynamically typed programming languages</a>', 'Text': 'Dynamically typed programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Educational_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Educational_programming_languages\">Educational programming languages</a>', 'Text': 'Educational programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/High-level_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/High-level_programming_languages\">High-level programming languages</a>', 'Text': 'High-level programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Object-oriented_programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Object-oriented_programming_languages\">Object-oriented programming languages</a>', 'Text': 'Object-oriented programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Scripting_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Scripting_languages\">Scripting languages</a>', 'Text': 'Scripting languages'}, {'FirstURL': 'https://duckduckgo.com/c/Dutch_inventions', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Dutch_inventions\">Dutch inventions</a>', 'Text': 'Dutch inventions'}, {'FirstURL': 'https://duckduckgo.com/c/Programming_languages', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Programming_languages\">Programming languages</a>', 'Text': 'Programming languages'}, {'FirstURL': 'https://duckduckgo.com/c/Cross-platform_free_software', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Cross-platform_free_software\">Cross-platform free software</a>', 'Text': 'Cross-platform free software'}, {'FirstURL': 'https://duckduckgo.com/c/Cross-platform_software', 'Icon': {'Height': '', 'URL': '', 'Width': ''}, 'Result': '<a href=\"https://duckduckgo.com/c/Cross-platform_software\">Cross-platform software</a>', 'Text': 'Cross-platform software'}]\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2. Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def search_ddg(query):\n",
    "    url = \"https://api.duckduckgo.com/\"\n",
    "    params = {\"q\": query, \"format\": \"json\"}\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "results = search_ddg(\"Python programming\")\n",
    "print(results[\"RelatedTopics\"])\n",
    "\n",
    "\n",
    "def fetch_instant_answer(query):\n",
    "    response = search_ddg(query)\n",
    "    return response.get(\"AbstractText\", \"No instant answer available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(fetch_instant_answer(\"How to commit in github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama-index 0.12.27\n",
      "Uninstalling llama-index-0.12.27:\n",
      "  Would remove:\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/bin/llamaindex-cli\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/llama_index-0.12.27.dist-info/*\n",
      "    /Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/llama_index/_bundle/*\n",
      "Proceed (Y/n)? ^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip uninstall llama-index llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from io import BytesIO\n",
    "from docling.backend.html_backend import HTMLDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.document import InputDocument\n",
    "\n",
    "def docling_cleanup(input_str):\n",
    "    # <!-- image --> tag cleanup\n",
    "    input_str = input_str.replace('<!-- image -->', '')\n",
    "    # Lines with empty spaces\n",
    "    lines = filter(lambda x: not x.isspace() and len(x) > 3, input_str.split('\\n'))\n",
    "    input_str = '\\n'.join(list(lines))\n",
    "    del lines\n",
    "    # clean excessive newlines\n",
    "    _cnt = 0\n",
    "    ret_str = ''\n",
    "    for c in input_str:\n",
    "        if c == '\\n':\n",
    "            _cnt += 1\n",
    "            if _cnt > 2: continue\n",
    "            else: ret_str += c\n",
    "        else:\n",
    "            _cnt = 0\n",
    "            ret_str += c\n",
    "    return ret_str\n",
    "\n",
    "import requests\n",
    "\n",
    "def url_content(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    ascii_text = response.text.encode(\"ascii\", \"ignore\")\n",
    "    ascii_text = re.sub(r'[^A-Za-z0-9\\s.,;:!?()\"\\']+', '', response.text)\n",
    "    in_doc = InputDocument(\n",
    "        path_or_stream=BytesIO(ascii_text),\n",
    "        format=InputFormat.HTML,\n",
    "        backend=HTMLDocumentBackend,\n",
    "        filename=\"duck.html\",\n",
    "    )\n",
    "\n",
    "    backend = HTMLDocumentBackend(in_doc=in_doc, path_or_stream=BytesIO(ascii_text))\n",
    "    dl_doc = backend.convert()\n",
    "    return docling_cleanup(dl_doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ret = url_content('https://stackoverflow.com/questions/3939660/sieve-of-eratosthenes-finding-primes-python')\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ollama_infr(prompt, extra_stops=[], model='deepseek-r1:7b', temperature=0.7):\n",
    "    # https://github.com/ollama/ollama-python/blob/00eafed0faa5dea6879a8eb3229c7a8f2439abb4/ollama/_types.py#L93\n",
    "    return ollama.generate(\n",
    "        model = model,\n",
    "        # system = system,\n",
    "        # Raw is set to true to feed the question as needed\n",
    "        raw=True,\n",
    "        prompt = prompt,\n",
    "        stream = True,\n",
    "        # Number of seconds to keep the connection alive\n",
    "        keep_alive=-1, # Will keep the model loaded,\n",
    "        options = {\n",
    "            'stop': [\n",
    "                \"<|start_header_id|>\",\n",
    "                \"<|end_header_id|>\",\n",
    "                \"<|eot_id|>\",\n",
    "            ] + extra_stops,\n",
    "            'temperature': temperature,\n",
    "            # 'top_k': 1,\n",
    "            'cache': False,\n",
    "            # 'tfs_z': 2.0,\n",
    "            'num_ctx': 6000,\n",
    "            # 'temperature': 0.0,\n",
    "            # 'top_p': 0.0\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is asking about the use of the `ST_AsEWKB` function in Python, specifically for converting a signal from 50 Hz to 60 Hz. They are interested in understanding how this function works and its potential applications. Additionally, they would like more information on whether there's precedent or precedent-setting cases where politicians have been barred from running for office due to fraud or embezzlement. The user also seeks help with the technical aspects of their query, including how to subscribe to a specific feed."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m think, answer\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mr1_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to find prime numbers?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m, in \u001b[0;36mr1_response\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_res) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m6000\u001b[39m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m think \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<think>(.*?)</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_res, re\u001b[38;5;241m.\u001b[39mDOTALL)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     52\u001b[0m answer \u001b[38;5;241m=\u001b[39m model_res[model_res\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# deepseek_r1 = '''You are a helpful AI. You will be given a question and a web content. You have to provide a summary of the web content.\n",
    "\n",
    "# <｜User｜>User question: {question}\n",
    "\n",
    "# Here is the web content:\n",
    "# {context}\n",
    "\n",
    "# <｜end▁of▁sentence｜>\n",
    "# <｜Assistant｜>\n",
    "# <think>\n",
    "\n",
    "# </think>\n",
    "\n",
    "# '''\n",
    "\n",
    "deepseek_r1 = '''<|im_start|>system\n",
    "You are a helpful AI. You will be given a question and a web content. You have to provide a summary of the web content.<|im_end|>\n",
    "<|im_start|>user\n",
    "User question: {question}\n",
    "\n",
    "Here is the web content:\n",
    "{context}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "def r1_response(question, context):    \n",
    "    prompt = deepseek_r1.format(question=question, context=context)\n",
    "    # print(\"Question:\", data['question'], flush=True)\n",
    "\n",
    "    stream = ollama_infr(prompt=prompt, model='SmolThink:latest', temperature=0.5)\n",
    "    model_res = '<think>\\n'\n",
    "    n_think_tokens = 0\n",
    "    think_finished = False\n",
    "\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        model_res += part['response']\n",
    "\n",
    "        if not think_finished and '</think>' in model_res:\n",
    "            think_finished = True\n",
    "        if not think_finished:\n",
    "            n_think_tokens += 1\n",
    "\n",
    "        # if n_think_tokens > 386:\n",
    "            # print(\"Generation limit exceeded\", flush=True)\n",
    "            # return None, None\n",
    "        \n",
    "        if len(model_res) > 6000:\n",
    "            return None, None\n",
    "\n",
    "    think = re.findall(r\"<think>(.*?)</think>\", model_res, re.DOTALL)[0].strip()\n",
    "    answer = model_res[model_res.find(\"</think>\")+len(\"</think>\"):].strip()\n",
    "\n",
    "    if not answer:\n",
    "        return None, None\n",
    "    \n",
    "    return think, answer\n",
    "\n",
    "print(r1_response(\"How to find prime numbers?\", ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"quwsarohi/SmolThink\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"quwsarohi/SmolThink\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,\n",
    "    tie_word_embeddings=True,\n",
    ").to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtool_def = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Can search the web for infomation which are doubtful/unknown/recent.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"search_str\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The whole question you want to ask.\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "base_prompt = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": \"What is the current stock price of Apple?\"}\n",
    "], tools=[webtool_def], tokenize=False, add_generation_prompt=True)\n",
    "print(base_prompt)\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
