{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from datasets import load_dataset, concatenate_datasets,  load_from_disk\n",
    "import peft\n",
    "\n",
    "# from safetensors.torch import load_model, save_model\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "from typing import Optional\n",
    "from jinja2 import Template\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = \"360M\"\n",
    "MODEL_PATH = f\"HuggingFaceTB/SmolLM2-{SIZE}-Instruct\"\n",
    "LORA_PATH = None\n",
    "# dataset = load_from_disk(\"/Users/ohi/Documents/GitHub/PersonalAssistant/dataset\")\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 0.72 GB of space (with buffer)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    # \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # attn_implementation='sdpa',\n",
    "    # attn_implementation='eager', # 'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    tie_word_embeddings=True,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Gradient checkpointing - Could take more memory in MPS\n",
    "# model.gradient_checkpointing_enable(dict(use_reentrant=False))\n",
    "model.gradient_checkpointing_disable()\n",
    "# model.resize_token_embeddings(49162)\n",
    "print(f\"Model took {model.get_memory_footprint()/1e9:.2f} GB of space (with buffer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft.PeftModel.from_pretrained(\n",
    "   model,\n",
    "   \"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-360M-sft-r64-old/checkpoint-8700/smolthink\",\n",
    "   is_trainable=False, # ðŸ‘ˆ here,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): ModulesToSaveWrapper(\n",
      "          (original_module): Embedding(49152, 960, padding_idx=2)\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Embedding(49152, 960, padding_idx=2)\n",
      "          )\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=320, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=320, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=960, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=960, out_features=49152, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=960, out_features=49152, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# 49152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens -> base_model.model.model.embed_tokens.original_module.weight : False\n",
      "embed_tokens -> base_model.model.model.embed_tokens.modules_to_save.smolthink.weight : True\n",
      "lm_head -> base_model.model.lm_head.modules_to_save.smolthink.weight : True\n",
      "Total LoRA params: 34.73 million (7.61 %) = 34.73 million\n",
      "Total LoRA layers: 448\n",
      "Approx size: 69.47 mb\n"
     ]
    }
   ],
   "source": [
    "# if lora_r:\n",
    "lora_r = 64\n",
    "# SAVE_PATH += f'r{lora_r}'\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=lora_r,                   # 64\n",
    "    lora_alpha=2*lora_r,        # alpha = 4 * r\n",
    "    lora_dropout=0.05,\n",
    "    target_modules='all-linear',\n",
    "    modules_to_save = [\n",
    "        \"embed_tokens\", \n",
    "        \"lm_head\"\n",
    "    ],\n",
    "    use_rslora=True,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "model = peft.get_peft_model(model, peft_config, adapter_name=\"smolthink\", autocast_adapter_dtype=False)\n",
    "\n",
    "# Sanity check\n",
    "non_lora_param = 0\n",
    "lora_param = 0\n",
    "lora_layers = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        # param.requires_grad = True\n",
    "        assert param.requires_grad == True, f\"{name} is not trainable\"\n",
    "        lora_param += param.numel()\n",
    "        lora_layers += 1\n",
    "    else:\n",
    "        # if not param.requires_grad:\n",
    "        #     print(f\"{name} is trainable\")\n",
    "        non_lora_param += param.numel()\n",
    "\n",
    "    if 'lm_head' in name:\n",
    "        print(\"lm_head ->\", name, \":\", param.requires_grad)\n",
    "    if 'embed_tokens' in name:\n",
    "        print(\"embed_tokens ->\", name, \":\", param.requires_grad)\n",
    "\n",
    "\n",
    "def into_million(val):\n",
    "    return f\"{val / 1000 / 1000 :.2f} million\"\n",
    "\n",
    "# print(\"LoRA adapter added.\")\n",
    "print(f\"Total LoRA params: {into_million(lora_param)} ({(lora_param/non_lora_param)*100:.2f} %) = {into_million(lora_param)}\")\n",
    "print(f\"Total LoRA layers: {lora_layers}\")\n",
    "print(f\"Approx size: {lora_param * 2e-6:.2f} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModulesToSaveWrapper(\n",
       "  (original_module): Embedding(49152, 960, padding_idx=2)\n",
       "  (modules_to_save): ModuleDict(\n",
       "    (smolthink): Embedding(49152, 960, padding_idx=2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is same weight of embed_tokens and lm_head? True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is same weight of embed_tokens and lm_head?\", torch.equal(model.base_model.model.model.embed_tokens.modules_to_save[\"smolthink\"].weight, model.base_model.model.lm_head.modules_to_save[\"smolthink\"].weight))\n",
    "print(model.base_model.model.model.embed_tokens.original_module.weight.data.data_ptr() == model.base_model.model.lm_head.original_module.weight.data.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.decoder.embed_tokens.modules_to_save[\"default\"].weight = model.base_model.model.lm_head.modules_to_save[\"default\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 0.72 GB of space (with buffer)\n"
     ]
    }
   ],
   "source": [
    "model = model.merge_and_unload(safe_merge=True).eval().to(torch.bfloat16)\n",
    "print(f\"Model took {model.get_memory_footprint()/1e9:.2f} GB of space (with buffer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409.00704\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_tokens -> base_model.model.model.embed_tokens.original_module.weight : False\n",
    "embed_tokens -> base_model.model.model.embed_tokens.modules_to_save.smolthink.weight : True\n",
    "lm_head -> base_model.model.lm_head.modules_to_save.smolthink.weight : True\n",
    "Total LoRA params: 34.73 million (7.61 %) = 34.73 million\n",
    "Total LoRA layers: 448\n",
    "Approx size: 69.47 mb\n",
    "Model took 0.82 GB of space (with buffer)\n",
    "409.00704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "69468160\n"
     ]
    }
   ],
   "source": [
    "total_param = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print('lora' in name.lower())\n",
    "    total_param += param.numel()\n",
    "\n",
    "print(lora_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer length: 49152\n",
      "[44, 17400, 46]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"\"\"{%- if tools %}\n",
    "    {{- '<|im_start|>system\\\\n' }}\n",
    "        {%- if messages[0]['role'] == 'system' %}\n",
    "            {- messages[0]['content'] }}\n",
    "        {%- else %}\n",
    "            {{- 'You are a helpful AI assistant named SmolThink.' }}\n",
    "        {%- endif %}\n",
    "    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> tags:\\\\n<tools>\\\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \\\"\\\\n\\\" }}\n",
    "            {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \\\"\\\\n</tools>\\\\n\\\\nYou first think/plan inside <think></think> tags.\\\\nThen for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\\\\n\\\" }}\n",
    "{%- else %}\n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\n",
    "    {%- else %}\n",
    "        {{- '<|im_start|>system\\\\nYou are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside \\\\'think\\\\' tag and provide final answer to user query inside \\\\'answer\\\\' tag.\\\\nRespond in the following format:\\\\n<think>\\\\nLet\\\\'s think step by step...\\\\n</think>\\\\n<answer>\\\\nThe final answer is...\\\\n</answer><|im_end|>\\\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- for message in messages %}\n",
    "    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\n",
    "    {%- elif message.role == \\\"assistant\\\" %}\n",
    "        {{- '<|im_start|>' + message.role }}\n",
    "        {%- if message.content %}\n",
    "            {{- '\\\\n' + message.content }}\n",
    "        {%- endif %}\n",
    "        {%- for tool_call in message.tool_calls %}\n",
    "            {%- if tool_call.function is defined %}\n",
    "                {%- set tool_call = tool_call.function %}\n",
    "            {%- endif %}\n",
    "            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\n",
    "            {{- tool_call.name }}\n",
    "            {{- '\\\", \\\"arguments\\\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- '}\\\\n</tool_call>' }}\n",
    "        {%- endfor %}\n",
    "        {{- '<|im_end|>\\\\n' }}\n",
    "    {%- elif message.role == \\\"tool\\\" %}\n",
    "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_response>\\\\n' }}\n",
    "        {{- message.content }}\n",
    "        {{- '\\\\n</tool_response>' }}\n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_end|>\\\\n' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\\\n' }}\n",
    "{%- endif %}\"\"\"\n",
    "\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"pad_token\": \"<|im_end|>\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"think_start\": \"<think>\",\n",
    "    \"think_end\": \"</think>\",\n",
    "    \"answer_start\": \"<answer>\",\n",
    "    \"answer_end\": \"</answer>\",\n",
    "    \"tool_def_start\": \"<tool>\",\n",
    "    \"tool_def_end\": \"</tool>\",\n",
    "    \"tool_call_start\": \"<tool_call>\",\n",
    "    \"tool_call_end\": \"</tool_call>\",\n",
    "    \"tool_res_start\": \"<tool_response>\",\n",
    "    \"tool_res_end\": \"</tool_response>\",\n",
    "}\n",
    "\n",
    "class SpecialTokens(str, Enum):\n",
    "    think_start = \"<think>\",\n",
    "    think_end = \"</think>\",\n",
    "    answer_start = \"<answer>\",\n",
    "    answer_end = \"</answer>\",\n",
    "    tool_def_start = \"<tool>\",\n",
    "    tool_def_end = \"</tool>\",\n",
    "    tool_call_start = \"<tool_call>\",\n",
    "    tool_call_end = \"</tool_call>\",\n",
    "    tool_res_start = \"<tool_response>\",\n",
    "    tool_res_end = \"</tool_response>\",\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    # additional_special_tokens=SpecialTokens.list()\n",
    ")\n",
    "tokenizer.chat_template = chat_template\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "print(\"Tokenizer length:\", len(tokenizer))\n",
    "tokenizer.save_pretrained(\"SmolThink-360M-Tokenizer\")\n",
    "\n",
    "\n",
    "# tokenizer.add_special_tokens(\n",
    "#     # special_tokens_dict=special_tokens_dict, \n",
    "#     {\"additional_special_tokens\":[\"<think>\"]},\n",
    "#     replace_additional_special_tokens=True)\n",
    "# print(\"Tokenizer length:\", len(tokenizer))\n",
    "\n",
    "# print(\"New token map\")\n",
    "# for v in SpecialTokens.list():\n",
    "#     print(v, '->', tokenizer.encode(v))\n",
    "# print(\"---\")\n",
    "\n",
    "# print(tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"I am fine\"}\n",
    "# ], tokenize=False))\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_payment_status\",\n",
    "            \"description\": \"Get payment status of a transaction\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"transaction_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The transaction id.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"transaction_id\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_payment_date\",\n",
    "            \"description\": \"Get payment date of a transaction\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"transaction_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The transaction id.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"transaction_id\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "# print(\"\\n-----\\n\")\n",
    "# print(tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"<tool_call>[retrieve_payment_date(12)]</tool_call>\"},\n",
    "#     {\"role\": \"tool\", \"content\": \"12/12/12\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"12/12/12\"}\n",
    "# ], tools=tools, tokenize=False))\n",
    "\n",
    "print(tokenizer.encode(\"<think>\"))\n",
    "print(tokenizer.encode(\"<|im_start|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling dataset length (after filter): 3497\n"
     ]
    }
   ],
   "source": [
    "def extract_tag(input_str, tag):\n",
    "    tool_def = re.findall(f\"<{tag}>(.*?)</{tag}>\", input_str, re.DOTALL)\n",
    "    tool_def = map(str.strip, tool_def)\n",
    "    tool_def = filter(lambda x: len(x) > 0, tool_def)\n",
    "    return list(tool_def)\n",
    "\n",
    "def hermes_fc_thinking(raw_data):\n",
    "    data = deepcopy(raw_data['conversations'])\n",
    "    seq = []\n",
    "    tool_def = None\n",
    "    tool_names = None\n",
    "    for d in data:\n",
    "        if d['role'] == 'system':\n",
    "            tool_def = extract_tag(d['content'], 'tools')\n",
    "            if len(tool_def) != 0:\n",
    "                try:\n",
    "                    tool_def = ast.literal_eval(tool_def[0])\n",
    "                    tool_names = [tool['function']['name'] for tool in tool_def]\n",
    "                    continue\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "            else:\n",
    "                return {\"conversations\": \"\"}\n",
    "\n",
    "        seq.append({})\n",
    "        seq[-1]['role'] = {\"human\": \"user\", \"model\": \"assistant\", \"system\": \"system\", \"tool\": \"tool\"}[d['role']]\n",
    "        seq[-1]['content'] = d['content']\n",
    "        if seq[-1]['role'] == 'assistant':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<think>', '<think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('</think>', '</think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<tool_call>\\n', '<tool_call>\\n[')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('\\n</tool_call>', ']\\n</tool_call>')\n",
    "            tool_calls = re.findall(r\"<tool_call>(.*?)</tool_call>\", seq[-1]['content'], re.DOTALL)\n",
    "            if tool_calls:\n",
    "                # print(tool_calls, tool_def)\n",
    "                try:\n",
    "                    tool_calls = json.loads(tool_calls[0].strip().replace(\"'\", '\"'))\n",
    "                    for tool_call in tool_calls:\n",
    "                        if tool_call['name'] not in tool_names:\n",
    "                            raise NotImplementedError\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "        if seq[-1]['role'] == 'tool':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"<tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"</tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].strip()\n",
    "        # seq[-1]['content'] = d['value']\n",
    "    \n",
    "    random.shuffle(tool_def)\n",
    "    ret = tokenizer.apply_chat_template(seq, tools=tool_def, tokenize=False, add_generation_prompt=False) #+ \"<tool_call>\\n\"\n",
    "    return {\"conversations\": ret}\n",
    "\n",
    "fc_dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")['train']\n",
    "# fc_dataset = fc_dataset.select(range(100))\n",
    "fc_dataset = fc_dataset.map(hermes_fc_thinking)\n",
    "fc_dataset = fc_dataset.filter(lambda x: len(x['conversations']) > 0)\n",
    "print(\"Function calling dataset length (after filter):\", len(fc_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(49162, 960, padding_idx=2)\n"
     ]
    }
   ],
   "source": [
    "print(model.cpu().resize_token_embeddings(len(tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49162, 960, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49162, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer\",\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    # additional_special_tokens=SpecialTokens.list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49152]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.encode(\"<think>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/tokenizer_config.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/special_tokens_map.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/vocab.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/merges.txt',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/added_tokens.json',\n",
       " '/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/Users/ohi/Documents/GitHub/PersonalAssistant/SmolThink-Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|> <|im_end|>\n",
      "[2]\n",
      "<issue_comment>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(tokenizer.encode(\"<|im_end|>\"))\n",
    "print(tokenizer.decode([9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_filter(data, limit):\n",
    "    # if data['thought_len'] + data['answer_len'] > 896:\n",
    "        # return False\n",
    "    return 0 < data['thought_len'] <= limit and 0 < data['answer_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1-distill dataset length (after filter): 29290\n"
     ]
    }
   ],
   "source": [
    "def r1distillsft_conv(data):\n",
    "    thought_len, answer_len = 0, 0\n",
    "    for idx, conv in enumerate(data['reannotated_messages']):\n",
    "        # print(conv)\n",
    "        role = conv['role']\n",
    "        if role == 'assistant':\n",
    "            reply = data['reannotated_messages'][idx]['content']\n",
    "            # print(reply)\n",
    "            thought = re.findall(r\"<think>(.*?)</think>\", reply, re.DOTALL)\n",
    "            thought = ''.join(thought).strip()\n",
    "            thought_len += len(thought.split()) #len(tokenizer.encode(thought))\n",
    "\n",
    "            end_tag = \"</think>\"\n",
    "            if end_tag in reply:\n",
    "                answer = reply[reply.find(end_tag)+len(end_tag):]\n",
    "                answer = answer.strip()\n",
    "            else:\n",
    "                answer = ''\n",
    "            if thought.lower() == answer.lower():\n",
    "                answer = ''\n",
    "            # print(\"Think:\", thought)\n",
    "            # print(\"Answer:\", answer)\n",
    "            # print(\"----\")\n",
    "            answer_len += len(answer.split()) #len(tokenizer.encode(answer))\n",
    "            data['reannotated_messages'][idx]['content'] = f\"<think>\\n{thought}\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n",
    "\n",
    "    if 'system' in data:\n",
    "        del data['system']\n",
    "    data['thought_len'] = thought_len\n",
    "    data['answer_len'] = answer_len\n",
    "    return data\n",
    "\n",
    "r1_dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", \"v1\")['train']\n",
    "r1_dataset.shuffle(123)\n",
    "r1_dataset = r1_dataset.select(range(90_000))\n",
    "r1_dataset = r1_dataset.map(r1distillsft_conv)\n",
    "r1_dataset = r1_dataset.filter(lambda x: length_filter(x, 256))\n",
    "delete_keys = list(r1_dataset.column_names)\n",
    "r1_dataset = r1_dataset.map(lambda x: {\"conversations\": tokenizer.apply_chat_template(x['reannotated_messages'], tools=None, tokenize=False)})\n",
    "r1_dataset = r1_dataset.remove_columns(delete_keys)\n",
    "print(\"R1-distill dataset length (after filter):\", len(r1_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing dataset length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 666.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DatasetGen_v1(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = None\n",
    "        self.cache_idx = -1\n",
    "        self.cache_len = 0\n",
    "        self.indices = []\n",
    "        self._get_len()\n",
    "\n",
    "    def _get_len(self):\n",
    "        print(\"Computing dataset length\")\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            self.gen(idx)\n",
    "            for i in range(self.cache_len):\n",
    "                self.indices.append((idx, i))\n",
    "        print(\"Total length of data:\", len(self.indices))\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def gen(self, idx):\n",
    "        self.cache = self.dataset[idx]['conversations'].rstrip()\n",
    "        self.cache = self.tokenizer(\n",
    "            self.cache,\n",
    "            max_length=CONTEXT_LEN,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=True, # Return the overflowing tokens\n",
    "            stride=CONTEXT_LEN // 8,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        self.cache_idx = idx\n",
    "        self.cache_len = len(self.cache['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p, q = self.indices[idx]\n",
    "        if self.cache_idx != p:\n",
    "            self.gen(p)\n",
    "        \n",
    "        input_ids = self.cache['input_ids'][q]\n",
    "        attention_mask = self.cache['attention_mask'][q]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "CONTEXT_LEN = 832 # 1024\n",
    "train_ds = DatasetGen_v1(\n",
    "    dataset=r1_dataset.select(range(10)), \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
      "Respond in the following format:\n",
      "<think>\n",
      "Let's think step by step...\n",
      "</think>\n",
      "<answer>\n",
      "The final answer is...\n",
      "</answer><|im_end|>\n",
      "<|im_start|>user\n",
      "There were 27 boys and 35 girls on the playground at recess. There were _____ children on the playground at recess.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "First, I need to determine the total number of children on the playground by adding the number of boys and girls.\n",
      "\n",
      "There are 27 boys and 35 girls.\n",
      "\n",
      "Adding these together: 27 boys + 35 girls = 62 children.\n",
      "\n",
      "Therefore, the total number of children on the playground is 62.\n",
      "</think>\n",
      "<answer>\n",
      "To find the total number of children on the playground, we simply add the number of boys and girls together.\n",
      "\n",
      "\\[\n",
      "\\text{Total children} = \\text{Number of boys} + \\text{Number of girls}\n",
      "\\]\n",
      "\n",
      "Plugging in the given values:\n",
      "\n",
      "\\[\n",
      "\\text{Total children} = 27 \\text{ boys} + 35 \\text{ girls} = 62 \\text{ children}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{62}\n",
      "\\]\n",
      "</answer><|im_end|>"
     ]
    }
   ],
   "source": [
    "for (t, m) in zip(train_ds[0]['input_ids'], train_ds[0]['attention_mask']):\n",
    "    if m == 1:\n",
    "        print(tokenizer.decode(t), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770 17520\n"
     ]
    }
   ],
   "source": [
    "long, short = 0, 0\n",
    "for idx, d in enumerate(r1_dataset):\n",
    "    \n",
    "    # print(d['conversations'])\n",
    "#     s = d['conversations'].replace('''<|im_start|>system\n",
    "# You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
    "# Respond in the following format:\n",
    "# <think>\n",
    "# Let's think step by step...\n",
    "# </think>\n",
    "# <answer>\n",
    "# The final answer is...\n",
    "# </answer><|im_end|>''', '')\n",
    "#     think_cnt = s.count(\"<think>\")\n",
    "#     ans_cnt = s.count(\"<answer>\")\n",
    "\n",
    "#     # print(s)\n",
    "#     if think_cnt > 1 or ans_cnt > 1:\n",
    "#         print(\"Check idx:\", idx)\n",
    "#         break\n",
    "    # break\n",
    "\n",
    "    en = tokenizer.encode(d['conversations'])\n",
    "    if len(en) > 832:\n",
    "        # print(idx, len(en))\n",
    "        long += 1\n",
    "    else:\n",
    "        short += 1\n",
    "\n",
    "print(long, short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4973 24317\n"
     ]
    }
   ],
   "source": [
    "long, short = 0, 0\n",
    "for idx, d in enumerate(r1_dataset):\n",
    "    \n",
    "    # print(d['conversations'])\n",
    "#     s = d['conversations'].replace('''<|im_start|>system\n",
    "# You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
    "# Respond in the following format:\n",
    "# <think>\n",
    "# Let's think step by step...\n",
    "# </think>\n",
    "# <answer>\n",
    "# The final answer is...\n",
    "# </answer><|im_end|>''', '')\n",
    "#     think_cnt = s.count(\"<think>\")\n",
    "#     ans_cnt = s.count(\"<answer>\")\n",
    "\n",
    "#     # print(s)\n",
    "#     if think_cnt > 1 or ans_cnt > 1:\n",
    "#         print(\"Check idx:\", idx)\n",
    "#         break\n",
    "    # break\n",
    "\n",
    "    en = tokenizer.encode(d['conversations'])\n",
    "    if len(en) > 1024:\n",
    "        # print(idx, len(en))\n",
    "        long += 1\n",
    "    else:\n",
    "        short += 1\n",
    "\n",
    "print(long, short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside 'think' tag and provide final answer to user query inside 'answer' tag.\n",
      "Respond in the following format:\n",
      "<think>\n",
      "Let's think step by step...\n",
      "</think>\n",
      "<answer>\n",
      "The final answer is...\n",
      "</answer><|im_end|>\n",
      "<|im_start|>user\n",
      "Michael and Thomas are selling their lego collections. They agree to split any money they earn. They sell them based on how many circles are on top. Each circle costs 1 cent. They sold a certain number of single pieces, 45 double pieces, 50 triple pieces and 165 quadruple pieces. They earned $5 each. How many single pieces did they sell?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "First, I need to determine the total amount of money Michael and Thomas earned together. Since each earned $5 and they split the money equally, the total earnings amount to $10.\n",
      "\n",
      "Next, I'll calculate the earnings from the double, triple, and quadruple pieces separately.\n",
      "\n",
      "For the double pieces, there are 45 pieces with 2 circles each. Each circle costs 1 cent, so the earnings from double pieces are 45 * 2 * $0.01 = $0.90.\n",
      "\n",
      "For the triple pieces, there are 50 pieces with 3 circles each. The earnings from triple pieces are 50 * 3 * $0.01 = $1.50.\n",
      "\n",
      "For the quadruple pieces, there are 165 pieces with 4 circles each. The earnings from quadruple pieces are 165 * 4 * $0.01 = $6.60.\n",
      "\n",
      "Adding up the earnings from double, triple, and quadruple pieces: $0.90 + $1.50 + $6.60 = $9.00.\n",
      "\n",
      "Subtracting this from the total earnings gives the amount earned from single pieces: $10 - $9.00 = $1.00.\n",
      "\n",
      "Since each single piece earns 1 cent, the number of single pieces sold is $1.00 / $0.01 = 100.\n",
      "\n",
      "Therefore, Michael and Thomas sold 100 single pieces.\n",
      "</think>\n",
      "<answer>\n",
      "To determine how many single pieces Michael and Thomas sold, let's break down the problem step by step.\n",
      "\n",
      "### **Given:**\n",
      "- **Single pieces:** \\( x \\) pieces (unknown)\n",
      "- **Double pieces:** 45 pieces (each with 2 circles)\n",
      "- **Triple pieces:** 50 pieces (each with 3 circles)\n",
      "- **Quadruple pieces:** 165 pieces (each with 4 circles)\n",
      "- **Earnings per person:** \\$5\n",
      "- **Total earnings:** \\$10 (since they split the money equally)\n",
      "\n",
      "### **Step 1: Calculate Earnings from Each Type of Piece**\n",
      "\n",
      "1. **Double Pieces:**\n",
      "   \\[\n",
      "   45 \\text{ pieces} \\times 2 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$0.90\n",
      "   \\]\n",
      "\n",
      "2. **Triple Pieces:**\n",
      "   \\[\n",
      "   50 \\text{ pieces} \\times 3 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$1.50\n",
      "   \\]\n",
      "\n",
      "3. **Quadruple Pieces:**\n",
      "   \\[\n",
      "   165 \\text{ pieces} \\times 4 \\text{ circles/piece} \\times \\$0.01/\\text{circle} = \\$6.60\n",
      "   \\]\n",
      "\n",
      "### **Step 2: Calculate Total Earnings from Double, Triple, and Quadruple Pieces**\n",
      "\n",
      "\\[\n",
      "\\$0.90 + \\$1.50 + \\$6.60 = \\$9.00\n",
      "\\]\n",
      "\n",
      "### **Step 3: Determine Earnings from Single Pieces**\n",
      "\n",
      "Total earnings: \\$10  \n",
      "Earnings from other pieces: \\$9.00  \n",
      "Earnings from single pieces:  \n",
      "\\[\n",
      "\\$10 - \\$9.00 = \\$1.00\n",
      "\\]\n",
      "\n",
      "### **Step 4: Calculate the Number of Single Pieces Sold**\n",
      "\n",
      "Each single piece earns \\$0.01.  \n",
      "Number of single pieces:  \n",
      "\\[\n",
      "\\$1.00 \\div \\$0.01 = 100 \\text{ pieces}\n",
      "\\]\n",
      "\n",
      "### **Final Answer**\n",
      "\n",
      "\\[\n",
      "\\boxed{100}\n",
      "\\]\n",
      "</answer><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling dataset length (after filter): 3497\n"
     ]
    }
   ],
   "source": [
    "def extract_tag(input_str, tag):\n",
    "    tool_def = re.findall(f\"<{tag}>(.*?)</{tag}>\", input_str, re.DOTALL)\n",
    "    tool_def = map(str.strip, tool_def)\n",
    "    tool_def = filter(lambda x: len(x) > 0, tool_def)\n",
    "    return list(tool_def)\n",
    "\n",
    "def hermes_fc_thinking(raw_data):\n",
    "    data = deepcopy(raw_data['conversations'])\n",
    "    seq = []\n",
    "    tool_def = None\n",
    "    tool_names = None\n",
    "    for d in data:\n",
    "        if d['role'] == 'system':\n",
    "            tool_def = extract_tag(d['content'], 'tools')\n",
    "            if len(tool_def) != 0:\n",
    "                try:\n",
    "                    tool_def = ast.literal_eval(tool_def[0])\n",
    "                    tool_names = [tool['function']['name'] for tool in tool_def]\n",
    "                    continue\n",
    "                except Exception as E:\n",
    "                    return {\"conversations\": \"\"}\n",
    "            else:\n",
    "                return {\"conversations\": \"\"}\n",
    "\n",
    "        seq.append({})\n",
    "        seq[-1]['role'] = {\"human\": \"user\", \"model\": \"assistant\", \"system\": \"system\", \"tool\": \"tool\"}[d['role']]\n",
    "        seq[-1]['content'] = d['content']\n",
    "        if seq[-1]['role'] == 'assistant':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('<think>', '<think>\\n')\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace('</think>', '</think>\\n')\n",
    "            # seq[-1]['content'] = seq[-1]['content'].replace('<tool_call>\\n', '<tool_call>\\n[')\n",
    "            # seq[-1]['content'] = seq[-1]['content'].replace('\\n</tool_call>', ']\\n</tool_call>')\n",
    "            tool_calls = re.findall(r\"<tool_call>(.*?)</tool_call>\", seq[-1]['content'], re.DOTALL)\n",
    "            seq[-1]['tool-call'] = []\n",
    "            if tool_calls:\n",
    "                # print(tool_calls, tool_def)\n",
    "                for tool_call in tool_calls:\n",
    "                    try:\n",
    "                        tool_call = json.loads(tool_call.strip().replace(\"'\", '\"'))\n",
    "                        if tool_call['name'] not in tool_names:\n",
    "                            raise NotImplementedError\n",
    "                        seq[-1]['tool_call'] = tool_call\n",
    "                    except Exception as E:\n",
    "                        return {\"conversations\": \"\"}\n",
    "        if seq[-1]['role'] == 'tool':\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"<tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].replace(\"</tool_response>\", \"\")\n",
    "            seq[-1]['content'] = seq[-1]['content'].strip()\n",
    "        # seq[-1]['content'] = d['value']\n",
    "    \n",
    "    random.shuffle(tool_def)\n",
    "    ret = tokenizer.apply_chat_template(seq, tools=tool_def, tokenize=False, add_generation_prompt=False) #+ \"<tool_call>\\n\"\n",
    "    return {\"conversations\": ret}\n",
    "\n",
    "fc_dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")['train']\n",
    "# fc_dataset = fc_dataset.select(range(100))\n",
    "fc_dataset = fc_dataset.map(hermes_fc_thinking)\n",
    "fc_dataset = fc_dataset.filter(lambda x: len(x['conversations']) > 0)\n",
    "print(\"Function calling dataset length (after filter):\", len(fc_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolThink.\n",
      "\n",
      "# Tools\n",
      "\n",
      "You may call one or more functions to assist with the user query.\n",
      "\n",
      "You are provided with function signatures within <tools></tools> tags:\n",
      "<tools>\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"get_stock_price\", \"description\": \"Get the current stock price of a company\", \"parameters\": {\"type\": \"object\", \"properties\": {\"company\": {\"type\": \"string\", \"description\": \"The name of the company\"}}, \"required\": [\"company\"]}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"get_movie_details\", \"description\": \"Get details about a movie\", \"parameters\": {\"type\": \"object\", \"properties\": {\"title\": {\"type\": \"string\", \"description\": \"The title of the movie\"}}, \"required\": [\"title\"]}}}\n",
      "</tools>\n",
      "\n",
      "You first think/plan inside <think></think> tags.\n",
      "Then for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hi, can you tell me the current stock price of Apple?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\n",
      "</think>\n",
      "<tool_call>\n",
      "{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "{'stock_price': '$150.75'}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "The current stock price of Apple is $150.75.<|im_end|>\n",
      "<|im_start|>user\n",
      "That's great. Can you also tell me the stock price for Microsoft?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\n",
      "\n",
      "Now, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \n",
      "\n",
      "I shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\n",
      "</think>\n",
      "<tool_call>\n",
      "{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "{'stock_price': '$210.22'}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "The current stock price of Microsoft is $210.22.<|im_end|>\n",
      "<|im_start|>user\n",
      "Thank you for the information.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "You're welcome! If you have any other questions, feel free to ask.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fc_dataset[0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/Users/ohi/Documents/GitHub/PersonalAssistant/datasets/merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'thought_len', 'answer_len', 'conversations'],\n",
       "    num_rows: 34125\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    conv = d['conversations']\n",
    "    if conv.count(\"</answer>\") > 2:\n",
    "        print(conv)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since XeTute/Open-Coding-Thoughts couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/ohi/.cache/huggingface/datasets/XeTute___open-coding-thoughts/default/0.0.0/b63d89b184b9048a18e2cd42be298db6e44ab255 (last modified on Sun Mar 16 12:25:21 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1025/1025 [00:00<00:00, 15778.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    Before organizing my code, I should consider the following to avoid mistakes:\\n    *   Make sure to create a clear directory structure to keep related files together.\\n    *   Use a consistent naming convention for your files and variables to avoid confusion.\\n    *   Be mindful of the file path and import statements to avoid errors.\\n    ', \"\\n    Before defining variables, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for variables to avoid confusion.\\n    *   Avoid using magic numbers or hardcoded values, as they can be difficult to maintain.\\n    *   Consider using a preprocessor-specific syntax for variables, such as Sass's `$` symbol or Less's `@` symbol.\\n    \", \"\\n    Before creating mixins and functions, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for mixins and functions to avoid confusion.\\n    *   Avoid over-engineering and keep the code simple and easy to understand.\\n    *   Consider using a preprocessor-specific syntax for mixins and functions, such as Sass's `@mixin` directive or Less's `.extend` method.\\n    \", \"\\n    Before using inheritance and nesting, I should consider the following to avoid mistakes:\\n    *   Make sure to use a consistent naming convention for classes and IDs to avoid confusion.\\n    *   Avoid over-nesting and keep the code simple and easy to understand.\\n    *   Consider using a preprocessor-specific syntax for inheritance and nesting, such as Sass's `@extend` directive or Less's `.extend` method.\\n    \", \"\\n    Before using a preprocessor-specific syntax, I should consider the following to avoid mistakes:\\n    *   Make sure to read and understand the documentation for the preprocessor you're using.\\n    *   Avoid using outdated or deprecated features, as they may not be supported in future versions.\\n    *   Consider using a code editor or IDE that provides support for the preprocessor-specific syntax, such as Sass's `scss` file type or Less's `less` file type.\\n    \"]\n",
      "[\"\\n\\n*   When dealing with large-scale projects, it's easy to get overwhelmed by the sheer amount of CSS code.\\n*   Modularizing your CSS code will help you break it down into smaller, manageable chunks.\\n*   This will make it easier to maintain and update individual components without affecting the entire project.\\n*   To modularize your CSS, consider creating separate files for different components or sections of your project.\\n\\n\", \"\\n\\n*   When working on a large team, it's essential to establish a consistent naming convention for your CSS classes.\\n*   This will help prevent conflicts and make it easier for others to understand your code.\\n*   Consider using a naming convention like BEM (Block, Element, Modifier) or OOCSS (Object-Oriented CSS).\\n\\n\", '\\n\\n*   Using broad, generic CSS selectors can lead to unintended styling of other elements on your page.\\n*   Instead, use specific selectors that target the exact elements you want to style.\\n*   Consider using the descendant combinator (` `) to target specific elements within a parent element.\\n\\n', '\\n\\n*   Inline styles can make it difficult to maintain and update your CSS code.\\n*   Instead, define your styles in an external CSS file.\\n*   Use a tool like a CSS preprocessor to help manage your styles.\\n\\n', '\\n\\n*   CSS preprocessors like Sass and Less can help you write more efficient and maintainable CSS code.\\n*   Tools like Gulp and Webpack can automate the concatenation and minification of your CSS files.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n\\n', '\\n\\n*   Testing and debugging your CSS code is crucial to ensuring it works as intended.\\n*   Use tools like the Chrome DevTools or Firefox Developer Edition to test and debug your CSS code.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n\\n', '\\n\\n*   Reviewing and refactoring your CSS code is essential to ensuring it remains maintainable and efficient.\\n*   Consider using a tool like CSS Lint to help identify potential issues with your CSS code.\\n*   Use a tool like a code analyzer to help identify potential issues with your CSS code.\\n\\n']\n",
      "[\"\\n\\nFirst, I need to design the HTML structure for the table. This will include a header row, a dropdown menu for selecting the number, and the table itself.\\n\\nNext, I need to think about how to link the dropdown menu to the table. I will use JavaScript to update the table when a number is selected from the dropdown menu.\\n\\nI will use CSS to style the table, making it look visually appealing.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start designing the HTML code.\\n\\n\", \"\\n\\nNext, I need to write the CSS code to style the table. This will include setting the table's width and border, and styling the table's rows and columns.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start writing the CSS code.\\n\\n\", \"\\n\\nNext, I need to write the JavaScript code to update the table when a number is selected. This will include getting the selected number from the dropdown menu, creating the table rows and columns, and updating the table's content.\\n\\nI will also think about how to handle errors, such as when the user selects a number outside the range of 1-10.\\n\\nNow, let's start writing the JavaScript code.\\n\\n\"]\n",
      "[\"\\n\\nTo create a responsive navigation menu, I'll consider the following requirements:\\n\\n*   A navigation menu that adapts to different screen sizes and devices (responsive design)\\n*   A clean and simple layout\\n*   Easy navigation for users with assistive technologies (e.g., screen readers)\\n\\nI'll start by designing a basic HTML structure for the navigation menu and then apply CSS styles to make it responsive.\\n\\n\", \"\\n\\nTo make this navigation menu responsive, I'll use CSS media queries to apply different styles based on screen size. I'll also use a CSS framework (in this case, Bootstrap) to simplify the process.\\n\\nOne potential mistake I might make is not considering screen reader users, who may have difficulty navigating menus with complex layouts. To fix this, I'll use ARIA attributes to provide a clear and consistent navigation experience.\\n\\nAnother potential mistake is not testing the responsiveness of the navigation menu on different devices and browsers. To fix this, I'll use tools like BrowserStack or CodePen to test the menu on various devices and browsers.\\n\\n\", \"\\n\\nTo further improve this design, I could use a CSS framework like Bootstrap to simplify the process of creating a responsive navigation menu. I could also use JavaScript to add interactivity to the menu, such as hover effects or animations.\\n\\nOne potential mistake I might make is not considering the accessibility of the navigation menu. To fix this, I'll use ARIA attributes to provide a clear and consistent navigation experience.\\n\\nAnother potential mistake is not testing the responsiveness of the navigation menu on different devices and browsers. To fix this, I'll use tools like BrowserStack or CodePen to test the menu on various devices and browsers.\\n\\n\"]\n",
      "['\\nFirst, I need to create a basic HTML file structure. I will include a `<script>` tag to link the JavaScript file.\\nNext, I will write the JavaScript code to display a greeting message when the page loads.\\nI need to ensure that the JavaScript code is correct and will not throw any errors.\\nI will also consider the security implications of directly executing JavaScript code from an HTML file. \\n', '\\nI will create a new file called greeting.js and write the JavaScript code to display the greeting message.\\nI will use the `window.onload` event to ensure that the code is executed when the page has finished loading.\\nI will also consider the best practices for coding, such as using a more secure way to execute the JavaScript code.\\n']\n",
      "[\"\\nI need to create a CSS rule that targets an HTML element with the class 'submit-button'. The rule should specify the styles for this element. However, I need to consider that CSS rules are case-sensitive and class selectors are usually defined inside a CSS selector, so I'll need to use a CSS selector that targets the class.\\n\", \"\\nTo ensure I don't make any mistakes, I've double-checked the CSS syntax. The above code defines a CSS rule that targets an HTML element with the class 'submit-button'. The styles defined are for a basic button with a green background color and white text. The padding and border radius are set to 10px and 5px respectively. The cursor is set to a pointing hand to indicate that the button is clickable. The font size is set to 16px. However, I also need to consider that this is a basic example and might need to be adjusted based on the specific requirements of the project.\\n\"]\n",
      "[\"\\n\\nBefore writing the script, I need to consider the following:\\n\\n1. What is the structure of the CSV file? Are there any header rows or specific column names?\\n2. What data manipulation is required? Is it as simple as filtering, sorting, or grouping data or do we need to perform more complex operations like data cleansing or data transformation?\\n3. Are there any specific libraries or dependencies required to read and manipulate the CSV file?\\n4. How should I handle errors or exceptions that may occur during the data manipulation process?\\n\\nAssuming the CSV file is well-structured and the data manipulation is relatively simple, I'll provide a basic script to get us started.\\n\\n\", '\\n\\nTo improve this script, I could consider the following:\\n\\n1. Add error handling mechanisms to handle exceptions that may occur during the data manipulation process.\\n2. Validate the structure of the CSV file to ensure it matches the expected format.\\n3. Provide more complex data manipulation operations, such as data cleansing or transformation.\\n4. Consider using more advanced libraries or tools to handle large-scale data manipulation tasks.\\n\\n']\n",
      "['\\n\\n*   The DOCTYPE declaration must be included at the beginning of the file.\\n*   The HTML tag is the root element of the HTML document.\\n*   The head section is used for the metadata of the document, such as the title, styles, and scripts.\\n*   The body section is where the content of the HTML document is placed.\\n*   In the head section, a link tag is used to link the external CSS file.\\n*   A script tag is used to link the external JavaScript file.\\n\\n', '\\n\\n*   The CSS style block is linked in the head section of the HTML file.\\n*   The CSS style block must have the same name as the linked file, in this case, style.css.\\n*   The body tag in CSS is used to select the HTML element with the tag name body.\\n*   The background-color property is used to set the background color of the element.\\n\\n', '\\n\\n*   The JavaScript function is linked in the body section of the HTML file.\\n*   The JavaScript function must be placed inside a script tag.\\n*   The console.log() function is used to output messages to the console.\\n*   The message to be logged must be placed inside the console.log() function.\\n\\n']\n",
      "[\"\\n\\nFirst, I need to determine the best way to simulate a coin toss. Since C++ does not have a built-in random number generator, I can use the `rand()` function to generate a random number. However, `rand()` is not recommended for generating random numbers for security or statistical purposes, so I will use the `random` library which is more secure.\\n\\nNext, I need to decide how to use the generated random number to determine the outcome of the coin toss. I can use a simple if statement to check if the number is even or odd, and return 'heads' or 'tails' accordingly.\\n\\nHowever, I also need to consider the potential mistakes I could make. One mistake could be to forget to include the necessary headers for the `random` library. Another mistake could be to not check if the random number generator is actually generating a random number.\\n\\n\", '\\n\\nA better way to simulate a coin toss would be to use the XOR shift algorithm to generate a random number. This algorithm is more complex than a simple if statement, but it is also more secure and more unpredictable.\\n\\nTo use the XOR shift algorithm, I need to generate a random number using the `random` library, and then use that number to calculate a new number using the XOR shift formula. I can then use that new number to determine the outcome of the coin toss.\\n\\nHowever, I also need to consider the potential mistakes I could make. One mistake could be to forget to include the necessary headers for the `random` library. Another mistake could be to not check if the random number generator is actually generating a random number.\\n\\n']\n",
      "[\"\\n\\nTo solve this problem, we need to:\\n\\n1. Import a library that will allow us to generate HTML. We'll use `string` and `template` for this, although it's worth noting that using Jinja2 or a similar templating engine would be a more conventional approach.\\n\\n2. Define a function that will take in the title and generate the HTML.\\n\\n3. Ensure that we're handling any potential errors that might occur during this process.\\n\\n\", \"\\n\\nIn this script, we use the `string` module to check if the title is a string and to remove any non-ASCII characters from it. We then define a function `generate_html` that takes in the title and returns the generated HTML.\\n\\nThe function first checks if the title is a string. If it's not, it raises a `TypeError`. It then removes any non-ASCII characters from the title to avoid encoding issues. Finally, it generates the HTML using an f-string and returns it.\\n\\nIn the example usage, we define a title and pass it to the `generate_html` function. The generated HTML is then printed out.\\n\\nMistakes I could make:\\n\\n* Not checking if the title is a string, which could lead to a `TypeError` or other errors.\\n* Not removing non-ASCII characters from the title, which could lead to encoding issues.\\n* Not handling potential errors that might occur during the generation of the HTML.\\n\\nTo fix these mistakes, I would:\\n\\n* Add a type check for the title to ensure it's a string.\\n* Remove non-ASCII characters from the title to avoid encoding issues.\\n* Handle potential errors that might occur during the generation of the HTML, such as raising a `TypeError` or other exceptions.\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1025/1025 [00:00<00:00, 13955.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenThought dataset length (after filter): 1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def openthought_code(data):\n",
    "    thought_len, answer_len = 0, 0\n",
    "\n",
    "    reply = data['output']\n",
    "    thought = re.findall(r\"<thoughts>(.*?)</thoughts>\", reply, re.DOTALL)\n",
    "    if len(thought) > 1:\n",
    "        print(thought)\n",
    "    thought = ''.join(thought).strip()\n",
    "    thought_len += len(thought.split())\n",
    "    \n",
    "    end_tag = \"</thoughts>\"\n",
    "    answer = reply[reply.find(end_tag)+len(end_tag):]\n",
    "    answer = answer.strip()\n",
    "    answer_len += len(answer.split()) #len(tokenizer.encode(answer))\n",
    "\n",
    "    if end_tag not in reply:\n",
    "        answer_len = 0\n",
    "\n",
    "    final_answer = f\"<think>\\n{thought}\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n",
    "    final_answer = final_answer.replace(\"<thoughts>\", \"\").replace(\"</thoughts>\", \"\")\n",
    "    \n",
    "    output_data = {\n",
    "        'thought_len': thought_len,\n",
    "        'answer_len': answer_len,\n",
    "        'conversations': [\n",
    "            {\"role\": \"user\", 'content': data['input']},\n",
    "            {\"role\": \"assistant\", 'content': final_answer}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return output_data\n",
    "\n",
    "openthought_dataset = load_dataset(\"XeTute/Open-Coding-Thoughts\")['train']\n",
    "print(\"Dataset length:\", len(openthought_dataset))\n",
    "openthought_dataset = openthought_dataset.map(openthought_code)\n",
    "openthought_dataset = openthought_dataset.map(lambda x: {\"conversations\": tokenizer.apply_chat_template(x['conversations'], tools=None, tokenize=False)})\n",
    "print(\"OpenThought dataset length (after filter):\", len(openthought_dataset))\n",
    "# print(openthought_dataset[0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8133/8133 [00:08<00:00, 984.77 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"open-r1/codeforces-cots\", \"solutions_py_decontaminated\")['train']\n",
    "\n",
    "def take(d):\n",
    "    return len(d['generation'].split()) <= 2000\n",
    "\n",
    "ds = ds.filter(take)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to solve this problem where I have to convert a given integer n into its binary representation without leading zeros. Let's think about how to approach this.\n",
      "\n",
      "The problem says that n can be up to 1e6. Wait, 1e6 isn't that big. But even so, I need an efficient way to convert it to binary. But what's the standard way to do this in Python?\n",
      "\n",
      "Hmm, Python has built-in functions for this. Like bin(n), which returns a string starting with '0b', so I can just slice that off. For example, bin(5) gives '0b101', so removing the first two characters gives '101'. That's exactly what the examples show. So for the input 5, output is 101. So using bin() seems straightforward.\n",
      "\n",
      "But wait, the problem says to output without leading zeros. But bin() automatically doesn't have leading zeros except for the case when n is 0, but since n is a positive integer (as per the problem statement), n can't be zero. So using bin(n)[2:] should give the correct result.\n",
      "\n",
      "So the code would be something like:\n",
      "\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "\n",
      "That's really simple. But is there any case where this might not work? Like when n is 1? Let's see. bin(1) is '0b1', so [2:] is '1', which is correct.\n",
      "\n",
      "What about edge cases like n=1e6? Let's compute 1e6 in binary. Well, bin(1000000) is '0b11110100001001000000', which when sliced gives the correct binary.\n",
      "\n",
      "This approach is O(1) time because converting an integer to binary in Python is done efficiently, even for large numbers. Since the maximum n is 1e6, which is manageable, this method should work within the time constraints. The built-in functions are optimized in C, so they're much faster than any manual implementation.\n",
      "\n",
      "Another approach would be to manually compute the binary digits by dividing n by 2 each time and collecting the remainders. But that's more code and possibly less efficient. For example:\n",
      "\n",
      "res = []\n",
      "while n > 0:\n",
      "    res.append(str(n % 2))\n",
      "    n = n // 2\n",
      "res.reverse()\n",
      "print(''.join(res))\n",
      "\n",
      "But this would also work and has O(log n) time complexity, which is acceptable. But why write all that code when Python's built-in does it more efficiently?\n",
      "\n",
      "So the optimal approach is to use bin() and slice off the first two characters. It's the fastest and most memory-efficient way. The code is straightforward and will handle all test cases within the constraints.\n",
      "\n",
      "Testing the examples:\n",
      "\n",
      "For input 5: bin(5)[2:] â†’ '101', correct.\n",
      "\n",
      "For input 13: bin(13)[2:] â†’ '1101', correct.\n",
      "\n",
      "So the code should work.\n",
      "</think>\n",
      "\n",
      "```python\n",
      "n = int(input())\n",
      "print(bin(n)[2:])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(ds[0]['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is 16*8?\n",
      "Assistant: 16 * 8 = 128\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\n",
      "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(processor.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<|im_start|>{% for message in messages %}\n",
    "  {{message['role'] | capitalize}}\n",
    "  {% if message['content'][0]['type'] == 'image' %}{{':'}}\n",
    "  {% else %}{{': '}}\n",
    "  {% endif %}\n",
    "  {% for line in message['content'] %}\n",
    "    {% if line['type'] == 'text' %}{{line['text']}}\n",
    "    {% elif line['type'] == 'image' %}{{ '<image>' }}\n",
    "    {% endif %}\n",
    "  {% endfor %}<end_of_utterance>\n",
    "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\"Muhammad Yunus[a] (born 28 June 1940) is a Bangladeshi economist, businessman, and politician who has been serving as Chief Adviser of the Interim Government of Bangladesh since 8 August 2024.[1] Yunus was awarded the Nobel Peace Prize in 2006 for founding the Grameen Bank and pioneering the concepts of microcredit and microfinance.[2] Yunus has received several other national and international honors, including the United States Presidential Medal of Freedom in 2009 and the Congressional Gold Medal in 2010.[3]\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
