{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(\n",
    "    [\n",
    "        Document(content=\"Super Mario was an important politician\"),\n",
    "        Document(content=\"Mario owns several castles and uses them to conduct important political business\"),\n",
    "        Document(\n",
    "            content=\"Super Mario was a successful military leader who fought off several invasion attempts by \"\n",
    "            \"his arch rival - Bowser\"\n",
    "        ),\n",
    "        Document(\n",
    "            content=\"Sheikh Hasina Wazed[a] (the dictator) (born 28 September 1947) is a Bangladeshi politician \"\n",
    "            \"and the tenth prime minister of Bangladesh from June 1996 to July 2001 and again serving \"\n",
    "            \"since January 2009. She is the daughter of Sheikh Mujibur Rahman, the founding father and \"\n",
    "            \"first president of Bangladesh. Having served for a combined total of over 20 years, she is \"\n",
    "            \"the longest serving prime minister in the history of Bangladesh. As of 4 August 2024, she is \"\n",
    "            \"the world's longest-serving female head of government.[2]\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "Given only the following information, answer the question.\n",
    "Ignore your own knowledge.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}?\n",
    "\"\"\"\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store, top_k=1))\n",
    "pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "pipe.add_component(\n",
    "    \"llm\", \n",
    "    OllamaGenerator(\n",
    "        model=\"gemma2:2b\", \n",
    "        url=\"http://localhost:11434/api/generate\",\n",
    "        generation_kwargs={\"temperature\": 0.2}\n",
    "    )\n",
    ")\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")\n",
    "query = \"What is the age of Seikh hasina?\"\n",
    "\n",
    "response = pipe.run(\n",
    "    {   \"prompt_builder\": {\"query\": query}, \n",
    "        \"retriever\": {\"query\": query}\n",
    "    },\n",
    "    include_outputs_from=\"prompt_builder\"\n",
    ")\n",
    "\n",
    "# print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to find Sheikh Hasina's age:\n",
      "\n",
      "1. **Find her birthdate:** The text states she was born on September 28, 1947.\n",
      "2. **Calculate the current year:**  The question asks for her age as of August 4, 2024.\n",
      "3. **Subtract her birth year from the current year:** 2024 - 1947 = 77\n",
      "\n",
      "**Therefore, Sheikh Hasina's age is 77.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['llm']['replies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given only the following information, answer the question.\n",
      "Ignore your own knowledge.\n",
      "\n",
      "Context:\n",
      "\n",
      "    Sheikh Hasina Wazed[a] (the dictator) (born 28 September 1947) is a Bangladeshi politician and the tenth prime minister of Bangladesh from June 1996 to July 2001 and again serving since January 2009. She is the daughter of Sheikh Mujibur Rahman, the founding father and first president of Bangladesh. Having served for a combined total of over 20 years, she is the longest serving prime minister in the history of Bangladesh. As of 4 August 2024, she is the world's longest-serving female head of government.[2]\n",
      "\n",
      "\n",
      "Question: Who is Seikh hasina??\n"
     ]
    }
   ],
   "source": [
    "print(response['prompt_builder']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_user(\"What's Natural Language Processing?\"),\n",
    "    ChatMessage.from_system(\n",
    "        \"Natural Language Processing (NLP) is a field of computer science and artificial \"\n",
    "        \"intelligence concerned with the interaction between computers and human language\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\"How do I get started?\"),\n",
    "]\n",
    "\n",
    "client = OllamaChatGenerator(model=\"phi3:mini\", timeout=45, url=\"http://localhost:11434/api/chat\")\n",
    "response = client.run(messages, generation_kwargs={\"temperature\": 0.2})\n",
    "print(response[\"replies\"][0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method run in module haystack_integrations.components.generators.ollama.chat.chat_generator:\n",
      "\n",
      "run(messages: List[haystack.dataclasses.chat_message.ChatMessage], generation_kwargs: Optional[Dict[str, Any]] = None) method of haystack_integrations.components.generators.ollama.chat.chat_generator.OllamaChatGenerator instance\n",
      "    Runs an Ollama Model on a given chat history.\n",
      "\n",
      "    :param messages:\n",
      "        A list of ChatMessage instances representing the input messages.\n",
      "    :param generation_kwargs:\n",
      "        Optional arguments to pass to the Ollama generation endpoint, such as temperature,\n",
      "        top_p, etc. See the\n",
      "        [Ollama docs](https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values).\n",
      "    :param streaming_callback:\n",
      "        A callback function that will be called with each response chunk in streaming mode.\n",
      "    :returns: A dictionary with the following keys:\n",
      "        - `replies`: The responses from the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id=cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94, content: 'There are over 7,000 languages spoken around the world today.', score: 14.802461901763598)]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "documents = [Document(content=\"There are over 7,000 languages spoken around the world today.\"),\n",
    "\t\t\t       Document(content=\"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"),\n",
    "\t\t\t       Document(content=\"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\")]\n",
    "\n",
    "document_store.write_documents(documents= documents + [Document(content=f\"nothing_{id}\") for id in range(10)])\n",
    "\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store, top_k=1)\n",
    "retriever.run(query=\"How many languages are spoken around the world today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'replies': [ChatMessage(content='\\nNatural Language Processing (NLP) ist ein Teilgebiet der künstlichen Intelligenz und des maschinellen Lernens, das sich mit dem Verständnis, der Analyse und Generierung menschlicher Sprache befasst.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'phi3:mini', 'created_at': '2024-08-05T00:21:16.699691Z', 'done_reason': 'stop', 'done': True, 'total_duration': 2828576875, 'load_duration': 10948666, 'prompt_eval_count': 57, 'prompt_eval_duration': 324602000, 'eval_count': 54, 'eval_duration': 2456960000})]}\n"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_system(\"Always respond in German even if some input data is in other languages.\"),\n",
    "    ChatMessage.from_user(\"What's Natural Language Processing? Be brief.\"),\n",
    "    ChatMessage.from_assistant(\"Sorry, I do not know German. Here is answer in English:\\n\")\n",
    "]\n",
    "\n",
    "generator = OllamaChatGenerator(\n",
    "    model=\"phi3:mini\",\n",
    "    url = \"http://localhost:11434/api/chat\",\n",
    "    generation_kwargs={\n",
    "        \"num_predict\": 100,\n",
    "        \"temperature\": 0.,\n",
    "    }\n",
    ")\n",
    "\n",
    "# chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\")\n",
    "print(generator.run(messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatMessage(content='Always respond in German even if some input data is in other languages.', role=<ChatRole.SYSTEM: 'system'>, name=None, meta={}), ChatMessage(content=\"What's Natural Language Processing? Be brief.\", role=<ChatRole.USER: 'user'>, name=None, meta={}), ChatMessage(content='Sorry, I do not know German. Here is answer in English:\\n', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={})]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't compile non template nodes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptBuilder\n\u001b[0;32m----> 3\u001b[0m something \u001b[38;5;241m=\u001b[39m \u001b[43mPromptBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/haystack/core/component/component.py:194\u001b[0m, in \u001b[0;36mComponentMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m pre_init_hook \u001b[38;5;241m=\u001b[39m _COMPONENT_PRE_INIT_HOOK\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_init_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pre_init_hook\u001b[38;5;241m.\u001b[39min_progress:\n\u001b[0;32m--> 194\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/haystack/components/builders/prompt_builder.py:164\u001b[0m, in \u001b[0;36mPromptBuilder.__init__\u001b[0;34m(self, template, required_variables, variables)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_variables \u001b[38;5;241m=\u001b[39m required_variables \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env \u001b[38;5;241m=\u001b[39m SandboxedEnvironment()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m variables:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# infere variables from template\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     ast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mparse(template)\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/jinja2/environment.py:1108\u001b[0m, in \u001b[0;36mEnvironment.from_string\u001b[0;34m(self, source, globals, template_class)\u001b[0m\n\u001b[1;32m   1106\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_globals(\u001b[38;5;28mglobals\u001b[39m)\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m template_class \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_class\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_code(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m, gs, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/jinja2/environment.py:761\u001b[0m, in \u001b[0;36mEnvironment.compile\u001b[0;34m(self, source, name, filename, raw, defer_init)\u001b[0m\n\u001b[1;32m    759\u001b[0m     source_hint \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    760\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(source, name, filename)\n\u001b[0;32m--> 761\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefer_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefer_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/jinja2/environment.py:691\u001b[0m, in \u001b[0;36mEnvironment._generate\u001b[0;34m(self, source, name, filename, defer_init)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    681\u001b[0m     source: nodes\u001b[38;5;241m.\u001b[39mTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m     defer_init: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal hook that can be overridden to hook a different generate\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    method in.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.5\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefer_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefer_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/jinja2/compiler.py:112\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(node, environment, name, filename, stream, defer_init, optimized)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate the python source for a node tree.\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, nodes\u001b[38;5;241m.\u001b[39mTemplate):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt compile non template nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m generator \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mcode_generator_class(\n\u001b[1;32m    115\u001b[0m     environment, name, filename, stream, defer_init, optimized\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    117\u001b[0m generator\u001b[38;5;241m.\u001b[39mvisit(node)\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't compile non template nodes"
     ]
    }
   ],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "something = PromptBuilder(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
