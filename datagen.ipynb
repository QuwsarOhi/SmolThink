{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import ollama\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# ds = load_dataset(\"neural-bridge/rag-dataset-12000\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi3 templates\n",
    "\n",
    "search_query_template = \\\n",
    "\"\"\"<|system|>\n",
    "You are a helpful assistant. You will be asked a question. Imagine you do not know the answer of the question. As a result, you want to search the web to find the answer. For the given user question, write a search question that could be used in search engine to find the answer to the question. The search string that you would produce should be inside <search tag>. \n",
    "For example: <search> your search string </search><|end|>\n",
    "<|user|>\n",
    "{query}<|end|>\n",
    "<|assistant|>\n",
    "Sure! Here is the one short search string that I would use to search on the web:\n",
    "<search>\"\"\"\n",
    "\n",
    "\n",
    "question_gen_template = \\\n",
    "\"\"\"<|system|>\n",
    "You are a helpful assistant who is very good at generating question. The user would give you a topic, you have to come up with a question so that the question can be used to train a good quality LLM.<|end|>\n",
    "<|user|>\n",
    "Generate question in topic of: {topic_name}\n",
    "\n",
    "Produce the question inside \"question\" tag. Example: <question> your generated question </question><|end|>\n",
    "<|assistant|>\n",
    "Sure! Here is the question on topic {topic_name}:\n",
    "<question>\"\"\"\n",
    "\n",
    "# Ref: https://huggingface.co/learn/cookbook/en/llm_judge\n",
    "llm_judge_search_prompt = \\\n",
    "\"\"\"<|system|>\n",
    "You will be given a topic, user_question, and search_string triplet.\n",
    "Your task is to provide a 'total rating' scoring how well the search_string alings the user concerns expressed in the user_question.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The system_question is not on topic: completely irrelevant to the topic, or very partial\n",
    "2: The system_question is mostly not helpful: has grammatical error or seems to be incomplete\n",
    "3: The search_string is mostly helpful: provides concise prompt that can be used in web search engine\n",
    "4: The search_string is excellent: relevant, direct, and addresses important concerns raised in the system_question\n",
    "\n",
    "Use the following rubrics to award the points:\n",
    "- Award 1 point if the user_question is related to the topic.\n",
    "- Give 1 additional point if the system_question is clear and precise.\n",
    "- Provide 1 further point if the search_str is concise.\n",
    "- One final point should be awarded if the search_str is direct and addresses important concerns raised in system_question.\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: <eval> (your rationale for the rating, as a text) </eval>\n",
    "Total rating: <rating> (your rating, as a number between 1 and 4) </rating>\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.<|end|>\n",
    "<|user|>\n",
    "Now here are the question and answer.\n",
    "\n",
    "Topic: {topic}\n",
    "Question: {question}\n",
    "Search string: {search_str}\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.<|end|>\n",
    "<|assistant|>\n",
    "Feedback:::\n",
    "Evaluation: <eval>\"\"\"\n",
    "\n",
    "\n",
    "llm_judge_prompt = \\\n",
    "\"\"\"<|system|>\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
    "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
    "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
    "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
    "\n",
    "Use the following rubrics to award the points:\n",
    "- Award 1 point if the answer is related to the question.\n",
    "- Give 1 additional point if the answer is clear and precise.\n",
    "- Provide 1 further point if the answer is true.\n",
    "- One final point should be awarded if the answer provides additional resources to support the user.\n",
    "\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: <eval> (your rationale for the rating, as a text) </eval>\n",
    "Total rating: <rating> (your rating, as a number between 1 and 4) </rating>\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.<|end|>\n",
    "<|user|>\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.<|end|>\n",
    "<|assistant|>\n",
    "Feedback:::\n",
    "Evaluation: <eval>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ollama/ollama-python/blob/main/examples/async-chat-stream/main.py\n",
    "def ollama_infr(prompt, extra_stops=[], model='phi3.5:latest', temperature=0.7):\n",
    "    # https://github.com/ollama/ollama-python/blob/00eafed0faa5dea6879a8eb3229c7a8f2439abb4/ollama/_types.py#L93\n",
    "    return ollama.generate(\n",
    "        model = model,\n",
    "        # system = system,\n",
    "        # Raw is set to true to feed the question as needed\n",
    "        raw=True,\n",
    "        prompt = prompt,\n",
    "        stream = True,\n",
    "        # Number of seconds to keep the connection alive\n",
    "        keep_alive=60*60,\n",
    "        options = {\n",
    "            'stop': [\n",
    "                \"<|start_header_id|>\",\n",
    "                \"<|end_header_id|>\",\n",
    "                \"<|eot_id|>\",\n",
    "            ] + extra_stops,\n",
    "            'temperature': temperature,\n",
    "            # 'top_k': 1,\n",
    "            'cache': False,\n",
    "            # 'tfs_z': 2.0,\n",
    "            'num_ctx': 6000,\n",
    "            # 'temperature': 0.0,\n",
    "            # 'top_p': 0.0\n",
    "        },\n",
    "    )\n",
    "\n",
    "DATA_PATH = \"datasets/search_data.jsonl\"\n",
    "def write_jsonl(data: dict):\n",
    "    with open(DATA_PATH, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "\n",
    "def extract_rating(input_str:str):\n",
    "    data = input_str.split('<rating>')[1]\n",
    "    data = data.strip()\n",
    "\n",
    "    try:\n",
    "        return float(data)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_r1 = '''You are a helpful AI. You will be given a question and a context. You have to first think (in 150 words) and then come up with the final answer based on the question and user-provided context\n",
    "\n",
    "<｜User｜>User question: {question}\n",
    "\n",
    "Use the following content to answer user question:\n",
    "{context}\n",
    "\n",
    "<｜end▁of▁sentence｜>\n",
    "<｜Assistant｜>\n",
    "<think>\n",
    "I will finish thinking in at most 150 words. Now let's think. '''\n",
    "\n",
    "def r1_response(question, context):    \n",
    "    prompt = deepseek_r1.format(question=question, context=context)\n",
    "    # print(\"Question:\", data['question'], flush=True)\n",
    "\n",
    "    stream = ollama_infr(prompt=prompt, model='deepseek-r1:7b', temperature=0.5)\n",
    "    model_res = '<think>\\n'\n",
    "    n_think_tokens = 0\n",
    "    think_finished = False\n",
    "\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        model_res += part['response']\n",
    "\n",
    "        if not think_finished and '</think>' in model_res:\n",
    "            think_finished = True\n",
    "        if not think_finished:\n",
    "            n_think_tokens += 1\n",
    "\n",
    "        if n_think_tokens > 256:\n",
    "            print(\"Generation limit exceeded\", flush=True)\n",
    "            return None, None\n",
    "\n",
    "    think = re.findall(r\"<think>(.*?)</think>\", model_res, re.DOTALL)[0].strip()\n",
    "    answer = model_res[model_res.find(\"</think>\")+len(\"</think>\"):].strip()\n",
    "\n",
    "    if not answer:\n",
    "        return None, None\n",
    "    \n",
    "    return think, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = \\\n",
    "\"\"\"<|system|>\n",
    "You are a helpful assistant. You will be given a web content in markdown format. You have to provide a summary of the web content.\n",
    "You have to summarize the web content inside 'summary' tag.\n",
    "For example: <summary> your summarization content in markdown </summary><|end|>\n",
    "<|user|>\n",
    "{web_content}<|end|>\n",
    "<|assistant|>\n",
    "Sure! Here is the summarized version of the provided content:\n",
    "<summary>\"\"\"\n",
    "\n",
    "def web_content_summarize(web_content):\n",
    "    prompt = summarize_template.format(web_content=web_content)\n",
    "    # print(\"Question:\", data['question'], flush=True)\n",
    "\n",
    "    stream = ollama_infr(prompt=prompt, model='phi3.5:latest', temperature=0.5)\n",
    "    model_res = '<summary>\\n'\n",
    "    n_tokens = 0\n",
    "\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        model_res += part['response']\n",
    "        n_tokens += 1\n",
    "\n",
    "        if n_tokens > 4000:\n",
    "            break\n",
    "\n",
    "    summary = re.findall(r\"<summary>(.*?)</summary>\", model_res, re.DOTALL)#[0].strip()    \n",
    "    if summary:\n",
    "        return summary[0].strip()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_content(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove scripts, styles, navs, headers, footers, and typical ad elements\n",
    "    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript', 'iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "    ad_classes = ['advertisement', 'ad', 'adsbygoogle', 'promo', 'banner', 'cookie-banner', 'subscribe']\n",
    "    for class_name in ad_classes:\n",
    "        for tag in soup.select(f'.{class_name}, #{class_name}'):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Remove all links and their content\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    # Markdown content building\n",
    "    markdown_lines = []\n",
    "\n",
    "    # Process headings and paragraphs\n",
    "    for element in soup.body.descendants if soup.body else soup.descendants:\n",
    "        if element.name:\n",
    "            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                level = int(element.name[1])\n",
    "                markdown_lines.append(f\"{'#' * level} {element.get_text(strip=True)}\\n\")\n",
    "            elif element.name in ['ul', 'ol']:\n",
    "                for li in element.find_all('li'):\n",
    "                    markdown_lines.append(f\"- {li.get_text(strip=True)}\")\n",
    "            elif element.name == 'p':\n",
    "                text = element.get_text(strip=True)\n",
    "                if text:\n",
    "                    markdown_lines.append(f\"{text}\\n\")\n",
    "\n",
    "    # Final cleanup: remove empty lines\n",
    "    markdown_lines = list(filter(lambda x: len(x) > 2, markdown_lines))\n",
    "    markdown_content = '\\n'.join([line for line in markdown_lines if line.strip()])    \n",
    "    # print(\"URL EXTRACTED:\", markdown_lines, flush=True)\n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "def search_tool(search_str, max_results=1):\n",
    "    rets = None\n",
    "    with DDGS() as ddg:\n",
    "        rets = list(ddg.text(keywords=search_str, region=\"wt-wt\", max_results=7))\n",
    "\n",
    "    str_rets = ''\n",
    "    web_contents = []\n",
    "    web_summary = []\n",
    "    web_url = []\n",
    "    n_results, i = 0, -1\n",
    "    while n_results < max_results and i+1 < len(rets):\n",
    "        i += 1\n",
    "        # try:\n",
    "        if True:\n",
    "            print(\"Parsing url:\", rets[i]['href'], flush=True)\n",
    "            web_content = url_content(rets[i]['href'])[:1024*4] + \" ...\"\n",
    "            web_content = web_content.strip()\n",
    "            if web_content == '':\n",
    "                continue\n",
    "\n",
    "            # web_content = web_content_summarize(web_content=web_content)\n",
    "            content = f\"\\n# Source {n_results+1}:\"\n",
    "            content += \"\\n\" + \"-\" * len(content) + f\"\\n\\n{web_content}\\n\\n\"\n",
    "            str_rets += content\n",
    "            n_results += 1\n",
    "        # except Exception as E:\n",
    "        #     # print(E)\n",
    "        #     continue\n",
    "        \n",
    "    return str_rets\n",
    "\n",
    "print(search_tool('current methods used by scientists to improve predictions of climate change and its environmental impact', max_results=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"python math question\", \"git\", \"problem solving\", \"python debugging\", \"current knowledge\", \"terminal commands\", \"computer science\"]\n",
    "\n",
    "for _ in range(500):\n",
    "    topic = random.choice(topics)\n",
    "    # topic = \"greeting\"\n",
    "    print(f\"## Topic: {topic}\", flush=True)\n",
    "    stream = ollama_infr(question_gen_template.format(topic_name=topic), extra_stops=[\"</question>\"])\n",
    "    question = ''\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        question += part['response']\n",
    "        if len(question) > 1000:\n",
    "            break\n",
    "    print(flush=True)\n",
    "\n",
    "    if len(question) > 1000:\n",
    "        print(\"Question overflow\", flush=True)\n",
    "        continue\n",
    "    question = question.strip()\n",
    "\n",
    "    stream = ollama_infr(search_query_template.format(query=question), extra_stops=[\"</search>\"], temperature=0.3)\n",
    "    search_str = ''\n",
    "    print(\"Search str: \", end='', flush=True)\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        search_str += part['response']\n",
    "    print(flush=True)\n",
    "    search_str = search_str.strip()\n",
    "    search_str = search_str.replace('\"', '')\n",
    "\n",
    "    if len(search_str) > 300:\n",
    "        print(\"Ignoring question and search string as search_str length is greater than limit\", flush=True)\n",
    "        continue\n",
    "\n",
    "    max_results = random.choice(range(3))\n",
    "    context = search_tool(search_str, max_results=max_results)\n",
    "    if context.strip() == '': \n",
    "        print(\"Empty context found\", flush=True)\n",
    "        continue\n",
    "    \n",
    "    print(\"### Search results:\", flush=True)\n",
    "    print(context, flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "    print(\"### Deepseek-r1\", flush=True)\n",
    "    think, answer = r1_response(question=question, context=context)\n",
    "    if answer is None:\n",
    "        print(\"No answer found from deepseek-r1\", flush=True)\n",
    "        continue\n",
    "\n",
    "    print(\"\\n\\n### LLM As Judge\", flush=True)\n",
    "    stream = ollama_infr(llm_judge_prompt.format(question=question, answer=answer), extra_stops=[\"</rating>\"])\n",
    "    judge_response = ''\n",
    "    for part in stream:\n",
    "        print(part['response'], sep='', end='', flush=True)\n",
    "        judge_response += part['response']\n",
    "\n",
    "    rating = extract_rating(judge_response)\n",
    "    print(\"\\n\\nRATING:\", rating, end=\"\", flush=True)\n",
    "    print(\"\\n--------\", flush=True)\n",
    "\n",
    "    if rating is not None and rating >= 3.0:\n",
    "        write_jsonl({\n",
    "            \"question\": question,\n",
    "            \"search_str\": search_str,\n",
    "            \"search_results\": context,\n",
    "            \"n_search_results\": max_results,\n",
    "            \"think\": think,\n",
    "            \"answer\": answer,\n",
    "            \"judge_response\": judge_response,\n",
    "            \"judge_rating\": rating\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
