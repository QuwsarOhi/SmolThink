{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/PersonalAssistant/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "# https://www.philschmid.de/mini-deepseek-r1\n",
    "# https://huggingface.co/blog/open-r1/update-1\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb#scrollTo=cXk993X6C2ZZ\n",
    "\n",
    "#import torch._dynamo\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from datasets import load_dataset\n",
    "import peft\n",
    "\n",
    "# from safetensors.torch import load_model, save_model\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Optional\n",
    "from jinja2 import Template\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(base_directory):\n",
    "    checkpoint_dirs = []\n",
    "    \n",
    "    # List all directories in the base directory\n",
    "    for dir_name in os.listdir(base_directory):\n",
    "        if re.match(r'checkpoint-\\d+', dir_name):  # Match pattern \"checkpoint-N\"\n",
    "            checkpoint_dirs.append(dir_name)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        return None  # No checkpoints found\n",
    "    \n",
    "    # Sort directories based on numerical value\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[1]))\n",
    "    \n",
    "    return os.path.join(base_directory, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 0.72 GB of space (with buffer)\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "SIZE = \"360M\"\n",
    "MODEL_PATH = f\"HuggingFaceTB/SmolLM2-{SIZE}-Instruct\"\n",
    "FILE_PATH = get_latest_checkpoint(\"/Users/ohi/Documents/GitHub/PersonalAssistant/weights/SmolThink-360M-sft/\")\n",
    "# LORA_PATH = os.path.join(LORA_PATH, \"think_lora\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # MODEL_PATH,\n",
    "    FILE_PATH,\n",
    "    device_map=\"mps\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # attn_implementation='sdpa', 'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "# model = peft.PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     LORA_PATH,\n",
    "#     is_trainable=True, # ðŸ‘ˆ here,\n",
    "# )\n",
    "\n",
    "# model = model.merge_and_unload().eval().type(torch.bfloat16)\n",
    "\n",
    "# Gradient checkpointing - Could take more memory in MPS\n",
    "model.gradient_checkpointing_enable(dict(use_reentrant=False))\n",
    "\n",
    "# Sanity check\n",
    "# non_lora_param = 0\n",
    "# lora_param = 0\n",
    "# lora_layers = 0\n",
    "# for name, param in model.named_parameters():\n",
    "#     # if ...:  # some check on name (ex. if 'lora' in name)\n",
    "#         # param.requires_grad = False\n",
    "#     # print(name, param.requires_grad)\n",
    "#     if 'lora' in name:\n",
    "#         # param.requires_grad = True\n",
    "#         assert param.requires_grad == True, f\"{name} is not trainable\"\n",
    "#         lora_param += param.numel()\n",
    "#         lora_layers += 1\n",
    "#     else:\n",
    "#         assert param.requires_grad == False\n",
    "#         non_lora_param += param.numel()\n",
    "\n",
    "# # print(\"LoRA adapter added.\")\n",
    "# print(f\"Total LoRA params: {lora_param} ({(lora_param/non_lora_param)*100:.2f} %) = ({(lora_param+non_lora_param)/1e6:.2f} million)\")\n",
    "# print(f\"Total LoRA layers: {lora_layers}\")\n",
    "# print(f\"Approx LoRA size: {lora_param * 2e-6:.2f} mb\")\n",
    "print(f\"Model took {model.get_memory_footprint()/1e9:.2f} GB of space (with buffer)\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"{%- if tools %}\n",
    "    {{- '<|endoftext|><|im_start|>system\\\\n' }}\n",
    "        {%- if messages[0]['role'] == 'system' %}\n",
    "            {- messages[0]['content'] }}\n",
    "        {%- else %}\n",
    "            {{- 'You are a helpful AI assistant named SmolThink.' }}\n",
    "        {%- endif %}\n",
    "    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> tags:\\\\n<tools>\\\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \\\"\\\\n\\\" }}\n",
    "            {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \\\"\\\\n</tools>\\\\n\\\\nYou first think/plan inside <think></think> tags.\\\\nThen for each function call, return a json object with function name and arguments within <tool_call></tool_call> tags.<|im_end|>\\\\n\\\" }}\n",
    "{%- else %}\n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- '<|endoftext|><|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\n",
    "    {%- else %}\n",
    "        {{- '<|endoftext|><|im_start|>system\\\\nYou are a helpful AI assistant named SmolThink. First plan/reason/code/validate inside <think></think> tag and provide final answer to user query inside <answer></answer> tag.<|im_end|>\\\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- for message in messages %}\n",
    "    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\n",
    "    {%- elif message.role == \\\"assistant\\\" %}\n",
    "        {{- '<|im_start|>' + message.role }}\n",
    "        {%- if message.content %}\n",
    "            {{- '\\\\n' + message.content }}\n",
    "        {%- endif %}\n",
    "        {%- for tool_call in message.tool_calls %}\n",
    "            {%- if tool_call.function is defined %}\n",
    "                {%- set tool_call = tool_call.function %}\n",
    "            {%- endif %}\n",
    "            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\n",
    "            {{- tool_call.name }}\n",
    "            {{- '\\\", \\\"arguments\\\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- '}\\\\n</tool_call>' }}\n",
    "        {%- endfor %}\n",
    "        {{- '<|im_end|>\\\\n' }}\n",
    "    {%- elif message.role == \\\"tool\\\" %}\n",
    "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_response>\\\\n' }}\n",
    "        {{- message.content }}\n",
    "        {{- '\\\\n</tool_response>' }}\n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_end|>\\\\n' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\\\n<think>\\\\n' }}\n",
    "    {%- if tools %}\n",
    "        {{- 'I have access to ' }}{% for tool in tools %}'{{ tool.function.name }}'{% if not loop.last %}, {% endif %}{% endfor %}\n",
    "        {{- ' as tools. Let\\\\'s evaluate each of them to and then identify the best tool based on given context:' }}\n",
    "    {% endif %}\n",
    "{%- endif %}\"\"\"\n",
    "\n",
    "SIZE = \"360M\"\n",
    "MODEL_PATH = f\"HuggingFaceTB/SmolLM2-{SIZE}-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.chat_template = chat_template\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'valid', 'tool', 'tool_call'],\n",
      "    num_rows: 551209\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def tool_call_process(data):\n",
    "    new_data = {\n",
    "        'prompt': '',\n",
    "        'valid': False,\n",
    "        'tool': None,\n",
    "        'tool_call': None\n",
    "    }\n",
    "    tool_def = None\n",
    "    try:\n",
    "        tool_def = json.loads(data['tools'])\n",
    "        new_data['tool'] = data['tools']\n",
    "        # print(new_data['tool'], flush=True)\n",
    "    except Exception as E:\n",
    "        # print(\"Error in tool:\", E)\n",
    "        return new_data\n",
    "\n",
    "    # print(tool_def)\n",
    "\n",
    "    seq = []\n",
    "    for s in json.loads(data['conversation']):\n",
    "        if s['role'] == 'user':\n",
    "            seq.append(s)\n",
    "        elif s['role'] == 'tool call':\n",
    "            tool_call = deepcopy(s['content'])\n",
    "            if tool_call:\n",
    "                tool_name = tool_call['name']\n",
    "                new_data['tool_call'] = str(tool_call)\n",
    "                tidx = -1\n",
    "                for idx, tool in enumerate(tool_def):\n",
    "                    if tool['name'] == tool_name:\n",
    "                        tidx = idx\n",
    "                if tidx == -1:\n",
    "                    return new_data\n",
    "\n",
    "                try:\n",
    "                    for (k, v) in tool_call['arguments'].items():\n",
    "                        if k not in tool_def[tidx].get('arguments'):\n",
    "                            # print(f\"{k} not in {new_data['tool']}\", flush=True)\n",
    "                            return new_data\n",
    "                except Exception as E:\n",
    "                    return new_data\n",
    "            else:\n",
    "                new_data['tool_call'] = \"[]\"\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    new_data['valid'] = True\n",
    "    tool_def = [{\"type\": \"function\", \"function\": e} for e in tool_def]\n",
    "    new_data['prompt'] = tokenizer.apply_chat_template(seq, tools=tool_def, tokenize=False, add_generation_prompt=False)\n",
    "    # print(json.dumps(new_data, indent=2), flush=True)\n",
    "    return new_data\n",
    "\n",
    "dataset = load_dataset(\"BitAgent/tool_calling_shuffle\")['train']\n",
    "col_names = dataset.column_names\n",
    "# dataset = dataset.select(range(3))\n",
    "dataset = dataset.map(tool_call_process)\n",
    "dataset = dataset.remove_columns(col_names)\n",
    "dataset = dataset.filter(lambda x: x['valid'])\n",
    "\n",
    "print(dataset)\n",
    "# print(\"---\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gen: {'name': 'calculate', 'arguments': {'expression': '(9 * 2 + 33) / 2'}}\n",
      "Grnd: {'name': 'calculate', 'arguments': {'expression': '(9 * 2 + 33) / 2'}}\n",
      "Tool-call Score: [1.25]\n",
      "[1.25]\n"
     ]
    }
   ],
   "source": [
    "question = \"Do the math: '(9 * 2 + 33) / 2'\"\n",
    "\n",
    "response = '''<think>\n",
    "Sure, I can help with that. Let me see what I have available. The tools provided include two functions: 'retrieve_payment_status' and 'calculate'. \n",
    "\n",
    "First, 'calculate' requires an expression, which in this case is 9 multiplied by 2 plus 33 divided by 2. Since the user has already given a number as input, I don't need to ask for more information here. So I'll use 'calculate' without any arguments because it will handle both numbers directly.\n",
    "\n",
    "Next, 'retrieve_payment_status' doesn't require any parameters since the user hasn't mentioned anything about payment status. It's just asking for the status of their recent transaction. Therefore, I won't make any changes to its function call.\n",
    "\n",
    "So, the next step is to call 'calculate' with the expression 9 * 2 + 33 / 2. This should give the user the desired result efficiently.\n",
    "</think>\n",
    "<tool_call>\n",
    "{'name': 'calculate', 'arguments': {'expression': '(9 * 2 + 33) / 2'}}\n",
    "</tool_call>\n",
    "<tool_call>\n",
    "{'name': 'calculate', 'arguments': {'expression': '(9 * 2 + 33) / 2'}}\n",
    "</tool_call>'''\n",
    "\n",
    "tool_call = \"{'name': 'calculate', 'arguments': {'expression': '(9 * 2 + 33) / 2'}}\"\n",
    "\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<tool_call>\\s*(.*?)\\s*</tool_call>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    print(responses)\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    print(matches)\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "\n",
    "def tool_call_reward(pred, ground):\n",
    "    score = 0.\n",
    "    ground_tools = [g['name'] for g in ground]\n",
    "    for p in pred:\n",
    "        if p['name'] in ground_tools:\n",
    "            score += 0.25\n",
    "        if p in ground:\n",
    "            score += 0.25\n",
    "    pred_tools = [p['name'] for p in pred]\n",
    "    for g in ground:\n",
    "        if g['name'] in pred_tools:\n",
    "            score += 0.25\n",
    "        if g in pred:\n",
    "            score += 0.25\n",
    "    return score\n",
    "\n",
    "\n",
    "def validate_format(text):\n",
    "    \"\"\"\n",
    "    Validate if the text strictly follows the pattern:\n",
    "    <think> ... </think><tool_call> ... </tool_call>\n",
    "    \n",
    "    Returns True if the string matches the pattern, False otherwise.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^\\s*<think>.*?</think>\\s*<tool_call>.*?</tool_call>\\s*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "\n",
    "def correctness_reward_func(prompts, completions, tool_call, **kwargs) -> list[float]:\n",
    "    # for i in range(1):\n",
    "    #     print(\"-----Question-----\", flush=True)\n",
    "    #     print(prompts[i], flush=True)\n",
    "    #     print(\"-----Generation-----\", flush=True)\n",
    "    #     print(completions[i], flush=True)\n",
    "\n",
    "    score = []\n",
    "    for gen in completions:\n",
    "        tag = \"</tool_call>\\n\"\n",
    "        gen = gen[:gen.find(tag)+len(tag)]\n",
    "        if validate_format(gen):\n",
    "            score.append(0.5)\n",
    "        else:\n",
    "            score.append(0.0)\n",
    "        \n",
    "    print(\"Correctness Score:\", score, flush=True)\n",
    "    return score\n",
    "\n",
    "\n",
    "from ast import literal_eval\n",
    "def tool_parse(tool_call:str):\n",
    "    ret = None\n",
    "    try:\n",
    "        ret = literal_eval(tool_call)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    _tool_call = tool_call.replace(\"'\", '\"')\n",
    "    ret = json.loads(_tool_call)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def tool_call_score(prompts, completions, tool_call, **kwargs):\n",
    "    # Compile regex to capture content inside <tool_call> tags, allowing for whitespace/newlines.\n",
    "    pattern = re.compile(r\"<tool_call>(.*?)</tool_call>\", re.DOTALL)\n",
    "    matches = [pattern.findall(text) for text in completions]\n",
    "    calls = []\n",
    "    score = []\n",
    "    for i, match in enumerate(matches):\n",
    "        try: \n",
    "            gen_tool_calls = match[0].strip() #.replace(\"'\", '\"')\n",
    "            ground_tool_calls = tool_call[i].strip() #.replace(\"'\", '\"')\n",
    "            print(f\" Gen: {gen_tool_calls}\\nGrnd: {ground_tool_calls}\")\n",
    "            gen_tool_calls = tool_parse(gen_tool_calls)\n",
    "            ground_tool_calls = tool_parse(ground_tool_calls)\n",
    "\n",
    "            if gen_tool_calls and gen_tool_calls == ground_tool_calls:\n",
    "                score.append(1.0 + len(gen_tool_calls['arguments'])*0.25)\n",
    "            elif gen_tool_calls == ground_tool_calls:\n",
    "                score.append(1.0)\n",
    "            elif gen_tool_calls['name'] == ground_tool_calls['name']:\n",
    "                s = 0.5\n",
    "                for k, v in gen_tool_calls['arguments'].items():\n",
    "                    if ground_tool_calls['arguments'].get(k, None) == v:\n",
    "                        s += 0.25\n",
    "                    elif ground_tool_calls['arguments'].get(k, None) != None:\n",
    "                        s += 0.1\n",
    "                score.append(s)\n",
    "            else:\n",
    "                score.append(0.25)\n",
    "        except Exception as E:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            print(E, \"|\", \"line:\", exc_tb.tb_lineno)\n",
    "            score.append(0.0)\n",
    "\n",
    "    print(\"Tool-call Score:\", score)\n",
    "    return score\n",
    "\n",
    "# print(soft_format_reward_func([[{\"content\": response}]]))\n",
    "print(tool_call_score([question], [response], [tool_call]))\n",
    "\n",
    "# # Example usage:\n",
    "# text_valid = \"\"\"<think>\n",
    "# This is some content inside think.\n",
    "# </think>\n",
    "# <tool_call>\n",
    "# [{'name': 'calculate', 'arguments': {'expression': '9 * 2'}}]\n",
    "# </tool_call>\"\"\"\n",
    "\n",
    "# text_invalid = \"\"\"<think>\n",
    "# This is some content inside think.\n",
    "# </think>\n",
    "# Extra content that should not be here.\n",
    "# <tool_call>\n",
    "# [{'name': 'calculate', 'arguments': {'expression': '9 * 2'}}]\n",
    "# </tool_call>\"\"\"\n",
    "\n",
    "# print(validate_format(text_valid))    # Expected output: True\n",
    "# print(validate_format(text_invalid))  # Expected output: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: [0.0, 0.0]\n",
      " Gen: {'name': 'check_alarm_status', 'arguments': {'alarm_type': 'example', 'location': 'example_location'}}\n",
      "Grnd: {'name': 'check_alarm_status', 'arguments': {}}\n",
      " Gen: {'name': 'check_alarm_status', 'arguments': {'armed': True, 'reason': 'active'}}\n",
      "Grnd: {'name': 'check_alarm_status', 'arguments': {}}\n",
      "Expecting value: line 1 column 55 (char 54) | line: 104\n",
      "Tool-call Score: [0.5, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='551209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     5/551209 03:05 < 9460:27:49, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: [0.0, 0.0]\n",
      " Gen: {'name': 'setup_auto_response_email', 'arguments': {'email_address': 'your_email_address', 'response_message': 'Our support email has been received. Please respond with your support message here'}}\n",
      "Grnd: {'name': 'setup_auto_response_email', 'arguments': {'email_address': 'support@example.com', 'response_message': 'Thank you for contacting us. We are currently unable to respond to support requests due to technical difficulties. We will resume operations as soon as possible.'}}\n",
      " Gen: {'name': 'setup_auto_response_email', ' arguments': {'email_address': 'support@example.com', 'response_message': 'Please provide the message you want to send as the automated reply'} }\n",
      "Grnd: {'name': 'setup_auto_response_email', 'arguments': {'email_address': 'support@example.com', 'response_message': 'Thank you for contacting us. We are currently unable to respond to support requests due to technical difficulties. We will resume operations as soon as possible.'}}\n",
      "'arguments' | line: 113\n",
      "Tool-call Score: [0.7, 0.0]\n",
      "Correctness Score: [0.0, 0.0]\n",
      " Gen: {'name': 'check_alarm_status', 'arguments': {'alarm_status': 'on'}}\n",
      "Grnd: {'name': 'check_alarm_status', 'arguments': {}}\n",
      " Gen: {'name': 'check_alarm_status', 'arguments': {'day': 100, 'is_armed': True}}\n",
      "Grnd: {'name': 'check_alarm_status', 'arguments': {}}\n",
      "Expecting value: line 1 column 70 (char 69) | line: 104\n",
      "Tool-call Score: [0.5, 0.0]\n",
      "Correctness Score: [0.0, 0.0]\n",
      " Gen: \"name\": \"create_churn_predictor\",\n",
      "  \"arguments\": {\n",
      "    \"data\": {\"pandas dataframe\": {\"required\": True, \"type\": \"list\", \"description\": \"A Pandas DataFrame containing customer data.\"}},\n",
      "    \"target_var\": {\"string\": {\"required\": True, \"type\": \"string\", \"description\": \"The name of the target variable column, which should be binary (0/1) representing churn/no churn.\"}},\n",
      "    \"model_type\": {\"string\": {\"required\": False, \"type\": \"string\", \"description\": \"The type of model to use. Default is ' logistic regression'.\"}}\n",
      "  }\n",
      "Grnd: {'name': 'create_churn_predictor', 'arguments': {'data': 'customer_data', 'target_var': 'churn', 'model_type': 'logistic regression', 'features': ['age', 'income', 'num_purchases']}}\n",
      "Extra data: line 1 column 7 (char 6) | line: 104\n",
      " Gen: {'name': 'create_churn_predictor', 'arguments': {'data': {'required': True, 'type': pd.DataFrame, 'description': 'A Pandas DataFrame containing customer data'}, 'target_var': {'required': True, 'type': 'string', 'description': 'The name of the target variable column, which should be binary (0/1) representing churn/no churn'}, 'model_type': {'required': {'type': 'string', 'description': 'The type of model to use. Default is 'logistic regression'. Other options:'}}}}\n",
      "Grnd: {'name': 'create_churn_predictor', 'arguments': {'data': 'customer_data', 'target_var': 'churn', 'model_type': 'logistic regression', 'features': ['age', 'income', 'num_purchases']}}\n",
      "Expecting value: line 1 column 71 (char 70) | line: 104\n",
      "Tool-call Score: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "import transformers, gc\n",
    "\n",
    "class MpsCacheClearCallback(transformers.TrainerCallback):\n",
    "    def __clearmem(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Note: Clearing model gradients\n",
    "        # for param in model.parameters():\n",
    "            # param.grad = None\n",
    "        # print(\"\\nMEMORY CLEARED\\n\")\n",
    "    #def on_step_begin(self, *args, **kwargs):      self.__clearmem()\n",
    "    def on_step_end(self, *args, **kwargs):        self.__clearmem()\n",
    "    #def on_substep_end(self, *args, **kwargs):     self.__clearmem()\n",
    "    #def on_evaluate(self, *args, **kwargs):        self.__clearmem()\n",
    "    #def on_optimizer_step(self, *args, **kwargs):  self.__clearmem()\n",
    "    #def on_predict(self, *args, **kwargs):         self.__clearmem()\n",
    "    #def on_prediction_step(self, *args, **kwargs): self.__clearmem()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = False,\n",
    "    learning_rate = 5e-4,\n",
    "    # adam_beta1 = 0.9,\n",
    "    # adam_beta2 = 0.99,\n",
    "    weight_decay = 0.2,\n",
    "    warmup_ratio = 0.1,\n",
    "    logging_steps=5,\n",
    "    max_steps=len(dataset),\n",
    "    save_steps = 10,\n",
    "    save_total_limit=1,\n",
    "    ds3_gather_for_generation=False,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    # Memory reduction\n",
    "    optim = \"adafactor\",    # adamw_torch\n",
    "    # Memory reduction\n",
    "    bf16 = True,\n",
    "    bf16_full_eval=True,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    # Memory reduction\n",
    "    torch_empty_cache_steps=1,\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = 512,\n",
    "    max_completion_length = 512,\n",
    "    temperature=0.9,\n",
    "    # top_k=15,   # default is 50\n",
    "    # repetition_penalty = 1.1, # default is 1\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = f\"/Users/ohi/Documents/GitHub/PersonalAssistant/weights/SmolLM2-{SIZE}-grpo\",\n",
    "    # Memory reduction\n",
    "    dataloader_pin_memory=False,\n",
    "    # Gradient checkpointing - could take more memory in MPS\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "    # torch_compile=True,\n",
    "    # include_tokens_per_second=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.temperature = 0.9\n",
    "model.generation_config.top_k = 20\n",
    "# model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    # processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        correctness_reward_func,\n",
    "        tool_call_score\n",
    "        # xmlcount_reward_func,\n",
    "        # soft_format_reward_func,\n",
    "        # strict_format_reward_func,\n",
    "        # #int_reward_func,\n",
    "        # correctness_reward_func,\n",
    "        # reason_len_reward,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[MpsCacheClearCallback()]\n",
    "    # peft_config=peft_config, #get_peft_config(model_config),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
